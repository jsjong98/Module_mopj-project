import os
import json
import csv
import hashlib
import pandas as pd
import numpy as np
import logging
import shutil
import traceback
from pathlib import Path
from datetime import datetime
import time
import os
import csv

from app.config import CACHE_ROOT_DIR, UPLOAD_FOLDER
from app.utils.date_utils import get_semimonthly_period, format_date, is_holiday
from app.core.state_manager import prediction_state
from app.utils.serialization import safe_serialize_value, clean_interval_scores_safe, convert_to_legacy_format, clean_predictions_data

# ìˆœí™˜ import ë°©ì§€ë¥¼ ìœ„í•´ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì§€ì—° import ì‚¬ìš©

logger = logging.getLogger(__name__)

# íŒŒì¼ í•´ì‹œ ìºì‹œ (ë©”ëª¨ë¦¬ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)
_file_hash_cache = {}
_cache_lookup_index = {}
_dataframe_cache = {}
_cache_expiry_seconds = 120

def get_file_cache_dirs(file_path=None):
    """
    ğŸš€ í†µí•© ì €ì¥ì†Œ ì‹œìŠ¤í…œ: íŒŒì¼ê³¼ ë¬´ê´€í•˜ê²Œ ëª¨ë“  ê²ƒì„ í†µí•© ê´€ë¦¬
    
    ì´ì œ íŒŒì¼ë³„ ìºì‹œ ëŒ€ì‹  í†µí•© ì €ì¥ì†Œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:
    - ëª¨ë“  ì˜ˆì¸¡ ê²°ê³¼ â†’ app/predictions/
    - ëª¨ë“  ëª¨ë¸ â†’ app/models/
    - ëª¨ë“  í”Œë¡¯ â†’ app/plots/
    
    ê¸°ì¡´ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œê³¼ì˜ í˜¸í™˜ì„±ì„ ìœ„í•´ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ë°˜í™˜í•˜ì§€ë§Œ
    ì‹¤ì œë¡œëŠ” í†µí•© ë””ë ‰í† ë¦¬ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.
    """
    try:
        # ğŸŒŸ í†µí•© ì €ì¥ì†Œ ì‚¬ìš© - íŒŒì¼ ê²½ë¡œì™€ ë¬´ê´€
        logger.info(f"ğŸŒŸ [UNIFIED_SYSTEM] Using unified storage system (file-agnostic)")
        
        return get_unified_storage_dirs()
        
    except Exception as e:
        logger.error(f"âŒ Error in get_file_cache_dirs: {str(e)}")
        logger.error(traceback.format_exc())
        raise e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ì „íŒŒ
    
def calculate_file_hash(file_path, chunk_size=8192):
    """íŒŒì¼ ë‚´ìš©ì˜ SHA256 í•´ì‹œë¥¼ ê³„ì‚°"""
    import hashlib
    
    hash_sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            while chunk := f.read(chunk_size):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        logger.error(f"File hash calculation failed: {str(e)}")
        return None
    
def get_data_content_hash(file_path):
    """ë°ì´í„° íŒŒì¼(CSV/Excel)ì˜ ì „ì²˜ë¦¬ëœ ë‚´ìš©ìœ¼ë¡œ í•´ì‹œ ìƒì„± (ìºì‹± ìµœì í™”)"""
    import hashlib
    import os
    
    try:
        # íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ë°˜ ìºì‹œ í™•ì¸
        if file_path in _file_hash_cache:
            cached_mtime, cached_hash = _file_hash_cache[file_path]
            current_mtime = os.path.getmtime(file_path)
            
            # íŒŒì¼ì´ ìˆ˜ì •ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ìºì‹œëœ í•´ì‹œ ë°˜í™˜
            if abs(current_mtime - cached_mtime) < 1.0:  # 1ì´ˆ ì´ë‚´ ì°¨ì´ëŠ” ë¬´ì‹œ
                logger.debug(f"ğŸ“‹ Using cached hash for {os.path.basename(file_path)}")
                return cached_hash
        
        # íŒŒì¼ì´ ìˆ˜ì •ë˜ì—ˆê±°ë‚˜ ìºì‹œê°€ ì—†ëŠ” ê²½ìš° ìƒˆë¡œ ê³„ì‚°
        logger.info(f"ğŸ”„ Calculating new hash for {os.path.basename(file_path)}")
        
        # ğŸ”§ ìˆœí™˜ ì°¸ì¡° ë°©ì§€: load_data ëŒ€ì‹  íŒŒì¼ ë‚´ìš© í•´ì‹œ ì§ì ‘ ê³„ì‚°
        # Excel íŒŒì¼ì˜ ê²½ìš°ì—ë„ íŒŒì¼ ë‚´ìš© í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
        logger.info(f"ğŸ”„ Using file content hash for {os.path.basename(file_path)} (avoid circular reference)")
        file_content_hash = calculate_file_hash(file_path)
        if file_content_hash:
            file_hash = file_content_hash[:16]
            logger.info(f"âœ… File-based hash calculated: {file_hash} for {os.path.basename(file_path)}")
        else:
            logger.error(f"âŒ File hash calculation failed for {os.path.basename(file_path)}")
            return None
        
        # ìºì‹œ ì €ì¥
        _file_hash_cache[file_path] = (os.path.getmtime(file_path), file_hash)
        
        return file_hash
    except Exception as e:
        logger.error(f"Data content hash calculation failed: {str(e)}")
        # í•´ì‹œ ê³„ì‚°ì— ì‹¤íŒ¨í•˜ë©´ íŒŒì¼ ê¸°ë³¸ í•´ì‹œë¥¼ ì‚¬ìš©
        try:
            fallback_hash = calculate_file_hash(file_path)
            if fallback_hash:
                return fallback_hash[:16]
        except Exception:
            pass
        return None
    
def build_cache_lookup_index():
    """ìºì‹œ ë””ë ‰í† ë¦¬ì˜ ì¸ë±ìŠ¤ë¥¼ ë¹Œë“œí•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥"""
    global _cache_lookup_index
    
    try:
        _cache_lookup_index = {}
        cache_root = Path(CACHE_ROOT_DIR)
        
        if not cache_root.exists():
            return
        
        for file_dir in cache_root.iterdir():
            if not file_dir.is_dir() or file_dir.name == "default":
                continue
            
            predictions_dir = file_dir / 'predictions'
            if not predictions_dir.exists():
                continue
            
            prediction_files = list(predictions_dir.glob("prediction_start_*_meta.json"))
            
            for meta_file in prediction_files:
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    file_hash = meta_data.get('file_content_hash')
                    data_end_date = meta_data.get('data_end_date')
                    
                    if file_hash and data_end_date:
                        semimonthly = get_semimonthly_period(pd.to_datetime(data_end_date))
                        cache_key = f"{file_hash}_{semimonthly}"
                        
                        _cache_lookup_index[cache_key] = {
                            'meta_file': str(meta_file),
                            'predictions_dir': str(predictions_dir),
                            'data_end_date': data_end_date,
                            'semimonthly': semimonthly
                        }
                        
                except Exception:
                    continue
                    
        logger.info(f"ğŸ“Š Built cache lookup index with {len(_cache_lookup_index)} entries")
        
    except Exception as e:
        logger.error(f"Failed to build cache lookup index: {str(e)}")
        _cache_lookup_index = {}

def refresh_cache_index():
    """ìºì‹œ ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œê³ ì¹¨ (ìƒˆë¡œìš´ ìºì‹œ íŒŒì¼ì´ ìƒì„±ëœ í›„ í˜¸ì¶œ)"""
    global _cache_lookup_index
    logger.info("ğŸ”„ Refreshing cache lookup index...")
    build_cache_lookup_index()

def clear_cache_memory():
    """ë©”ëª¨ë¦¬ ìºì‹œë¥¼ í´ë¦¬ì–´ (ë©”ëª¨ë¦¬ ì ˆì•½ìš©)"""
    global _file_hash_cache, _cache_lookup_index
    _file_hash_cache.clear()
    _cache_lookup_index.clear()
    logger.info("ğŸ§¹ Cleared memory cache")

def check_existing_prediction(current_date, file_path=None):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ì„ í™•ì¸í•˜ê³  ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ğŸ¯ í˜„ì¬ íŒŒì¼ì˜ ë””ë ‰í† ë¦¬ì—ì„œ ìš°ì„  ê²€ìƒ‰ (ì •í™•í•œ ë‚ ì§œ ë§¤ì¹­ë§Œ ì‚¬ìš©)
    
    ìˆ˜ì •ì‚¬í•­:
    - ë°˜ì›” ê¸°ê°„(semimonthly) ë§¤ì¹­ ì œê±°
    - ì •í™•í•œ ë‚ ì§œ ë§¤ì¹­ë§Œ í—ˆìš©í•˜ì—¬ ë™ì¼í•œ ë‚ ì§œì˜ ìºì‹œë§Œ ì‚¬ìš©
    """
    try:
        # í˜„ì¬ ë‚ ì§œ(ë°ì´í„° ê¸°ì¤€ì¼)ì—ì„œ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ ê³„ì‚°
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # ë‹¤ìŒ ì˜ì—…ì¼ ì°¾ê¸° (í˜„ì¬ ë‚ ì§œì˜ ë‹¤ìŒ ì˜ì—…ì¼ì´ ì²« ë²ˆì§¸ ì˜ˆì¸¡ ë‚ ì§œ)
        next_date = current_date + pd.Timedelta(days=1)
        while next_date.weekday() >= 5 or is_holiday(next_date):
            next_date += pd.Timedelta(days=1)
        
        first_prediction_date = next_date
        date_str = first_prediction_date.strftime('%Y%m%d')
        
        logger.info(f"ğŸ” Checking cache for EXACT prediction date: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Data end date: {current_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“… Expected prediction start: {first_prediction_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“„ Expected filename pattern: prediction_start_{date_str}.*")
        
        # ğŸ¯ 1ë‹¨ê³„: í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì •í™•í•œ ë‚ ì§œ ë§¤ì¹˜ë¡œ ìºì‹œ ì°¾ê¸°
        try:
            # ğŸ”§ ìˆ˜ì •: íŒŒì¼ ê²½ë¡œë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
            cache_dirs = get_file_cache_dirs(file_path)
            file_predictions_dir = cache_dirs['predictions']
            
            logger.info(f"  ğŸ“ Cache directory: {cache_dirs['root']}")
            logger.info(f"  ğŸ“ Predictions directory: {file_predictions_dir}")
            logger.info(f"  ğŸ“ Directory exists: {file_predictions_dir.exists()}")
            
        except Exception as e:
            logger.error(f"âŒ Failed to get cache directories: {str(e)}")
            return None
        
        if file_predictions_dir.exists():
            exact_csv = file_predictions_dir / f"prediction_start_{date_str}.cs"
            exact_meta = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
            
            logger.info(f"  ğŸ” Looking for: {exact_csv}")
            logger.info(f"  ğŸ” CSV exists: {exact_csv.exists()}")
            logger.info(f"  ğŸ” Meta exists: {exact_meta.exists()}")
            
            if exact_csv.exists() and exact_meta.exists():
                from app.data.cache_manager import load_prediction_with_attention_from_csv_in_dir
                logger.info(f"âœ… Found EXACT prediction cache in file directory: {exact_csv.name}")
                return load_prediction_with_attention_from_csv_in_dir(first_prediction_date, file_predictions_dir)
            
            # í•´ë‹¹ íŒŒì¼ ë””ë ‰í† ë¦¬ì—ì„œ ë‹¤ë¥¸ ë‚ ì§œì˜ ì˜ˆì¸¡ ì°¾ê¸°
            logger.info("ğŸ” Searching for other predictions in file directory...")
            prediction_files = list(file_predictions_dir.glob("prediction_start_*_meta.json"))
            
            logger.info(f"  ğŸ“‹ Found {len(prediction_files)} prediction files:")
            for i, pf in enumerate(prediction_files):
                logger.info(f"    {i+1}. {pf.name}")
            
            # ğŸ”§ ìˆ˜ì •: ë°˜ì›” ê¸°ê°„ ë§¤ì¹­ ì œê±° - ì •í™•í•œ ë‚ ì§œë§Œ í—ˆìš©
            logger.info("âŒ No exact date match found in file directory")
            logger.info("  ğŸ“ Note: Only exact date matches are allowed (no approximate/semimonthly matching)")
        else:
            logger.warning(f"âŒ Predictions directory does not exist: {file_predictions_dir}")
        
        # ğŸ¯ 2ë‹¨ê³„: ë‹¤ë¥¸ íŒŒì¼ë“¤ì˜ ìºì‹œì—ì„œ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ì°¾ê¸°
        current_file_path = file_path or prediction_state.get('current_file', None)
        if current_file_path:
            # ğŸ”§ ìˆ˜ì •: ëª¨ë“  ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ íƒìƒ‰
            upload_dir = Path(UPLOAD_FOLDER)
            existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
            
            logger.info(f"ğŸ” [EXACT_DATE_CACHE] Searching other files for EXACT date match: {len(existing_files)} files")
            
            for existing_file in existing_files:
                try:
                    # ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_predictions_dir = existing_cache_dirs['predictions']
                    
                    if existing_predictions_dir.exists():
                        # ë™ì¼í•œ ë°˜ì›” ê¸°ê°„ì˜ ì˜ˆì¸¡ íŒŒì¼ ì°¾ê¸°
                        pattern = f"prediction_start_*_meta.json"
                        meta_files = list(existing_predictions_dir.glob(pattern))
                        
                        logger.info(f"    ğŸ“ {existing_file.name}: {len(meta_files)}ê°œ ì˜ˆì¸¡ íŒŒì¼")
                        
                        # ğŸ”§ ìˆ˜ì •: ì •í™•í•œ ë‚ ì§œì˜ ì˜ˆì¸¡ íŒŒì¼ë§Œ ì°¾ê¸°
                        exact_csv_other = existing_predictions_dir / f"prediction_start_{date_str}.cs"
                        exact_meta_other = existing_predictions_dir / f"prediction_start_{date_str}_meta.json"
                        
                        if exact_csv_other.exists() and exact_meta_other.exists():
                            logger.info(f"    ğŸ¯ Found EXACT date match in {existing_file.name}!")
                            logger.info(f"    ğŸ“… Exact prediction date: {first_prediction_date.strftime('%Y-%m-%d')}")
                            logger.info(f"    ğŸ“„ Using file: {exact_csv_other.name}")
                            
                            return load_prediction_with_attention_from_csv_in_dir(first_prediction_date, existing_predictions_dir)
                        else:
                            logger.debug(f"    âŒ No exact date match in {existing_file.name}")
                except Exception as e:
                    logger.debug(f"    âš ï¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì ‘ê·¼ ì‹¤íŒ¨ {existing_file.name}: {str(e)}")
                    continue
                    
            logger.info("âŒ No exact date match found in other files' caches")
            
        logger.info("âŒ No EXACT prediction cache found - only exact date matches are allowed")
        return None
        
    except Exception as e:
        logger.error(f"âŒ Error checking existing prediction: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def check_data_extension(old_file_path, new_file_path):
    """
    ìƒˆ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ ìˆœì°¨ì  í™•ì¥(ê¸°ì¡´ ë°ì´í„° ì´í›„ì—ë§Œ ìƒˆ í–‰ ì¶”ê°€)ì¸ì§€ ì—„ê²©í•˜ê²Œ í™•ì¸
    
    âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ê²½ìš°ë§Œ í™•ì¥ìœ¼ë¡œ ì¸ì •:
    1. ê¸°ì¡´ ë°ì´í„°ì™€ ì •í™•íˆ ë™ì¼í•œ ë¶€ë¶„ì´ ìˆìŒ
    2. ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ì—ë§Œ ì¶”ê°€ë¨
    3. ê¸°ì¡´ ë°ì´í„°ì˜ ì‹œì‘/ì¤‘ê°„ ë‚ ì§œê°€ ë³€ê²½ë˜ì§€ ì•ŠìŒ
    
    Returns:
    --------
    dict: {
        'is_extension': bool,
        'new_rows_count': int,
        'base_hash': str,  # ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì˜ í•´ì‹œ
        'old_start_date': str,
        'old_end_date': str,
        'new_start_date': str,
        'new_end_date': str,
        'validation_details': dict
    }
    """
    try:
                # íŒŒì¼ í˜•ì‹ì— ë§ê²Œ ë¡œë“œ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ë¡œë“œ)
        def load_file_safely(filepath, is_new_file=False):
            file_ext = os.path.splitext(filepath.lower())[1]
            if file_ext == '.csv':
                return pd.read_csv(filepath)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° pandasë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
                try:
                    # ëª¨ë“  ì‹œíŠ¸ëª… í™•ì¸í•˜ì—¬ ë°ì´í„°ê°€ ìˆëŠ” ì‹œíŠ¸ ì°¾ê¸°
                    excel_file = pd.ExcelFile(filepath, engine='openpyxl')
                    sheet_names = excel_file.sheet_names
                    logger.info(f"ğŸ” [EXTENSION_CHECK] Available sheets in {os.path.basename(filepath)}: {sheet_names}")
                    
                    # ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ì‹œíŠ¸ ì°¾ê¸° - íŠ¹ì • íŒ¨í„´ ìš°ì„  í™•ì¸
                    target_sheet_patterns = [
                        '29 Nov 2010 till todate',  # ì‹¤ì œ ë°ì´í„° ì‹œíŠ¸
                        'till todate',
                        'data'
                    ]
                    
                    target_sheet = None
                    # íŒ¨í„´ ë§¤ì¹­ ì‹œë„
                    for pattern in target_sheet_patterns:
                        for sheet_name in sheet_names:
                            if pattern.lower() in sheet_name.lower():
                                target_sheet = sheet_name
                                logger.info(f"ğŸ“‹ [EXTENSION_CHECK] Found target sheet by pattern '{pattern}': {target_sheet}")
                                break
                        if target_sheet:
                            break
                    
                    # íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•˜ë©´ ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
                    if not target_sheet:
                        target_sheet = sheet_names[0]
                        logger.info(f"ğŸ“‹ [EXTENSION_CHECK] Using first sheet: {target_sheet}")
                    
                    # ì„ íƒëœ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ë¡œë“œ
                    df = pd.read_excel(filepath, sheet_name=target_sheet, engine='openpyxl')
                    logger.info(f"ğŸ“Š [EXTENSION_CHECK] Loaded sheet '{target_sheet}': {df.shape}")
                    
                    # Date ì»¬ëŸ¼ ì°¾ê¸° ë° íŒŒì‹±
                    date_col = None
                    for col in df.columns:
                        if 'date' in str(col).lower() or col == 'Date':
                            date_col = col
                            break
                    
                    if date_col and len(df) > 0:
                        logger.info(f"ğŸ“… [EXTENSION_CHECK] Found date column: {date_col}")
                        # ê²¬ê³ í•œ ë‚ ì§œ íŒŒì‹± - ì˜ëª»ëœ í˜•ì‹ ì²˜ë¦¬
                        df['Date'] = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True, format='mixed')
                        # íŒŒì‹± ì‹¤íŒ¨í•œ ë‚ ì§œ ì œê±°
                        invalid_dates = df['Date'].isna().sum()
                        if invalid_dates > 0:
                            logger.warning(f"âš ï¸ [EXTENSION_CHECK] {invalid_dates}ê°œì˜ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì„ ë°œê²¬í•˜ì—¬ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
                            df = df.dropna(subset=['Date'])
                        
                        logger.info(f"ğŸ“… [EXTENSION_CHECK] Date range after parsing: {df['Date'].min()} ~ {df['Date'].max()}")
                        return df
                    else:
                        logger.warning(f"âš ï¸ [EXTENSION_CHECK] No date column found in {os.path.basename(filepath)}")
                        return df
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ [EXTENSION_CHECK] Failed to load Excel file {filepath}: {e}")
                    # ë¹ˆ DataFrame ë°˜í™˜ (í™•ì¥ ì²´í¬ ì‹¤íŒ¨)
                    return pd.DataFrame()
        
        logger.info(f"ğŸ” [EXTENSION_CHECK] Loading data files for comparison...")
        old_df = load_file_safely(old_file_path, is_new_file=False)
        new_df = load_file_safely(new_file_path, is_new_file=True)
        
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'Date' not in old_df.columns or 'Date' not in new_df.columns:
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'validation_details': {'error': 'No Date column found'}
            }
        
        # ë‚ ì§œë¡œ ì •ë ¬
        old_df = old_df.sort_values('Date').reset_index(drop=True)
        new_df = new_df.sort_values('Date').reset_index(drop=True)
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        old_df['Date'] = pd.to_datetime(old_df['Date'])
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        old_start_date = old_df['Date'].iloc[0]
        old_end_date = old_df['Date'].iloc[-1]
        new_start_date = new_df['Date'].iloc[0]
        new_end_date = new_df['Date'].iloc[-1]
        
        logger.info(f"ğŸ” [EXTENSION_CHECK] Old data: {old_start_date.strftime('%Y-%m-%d')} ~ {old_end_date.strftime('%Y-%m-%d')} ({len(old_df)} rows)")
        logger.info(f"ğŸ” [EXTENSION_CHECK] New data: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')} ({len(new_df)} rows)")
        
        # âœ… ê²€ì¦ 1: ìƒˆ íŒŒì¼ì´ ë” ê¸¸ì–´ì•¼ í•¨
        if len(new_df) <= len(old_df):
            logger.info(f"âŒ [EXTENSION_CHECK] New file is not longer ({len(new_df)} <= {len(old_df)})")
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New file is not longer than old file'}
            }
        
        # âœ… ê²€ì¦ 2: ìƒˆ íŒŒì¼ì´ ë” ê¸¸ê±°ë‚˜ ìµœì†Œí•œ ê°™ì€ ê¸¸ì´ì—¬ì•¼ í•¨ (ê³¼ê±° ë°ì´í„° í—ˆìš©)
        # ê³¼ê±° ë°ì´í„°ê°€ í¬í•¨ëœ ê²½ìš°ë„ í—ˆìš©í•˜ë„ë¡ ë³€ê²½
        logger.info(f"ğŸ“… [EXTENSION_CHECK] Date ranges - Old: {old_start_date} ~ {old_end_date}, New: {new_start_date} ~ {new_end_date}")
        
        # âœ… ê²€ì¦ 3: ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•¨ (ì™„í™”ëœ ì¡°ê±´)
        # ê³¼ê±° ë°ì´í„° í™•ì¥ ë˜ëŠ” ë¯¸ë˜ ë°ì´í„° í™•ì¥ ë‘˜ ë‹¤ í—ˆìš©
        has_more_data = (new_start_date < old_start_date) or (new_end_date > old_end_date) or (len(new_df) > len(old_df))
        if not has_more_data:
            logger.info(f"âŒ [EXTENSION_CHECK] New data doesn't provide additional information")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New data does not provide additional information beyond existing data'}
            }
        
        # âœ… ê²€ì¦ 4: ê¸°ì¡´ ë°ì´í„°ì˜ ëª¨ë“  ë‚ ì§œê°€ ìƒˆ ë°ì´í„°ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        old_dates = set(old_df['Date'].dt.strftime('%Y-%m-%d'))
        new_dates = set(new_df['Date'].dt.strftime('%Y-%m-%d'))
        
        missing_dates = old_dates - new_dates
        if missing_dates:
            logger.info(f"âŒ [EXTENSION_CHECK] Some old dates are missing in new data: {missing_dates}")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': f'Missing dates from old data: {list(missing_dates)}'}
            }
        
        # âœ… ê²€ì¦ 5: ì»¬ëŸ¼ì´ ë™ì¼í•´ì•¼ í•¨
        if list(old_df.columns) != list(new_df.columns):
            logger.info(f"âŒ [EXTENSION_CHECK] Column structure differs")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'Column structure differs'}
            }
        
        # âœ… ê²€ì¦ 6: ê¸°ì¡´ ë°ì´í„° ë¶€ë¶„ì´ ì •í™•íˆ ë™ì¼í•œì§€ í™•ì¸ (ê´€ëŒ€í•œ ì¡°ê±´ìœ¼ë¡œ ì™„í™”)
        logger.info(f"ğŸ” [EXTENSION_CHECK] Comparing overlapping data...")
        logger.info(f"  ğŸ“Š Checking {len(old_df)} existing dates...")
        
        # ğŸ”§ ê´€ëŒ€í•œ í™•ì¥ ê²€ì¦: ìƒ˜í”Œë§ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ ì¼ë¶€ë§Œ ê²€ì‚¬)
        sample_size = min(50, len(old_df))  # ìµœëŒ€ 50ê°œ ë‚ ì§œë§Œ ê²€ì‚¬
        sample_indices = list(range(0, len(old_df), max(1, len(old_df) // sample_size)))
        
        logger.info(f"  ğŸ”¬ Sampling {len(sample_indices)} dates out of {len(old_df)} for validation...")
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ ê° ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ìƒˆ ë°ì´í„° í–‰ ì°¾ê¸°
        data_matches = True
        mismatch_details = []
        checked_dates = 0
        mismatched_dates = 0
        allowed_mismatches = max(1, len(sample_indices) // 10)  # 10% ì •ë„ì˜ ë¯¸ìŠ¤ë§¤ì¹˜ëŠ” í—ˆìš©
        
        for idx in sample_indices:
            if idx >= len(old_df):
                continue
                
            old_row = old_df.iloc[idx]
            old_date = old_row['Date']
            old_date_str = old_date.strftime('%Y-%m-%d')
            checked_dates += 1
            
            # ìƒˆ ë°ì´í„°ì—ì„œ í•´ë‹¹ ë‚ ì§œ ì°¾ê¸°
            new_matching_rows = new_df[new_df['Date'] == old_date]
            
            if len(new_matching_rows) == 0:
                data_matches = False
                mismatch_details.append(f"Date {old_date_str} missing in new data")
                break
            elif len(new_matching_rows) > 1:
                data_matches = False
                mismatch_details.append(f"Duplicate date {old_date_str} in new data")
                break
            
            new_row = new_matching_rows.iloc[0]
            
            # ìˆ˜ì¹˜ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸) - ì™„í™”ëœ ì¡°ê±´
            numeric_cols = old_df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                old_val = old_row[col]
                new_val = new_row[col]
                
                # NaN ê°’ ì²˜ë¦¬
                if pd.isna(old_val) and pd.isna(new_val):
                    continue
                elif pd.isna(old_val) or pd.isna(new_val):
                    data_matches = False
                    mismatch_details.append(f"NaN mismatch on {old_date_str}, column {col}: {old_val} != {new_val}")
                    break
                
                # ìˆ˜ì¹˜ ë¹„êµ - ìƒëŒ€ì ìœ¼ë¡œ ê´€ëŒ€í•œ ì¡°ê±´ (0.01% ì˜¤ì°¨ í—ˆìš©)
                if not np.allclose([old_val], [new_val], rtol=1e-4, atol=1e-6, equal_nan=True):
                    # ì¶”ê°€ ê²€ì¦: ì •ìˆ˜ê°’ì´ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜ëœ ê²½ìš° í—ˆìš© (ì˜ˆ: 100 vs 100.0)
                    try:
                        if abs(float(old_val) - float(new_val)) < 1e-6:
                            continue
                    except:
                        pass
                    
                    mismatch_details.append(f"Value mismatch on {old_date_str}, column {col}: {old_val} != {new_val}")
                    mismatched_dates += 1
                    # ğŸ”§ ê´€ëŒ€í•œ ì¡°ê±´: ì¦‰ì‹œ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  í—ˆìš© í•œë„ê¹Œì§€ ê³„ì† ê²€ì‚¬
                    if mismatched_dates > allowed_mismatches:
                        data_matches = False
                        break
            
            if not data_matches:
                break
            
            # ë¬¸ìì—´ ì»¬ëŸ¼ ë¹„êµ (Date ì œì™¸) - ì™„í™”ëœ ì¡°ê±´
            str_cols = old_df.select_dtypes(include=['object']).columns
            str_cols = [col for col in str_cols if col != 'Date']
            for col in str_cols:
                old_str = str(old_row[col]).strip() if not pd.isna(old_row[col]) else ''
                new_str = str(new_row[col]).strip() if not pd.isna(new_row[col]) else ''
                
                if old_str != new_str:
                    mismatch_details.append(f"String mismatch on {old_date_str}, column {col}: '{old_str}' != '{new_str}'")
                    mismatched_dates += 1
                    # ğŸ”§ ê´€ëŒ€í•œ ì¡°ê±´: í—ˆìš© í•œë„ê¹Œì§€ ê³„ì† ê²€ì‚¬
                    if mismatched_dates > allowed_mismatches:
                        data_matches = False
                        break
            
            if not data_matches:
                break
        
        # ğŸ”§ ê´€ëŒ€í•œ ê²€ì¦ ê²°ê³¼ í‰ê°€
        logger.info(f"  âœ… Checked {checked_dates} sample dates, {mismatched_dates} mismatches found (allowed: {allowed_mismatches})")
        if mismatch_details:
            logger.info(f"  âš ï¸ Sample mismatches: {mismatch_details[:3]}...")
        
        if not data_matches:
            logger.info(f"âŒ [EXTENSION_CHECK] Too many data mismatches ({mismatched_dates} > {allowed_mismatches})")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {
                    'reason': f'Too many data mismatches: {mismatched_dates} > {allowed_mismatches}',
                    'mismatches_found': mismatched_dates,
                    'allowed_mismatches': allowed_mismatches,
                    'sample_details': mismatch_details[:5]
                }
            }
        elif mismatched_dates > 0:
            logger.info(f"âš ï¸ [EXTENSION_CHECK] Minor mismatches found but within tolerance ({mismatched_dates} <= {allowed_mismatches})")
        
        # âœ… ê²€ì¦ 7: ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° ë¶„ì„ (ê³¼ê±°/ë¯¸ë˜ ë°ì´í„° ëª¨ë‘ í—ˆìš©)
        new_only_dates = new_dates - old_dates
        
        # í™•ì¥ ìœ í˜• ë¶„ì„
        extension_type = []
        if new_start_date < old_start_date:
            past_dates = len([d for d in new_only_dates if pd.to_datetime(d) < old_start_date])
            extension_type.append(f"ê³¼ê±° ë°ì´í„° {past_dates}ê°œ ì¶”ê°€")
        if new_end_date > old_end_date:
            future_dates = len([d for d in new_only_dates if pd.to_datetime(d) > old_end_date])
            extension_type.append(f"ë¯¸ë˜ ë°ì´í„° {future_dates}ê°œ ì¶”ê°€")
        
        extension_desc = " + ".join(extension_type) if extension_type else "ë°ì´í„° ë³´ì™„"
        
        # âœ… ëª¨ë“  ê²€ì¦ í†µê³¼: ë°ì´í„° í™•ì¥ìœ¼ë¡œ ì¸ì • (ê³¼ê±°/ë¯¸ë˜ ëª¨ë‘ í—ˆìš©)
        new_rows_count = len(new_only_dates)
        base_hash = get_data_content_hash(old_file_path)
        
        logger.info(f"âœ… [EXTENSION_CHECK] Valid data extension: {extension_desc} (+{new_rows_count} new dates)")
        
        return {
            'is_extension': True,
            'new_rows_count': new_rows_count,
            'base_hash': base_hash,
            'old_start_date': old_start_date.strftime('%Y-%m-%d'),
            'old_end_date': old_end_date.strftime('%Y-%m-%d'),
            'new_start_date': new_start_date.strftime('%Y-%m-%d'),
            'new_end_date': new_end_date.strftime('%Y-%m-%d'),
            'validation_details': {
                'reason': f'Valid data extension: {extension_desc}',
                'new_dates_added': sorted(list(new_only_dates)),
                'extension_type': extension_type
            }
        }
        
    except Exception as e:
        logger.error(f"Data extension check failed: {str(e)}")
        return {
            'is_extension': False, 
            'new_rows_count': 0,
            'old_start_date': None,
            'old_end_date': None,
            'new_start_date': None,
            'new_end_date': None,
            'validation_details': {'error': str(e)}
        }

def find_existing_cache_range(file_path):
    """
    ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Returns:
    --------
    dict or None: {'start_date': 'YYYY-MM-DD', 'cutoff_date': 'YYYY-MM-DD'} ë˜ëŠ” None
    """
    try:
        # íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        
        if not predictions_dir.exists():
            return None
            
        # ìµœê·¼ ë©”íƒ€ íŒŒì¼ì—ì„œ ë°ì´í„° ë²”ìœ„ ì •ë³´ í™•ì¸
        meta_files = list(predictions_dir.glob("*_meta.json"))
        if not meta_files:
            return None
            
        # ê°€ì¥ ìµœê·¼ ë©”íƒ€ íŒŒì¼ ì„ íƒ
        latest_meta = max(meta_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_meta, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
            
        # ë°ì´í„° ë²”ìœ„ ì •ë³´ ì¶”ì¶œ
        model_config = meta_data.get('model_config', {})
        lstm_config = model_config.get('lstm', {})
        
        start_date = lstm_config.get('data_start_date')
        cutoff_date = lstm_config.get('data_cutoff_date') or meta_data.get('data_end_date')
        
        if start_date and cutoff_date:
            return {
                'start_date': start_date,
                'cutoff_date': cutoff_date,
                'meta_file': str(latest_meta)
            }
            
        return None
        
    except Exception as e:
        logger.warning(f"Failed to find cache range for {file_path}: {str(e)}")
        return None

def find_existing_cache_range(file_path):
    """
    ê¸°ì¡´ íŒŒì¼ì˜ ìºì‹œì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Returns:
    --------
    dict or None: {'start_date': 'YYYY-MM-DD', 'cutoff_date': 'YYYY-MM-DD'} ë˜ëŠ” None
    """
    try:
        # íŒŒì¼ì— ëŒ€ì‘í•˜ëŠ” ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        
        if not predictions_dir.exists():
            return None
            
        # ìµœê·¼ ë©”íƒ€ íŒŒì¼ì—ì„œ ë°ì´í„° ë²”ìœ„ ì •ë³´ í™•ì¸
        meta_files = list(predictions_dir.glob("*_meta.json"))
        if not meta_files:
            return None
            
        # ê°€ì¥ ìµœê·¼ ë©”íƒ€ íŒŒì¼ ì„ íƒ
        latest_meta = max(meta_files, key=lambda f: f.stat().st_mtime)
        
        with open(latest_meta, 'r', encoding='utf-8') as f:
            meta_data = json.load(f)
            
        # ë°ì´í„° ë²”ìœ„ ì •ë³´ ì¶”ì¶œ
        model_config = meta_data.get('model_config', {})
        lstm_config = model_config.get('lstm', {})
        
        start_date = lstm_config.get('data_start_date')
        cutoff_date = lstm_config.get('data_cutoff_date') or meta_data.get('data_end_date')
        
        if start_date and cutoff_date:
            return {
                'start_date': start_date,
                'cutoff_date': cutoff_date,
                'meta_file': str(latest_meta)
            }
            
        return None
        
    except Exception as e:
        logger.warning(f"Failed to find cache range for {file_path}: {str(e)}")
        return None

def migrate_legacy_cache_if_needed(file_path):
    """
    ì´ì „ íŒŒì¼ í•´ì‹œë¡œ ìƒì„±ëœ ìºì‹œë¥¼ ìƒˆë¡œìš´ ë°ì´í„° í•´ì‹œì™€ ì—°ê²°
    Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨/ì„±ê³µì— ë”°ë¥¸ í•´ì‹œ ì°¨ì´ ë¬¸ì œ í•´ê²°
    """
    try:
        current_hash = get_data_content_hash(file_path)
        if not current_hash:
            return False
            
        # íŒŒì¼ ê¸°ë°˜ í•´ì‹œë„ ê³„ì‚°
        file_hash = calculate_file_hash(file_path)
        if not file_hash:
            return False
            
        file_hash_short = file_hash[:16]
        
        # í˜„ì¬ í•´ì‹œì™€ íŒŒì¼ í•´ì‹œê°€ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œë„
        if current_hash == file_hash_short:
            return False
            
        logger.info(f"ğŸ”„ [CACHE_MIGRATION] Checking for legacy cache migration:")
        logger.info(f"  ğŸ“„ File: {os.path.basename(file_path)}")
        logger.info(f"  ğŸ”‘ Data hash: {current_hash}")
        logger.info(f"  ğŸ”‘ File hash: {file_hash_short}")
        
        # ê¸°ì¡´ ìºì‹œ ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        cache_root = Path(CACHE_ROOT_DIR)
        if not cache_root.exists():
            return False
            
        file_name = Path(file_path).stem
        
        # íŒŒì¼ í•´ì‹œë¡œ ëœ ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
        legacy_cache_dir = None
        for cache_dir in cache_root.iterdir():
            if cache_dir.is_dir() and cache_dir.name.startswith(file_hash_short):
                if file_name in cache_dir.name:
                    legacy_cache_dir = cache_dir
                    break
                    
        if not legacy_cache_dir or not legacy_cache_dir.exists():
            logger.info(f"ğŸ“‹ [CACHE_MIGRATION] No legacy cache found")
            return False
            
        # ìƒˆë¡œìš´ ë°ì´í„° í•´ì‹œë¡œ ëœ ìºì‹œ ë””ë ‰í† ë¦¬ëª…
        new_cache_dir_name = f"{current_hash}_{file_name}"
        new_cache_dir = cache_root / new_cache_dir_name
        
        if new_cache_dir.exists():
            logger.info(f"ğŸ“‹ [CACHE_MIGRATION] New cache already exists, no migration needed")
            return False
            
        # ìºì‹œ ë””ë ‰í† ë¦¬ ì´ë¦„ ë³€ê²½ (ë§ˆì´ê·¸ë ˆì´ì…˜)
        try:
            legacy_cache_dir.rename(new_cache_dir)
            logger.info(f"âœ… [CACHE_MIGRATION] Successfully migrated cache:")
            logger.info(f"  ğŸ“ From: {legacy_cache_dir.name}")
            logger.info(f"  ğŸ“ To: {new_cache_dir.name}")
            
            # ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì˜ í•´ì‹œ ì •ë³´ ì—…ë°ì´íŠ¸
            predictions_dir = new_cache_dir / 'predictions'
            if predictions_dir.exists():
                for meta_file in predictions_dir.glob("*_meta.json"):
                    try:
                        with open(meta_file, 'r', encoding='utf-8') as f:
                            meta_data = json.load(f)
                        
                        # í•´ì‹œ ì •ë³´ ì—…ë°ì´íŠ¸
                        meta_data['file_content_hash'] = current_hash
                        meta_data['migration_info'] = {
                            'original_hash': file_hash_short,
                            'migrated_to': current_hash,
                            'migration_date': datetime.now().isoformat()
                        }
                        
                        with open(meta_file, 'w', encoding='utf-8') as f:
                            json.dump(meta_data, f, indent=2, ensure_ascii=False)
                            
                        logger.info(f"ğŸ“ [CACHE_MIGRATION] Updated meta file: {meta_file.name}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ [CACHE_MIGRATION] Failed to update meta file {meta_file.name}: {e}")
            
            return True
            
        except Exception as rename_error:
            logger.error(f"âŒ [CACHE_MIGRATION] Failed to rename cache directory: {rename_error}")
            return False
            
    except Exception as e:
        logger.error(f"âŒ [CACHE_MIGRATION] Migration check failed: {e}")
        return False

def load_existing_predictions_for_extension(file_path, target_date, compatibility_info):
    """
    í™•ì¥ëœ ë°ì´í„°ì˜ ê¸°ì¡´ ìºì‹œì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì¸¡ë“¤ì„ ë¡œë“œ
    
    Args:
        file_path (str): í˜„ì¬ íŒŒì¼ ê²½ë¡œ
        target_date (pd.Timestamp): ëª©í‘œ ë‚ ì§œ
        compatibility_info (dict): í˜¸í™˜ì„± ì •ë³´
    
    Returns:
        list: ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ë“¤
    """
    try:
        if not compatibility_info.get('found'):
            return []
            
        # ê°œì„ ëœ í˜¸í™˜ì„± ì •ë³´ í™œìš©
        cache_files = compatibility_info.get('cache_files', [])
        predictions_dir_path = compatibility_info.get('compatibility_info', {}).get('predictions_dir', '')
        
        logger.info(f"ğŸ”„ [EXTENSION_CACHE] Using compatibility info:")
        logger.info(f"    Cache files: {len(cache_files)}")
        logger.info(f"    Predictions dir: {predictions_dir_path}")
        
        if not cache_files and not predictions_dir_path:
            logger.warning(f"âŒ [EXTENSION_CACHE] No cache files or predictions directory found")
            return []
            
        # ì§ì ‘ predictions ë””ë ‰í† ë¦¬ê°€ ì£¼ì–´ì§„ ê²½ìš° ë°”ë¡œ ì‚¬ìš©
        if predictions_dir_path and os.path.exists(predictions_dir_path):
            predictions_dir = Path(predictions_dir_path)
            logger.info(f"âœ… [EXTENSION_CACHE] Using direct predictions directory: {predictions_dir}")
        else:
            # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            original_cache_file = cache_files[0] if cache_files else file_path
            logger.info(f"ğŸ”„ [EXTENSION_CACHE] Loading existing predictions from: {os.path.basename(original_cache_file)}")
            
            # ì›ë³¸ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸°
            original_hash = get_data_content_hash(original_cache_file)
            if not original_hash:
                logger.warning(f"âš ï¸ [EXTENSION_CACHE] Cannot get hash for original file")
                return []
                
            cache_root = Path(CACHE_ROOT_DIR)
            original_file_name = Path(original_cache_file).stem
        
        # ğŸ”§ ê°•í™”ëœ ìºì‹œ ë””ë ‰í† ë¦¬ ì°¾ê¸° ë¡œì§
        logger.info(f"ğŸ” [EXTENSION_CACHE] Searching for cache directory with hash: {original_hash[:12]}")
        logger.info(f"ğŸ” [EXTENSION_CACHE] Original file name: {original_file_name}")
        
        original_cache_dir = None
        possible_dirs = []
        
        # ëª¨ë“  ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ í™•ì¸
        for cache_dir in cache_root.iterdir():
            if cache_dir.is_dir():
                logger.info(f"ğŸ” [EXTENSION_CACHE] Checking directory: {cache_dir.name}")
                
                # í•´ì‹œ ë§¤ì¹­ ì²´í¬
                hash_match = cache_dir.name.startswith(original_hash[:12])
                # íŒŒì¼ëª… ë§¤ì¹­ ì²´í¬ (ë” ìœ ì—°í•˜ê²Œ)
                name_match = (original_file_name in cache_dir.name or 
                             any(part in cache_dir.name for part in original_file_name.split('_')))
                
                logger.info(f"    Hash match: {hash_match}, Name match: {name_match}")
                
                if hash_match:
                    possible_dirs.append(cache_dir)
                    if name_match:
                        original_cache_dir = cache_dir
                        logger.info(f"âœ… [EXTENSION_CACHE] Found perfect match: {cache_dir.name}")
                        break
        
        # ì •í™•í•œ ë§¤ì¹˜ê°€ ì—†ìœ¼ë©´ í•´ì‹œë§Œ ë§¤ì¹­ë˜ëŠ” ê²ƒ ì¤‘ ì²« ë²ˆì§¸ ì‚¬ìš©
        if not original_cache_dir and possible_dirs:
            original_cache_dir = possible_dirs[0]
            logger.info(f"âš ï¸ [EXTENSION_CACHE] Using hash-only match: {original_cache_dir.name}")
        
        if not original_cache_dir:
            logger.warning(f"âŒ [EXTENSION_CACHE] No cache directory found for hash {original_hash[:12]}")
            logger.warning(f"âŒ [EXTENSION_CACHE] Available directories: {[d.name for d in cache_root.iterdir() if d.is_dir()]}")
            return []
        
        logger.info(f"âœ… [EXTENSION_CACHE] Using cache directory: {original_cache_dir.name}")
            
        predictions_dir = original_cache_dir / 'predictions'
        if not predictions_dir.exists():
            logger.warning(f"âŒ [EXTENSION_CACHE] Predictions directory not found: {predictions_dir}")
            return []
            
        # ğŸ”§ ê°•í™”ëœ ì˜ˆì¸¡ íŒŒì¼ ë¡œë“œ ë¡œì§
        logger.info(f"ğŸ“‚ [EXTENSION_CACHE] Predictions directory: {predictions_dir}")
        
        # ëª¨ë“  íŒŒì¼ ëª©ë¡ í™•ì¸
        all_files = list(predictions_dir.iterdir())
        logger.info(f"ğŸ“Š [EXTENSION_CACHE] Total files in predictions directory: {len(all_files)}")
        
        # CSV íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë¡œë“œ
        csv_files = list(predictions_dir.glob("prediction_start_*.cs"))
        json_files = list(predictions_dir.glob("prediction_start_*.json"))
        
        logger.info(f"ğŸ“Š [EXTENSION_CACHE] Found {len(csv_files)} CSV files and {len(json_files)} JSON files")
        logger.info(f"ğŸ“Š [EXTENSION_CACHE] CSV files: {[f.name for f in csv_files]}")
        
        existing_predictions = []
        
        # CSV íŒŒì¼ë“¤ ìš°ì„  ì²˜ë¦¬
        for csv_file in csv_files:
            try:
                logger.info(f"ğŸ“„ [EXTENSION_CACHE] Loading CSV file: {csv_file.name}")
                
                # CSV ë¡œë“œ
                df = pd.read_csv(csv_file)
                logger.info(f"ğŸ“Š [EXTENSION_CACHE] CSV shape: {df.shape}, columns: {list(df.columns)}")
                
                # ì»¬ëŸ¼ëª… ì •ê·œí™”
                if 'date' in df.columns:
                    df['Date'] = pd.to_datetime(df['date'])
                elif 'Date' in df.columns:
                    df['Date'] = pd.to_datetime(df['Date'])
                else:
                    logger.warning(f"âš ï¸ [EXTENSION_CACHE] No Date column found in {csv_file.name}")
                    continue
                    
                if 'prediction' in df.columns:
                    df['Prediction'] = df['prediction']
                elif 'Prediction' not in df.columns:
                    logger.warning(f"âš ï¸ [EXTENSION_CACHE] No Prediction column found in {csv_file.name}")
                    continue
                    
                # ğŸ”´ ë‚ ì§œ í•„í„°ë§ ë¡œì§ì„ ì œê±°í•˜ê³  ëª¨ë“  ì˜ˆì¸¡ì„ ë¡œë“œí•©ë‹ˆë‹¤.
                if 'Date' in df.columns:
                    # valid_predictions = df[df['Date'] <= target_date] # ì´ ì¤„ì„ ì‚­ì œí•˜ê±°ë‚˜ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
                    logger.info(f"ğŸ“Š [EXTENSION_CACHE] Loading all {len(df)} predictions from cached file (filter removed).")
                    
                    for _, row in df.iterrows(): # dfë¥¼ ì§ì ‘ ìˆœíšŒí•˜ë„ë¡ ë³€ê²½í•©ë‹ˆë‹¤.
                        try:
                            date_str = pd.to_datetime(row['Date']).strftime('%Y-%m-%d')
                        except:
                            date_str = str(row['Date'])
                        pred_value = float(row.get('Prediction') or row.get('prediction') or 0)
                        actual_value = row.get('Actual', row.get('actual', None))
                        
                        existing_predictions.append({
                            'Date': date_str,
                            'Prediction': pred_value,
                            'Actual': actual_value
                        })
                        
            except Exception as e:
                logger.warning(f"âŒ [EXTENSION_CACHE] Error loading {csv_file.name}: {e}")
                continue
        
        # CSV íŒŒì¼ì´ ì—†ìœ¼ë©´ JSON íŒŒì¼ë„ ì‹œë„
        if not existing_predictions and json_files:
            logger.info(f"ğŸ“„ [EXTENSION_CACHE] No data from CSV files, trying JSON files...")
            
            for json_file in json_files:
                try:
                    logger.info(f"ğŸ“„ [EXTENSION_CACHE] Loading JSON file: {json_file.name}")
                    
                    with open(json_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # JSON êµ¬ì¡°ì—ì„œ ì˜ˆì¸¡ ë°ì´í„° ì¶”ì¶œ
                    predictions_data = data.get('predictions', [])
                    if not predictions_data:
                        predictions_data = data.get('predictions_flat', [])
                    
                    logger.info(f"ğŸ“Š [EXTENSION_CACHE] JSON predictions count: {len(predictions_data)}")
                    
                    for pred in predictions_data:
                        pred_date = pd.to_datetime(pred.get('date') or pred.get('Date'))
                        if pred_date <= target_date:
                            existing_predictions.append({
                                'Date': pred_date.strftime('%Y-%m-%d'),
                                'Prediction': float(pred.get('prediction', 0)),
                                'Actual': pred.get('actual', None)
                            })
                            
                except Exception as e:
                    logger.warning(f"âŒ [EXTENSION_CACHE] Error loading JSON {json_file.name}: {e}")
                    continue
                
        # ë‚ ì§œìˆœ ì •ë ¬
        existing_predictions.sort(key=lambda x: x['Date'])
        
        logger.info(f"âœ… [EXTENSION_CACHE] Loaded {len(existing_predictions)} existing predictions")
        return existing_predictions
        
    except Exception as e:
        logger.error(f"âŒ [EXTENSION_CACHE] Failed to load existing predictions: {e}")
        return []

def find_compatible_predictions(current_file_path, prediction_start_date):
    """
    í˜„ì¬ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ í™•ì¥ì¸ ê²½ìš°, ê¸°ì¡´ íŒŒì¼ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    current_file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    prediction_start_date : str or pd.Timestamp
        ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ
        
    Returns:
    --------
    dict or None: {
        'predictions': list,
        'metadata': dict,
        'attention_data': dict,
        'ma_results': dict,
        'source_file': str,
        'extension_info': dict
    } ë˜ëŠ” None (í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°)
    """
    try:
        # uploads í´ë”ì˜ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ í™•ì¸
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
        logger.info(f"ğŸ” [PREDICTIONS_SEARCH] íƒìƒ‰í•  ê¸°ì¡´ íŒŒì¼ ìˆ˜: {len(existing_files)}")
        
        for existing_file in existing_files:
            try:
                # í™•ì¥ ê´€ê³„ í™•ì¸ + ë‹¨ìˆœ íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸
                extension_result = check_data_extension(str(existing_file), current_file_path)
                is_extension = extension_result.get('is_extension', False)
                
                # í™•ì¥ ê´€ê³„ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš° íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ ëŒ€ì²´ í™•ì¸
                if not is_extension:
                    existing_name = existing_file.stem.lower()
                    current_name = Path(current_file_path).stem.lower()
                    # ê¸°ë³¸ ì´ë¦„ì´ ê°™ê±°ë‚˜ í•˜ë‚˜ê°€ ë‹¤ë¥¸ í•˜ë‚˜ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if (existing_name in current_name or current_name in existing_name or 
                        existing_name.replace('_', '') == current_name.replace('_', '')):
                        is_extension = True
                        logger.info(f"ğŸ” [PREDICTIONS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ í™•ì¥ ê´€ê³„ ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                
                if is_extension:
                    if extension_result.get('is_extension', False):
                        logger.info(f"ğŸ” [PREDICTIONS_SEARCH] í™•ì¥ ê´€ê³„ ë°œê²¬: {existing_file.name} -> {Path(current_file_path).name}")
                        logger.info(f"    ğŸ“ˆ Extension type: {extension_result.get('validation_details', {}).get('extension_type', 'Unknown')}")
                        logger.info(f"    â• New rows: {extension_result.get('new_rows_count', 0)}")
                    else:
                        logger.info(f"ğŸ” [PREDICTIONS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„± ê¸°ë°˜ í˜¸í™˜ì„± ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                    
                    # ê¸°ì¡´ íŒŒì¼ì˜ ì˜ˆì¸¡ ê²°ê³¼ ìºì‹œ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_predictions_dir = existing_cache_dirs['predictions']
                    
                    if os.path.exists(existing_predictions_dir):
                        # í•´ë‹¹ ë‚ ì§œì˜ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                        if isinstance(prediction_start_date, str):
                            start_date = pd.to_datetime(prediction_start_date)
                        else:
                            start_date = prediction_start_date
                        
                        date_str = start_date.strftime('%Y%m%d')
                        
                        # íŒŒì¼ëª… íŒ¨í„´ ì‹œë„ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ìš°ì„  ì‚¬ìš©)
                        csv_patterns = [
                            f"prediction_start_{date_str}.cs",
                            f"prediction_{date_str}.cs"
                        ]
                        meta_patterns = [
                            f"prediction_start_{date_str}_meta.json",
                            f"prediction_{date_str}_meta.json"
                        ]
                        
                        csv_filepath = None
                        meta_filepath = None
                        
                        for csv_pattern, meta_pattern in zip(csv_patterns, meta_patterns):
                            csv_test = existing_predictions_dir / csv_pattern
                            meta_test = existing_predictions_dir / meta_pattern
                            
                            if csv_test.exists() and meta_test.exists():
                                csv_filepath = csv_test
                                meta_filepath = meta_test
                                break
                        
                        if csv_filepath and meta_filepath:
                            try:
                                # CSV ë¡œë“œ - ì•ˆì „í•œ fallback ì‚¬ìš©
                                from app.data.loader import load_csv_safe_with_fallback
                                predictions_df = load_csv_safe_with_fallback(csv_filepath)
                                
                                # ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬
                                if 'date' in predictions_df.columns:
                                    predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
                                elif 'Date' in predictions_df.columns:
                                    predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
                                
                                if 'prediction' in predictions_df.columns:
                                    predictions_df['Prediction'] = predictions_df['prediction']
                                
                                if 'prediction_from' in predictions_df.columns:
                                    predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
                                elif 'Prediction_From' in predictions_df.columns:
                                    predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
                                
                                predictions = predictions_df.to_dict('records')
                                
                                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                                with open(meta_filepath, 'r', encoding='utf-8') as f:
                                    metadata = json.load(f)
                                
                                # Attention ë°ì´í„° ë¡œë“œ
                                attention_data = {}
                                attention_patterns = [
                                    f"prediction_start_{date_str}_attention.json",
                                    f"prediction_{date_str}_attention.json"
                                ]
                                
                                for attention_pattern in attention_patterns:
                                    attention_filepath = existing_predictions_dir / attention_pattern
                                    if attention_filepath.exists():
                                        try:
                                            with open(attention_filepath, 'r', encoding='utf-8') as f:
                                                attention_raw = json.load(f)
                                            attention_data = {
                                                'image': attention_raw.get('image_base64', ''),
                                                'feature_importance': attention_raw.get('feature_importance', {}),
                                                'temporal_importance': attention_raw.get('temporal_importance', {})
                                            }
                                            break
                                        except Exception as e:
                                            logger.warning(f"âš ï¸ Failed to load attention data: {str(e)}")
                                
                                # ì´ë™í‰ê·  ë°ì´í„° ë¡œë“œ
                                ma_results = {}
                                ma_patterns = [
                                    f"prediction_start_{date_str}_ma.json",
                                    f"prediction_{date_str}_ma.json"
                                ]
                                
                                for ma_pattern in ma_patterns:
                                    ma_filepath = existing_predictions_dir / ma_pattern
                                    if ma_filepath.exists():
                                        try:
                                            with open(ma_filepath, 'r', encoding='utf-8') as f:
                                                ma_results = json.load(f)
                                            break
                                        except Exception as e:
                                            logger.warning(f"âš ï¸ Failed to load MA results: {str(e)}")
                                
                                logger.info(f"âœ… [PREDICTIONS_SEARCH] ê¸°ì¡´ íŒŒì¼ì—ì„œ í˜¸í™˜ ì˜ˆì¸¡ ê²°ê³¼ ë°œê²¬!")
                                logger.info(f"    ğŸ“ Source file: {existing_file.name}")
                                logger.info(f"    ğŸ“Š Predictions: {len(predictions)}")
                                logger.info(f"    ğŸ§  Attention data: {'Yes' if attention_data else 'No'}")
                                logger.info(f"    ğŸ“ˆ MA results: {'Yes' if ma_results else 'No'}")
                                
                                return {
                                    'predictions': predictions,
                                    'metadata': metadata,
                                    'attention_data': attention_data,
                                    'ma_results': ma_results,
                                    'source_file': str(existing_file),
                                    'extension_info': extension_result
                                }
                                
                            except Exception as e:
                                logger.warning(f"ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                        else:
                            logger.info(f"ğŸ” [PREDICTIONS_SEARCH] {start_date.strftime('%Y-%m-%d')} ë‚ ì§œì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"íŒŒì¼ í™•ì¥ ê´€ê³„ í™•ì¸ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                continue
        
        logger.info(f"âŒ [PREDICTIONS_SEARCH] í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.error(f"í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ ì°¾ê¸° ì‹¤íŒ¨: {str(e)}")
        return None

def find_compatible_hyperparameters(current_file_path, current_period):
    """
    í˜„ì¬ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ í™•ì¥ì¸ ê²½ìš°, ê¸°ì¡´ íŒŒì¼ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    current_file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    current_period : str
        í˜„ì¬ ì˜ˆì¸¡ ê¸°ê°„
        
    Returns:
    --------
    dict or None: {
        'hyperparams': dict,
        'source_file': str,
        'extension_info': dict
    } ë˜ëŠ” None (í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ê²½ìš°)
    """
    try:
        # uploads í´ë”ì˜ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ í™•ì¸ (ğŸ”§ ìˆ˜ì •: xlsx íŒŒì¼ë„ í¬í•¨)
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íƒìƒ‰í•  ê¸°ì¡´ íŒŒì¼ ìˆ˜: {len(existing_files)}")
        for i, file in enumerate(existing_files):
            logger.info(f"    {i+1}. {file.name}")
        
        for existing_file in existing_files:
            try:
                # ğŸ”§ ìˆ˜ì •: í™•ì¥ ê´€ê³„ í™•ì¸ + ë‹¨ìˆœ íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸
                extension_result = check_data_extension(str(existing_file), current_file_path)
                is_extension = extension_result.get('is_extension', False)
                
                # ğŸ“ í™•ì¥ ê´€ê³„ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš° íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ ëŒ€ì²´ í™•ì¸
                if not is_extension:
                    existing_name = existing_file.stem.lower()
                    current_name = Path(current_file_path).stem.lower()
                    # ê¸°ë³¸ ì´ë¦„ì´ ê°™ê±°ë‚˜ í•˜ë‚˜ê°€ ë‹¤ë¥¸ í•˜ë‚˜ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if (existing_name in current_name or current_name in existing_name or 
                        existing_name.replace('_', '') == current_name.replace('_', '')):
                        is_extension = True
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ í™•ì¥ ê´€ê³„ ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                
                if is_extension:
                    if extension_result.get('is_extension', False):
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] í™•ì¥ ê´€ê³„ ë°œê²¬: {existing_file.name} -> {Path(current_file_path).name}")
                        logger.info(f"    ğŸ“ˆ Extension type: {extension_result.get('validation_details', {}).get('extension_type', 'Unknown')}")
                        logger.info(f"    â• New rows: {extension_result.get('new_rows_count', 0)}")
                    else:
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„± ê¸°ë°˜ í˜¸í™˜ì„± ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                    
                    # ê¸°ì¡´ íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_models_dir = existing_cache_dirs['models']
                    
                    if os.path.exists(existing_models_dir):
                        # í•´ë‹¹ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ì°¾ê¸°
                        hyperparams_pattern = f"hyperparams_kfold_{current_period.replace('-', '_')}.json"
                        hyperparams_file = os.path.join(existing_models_dir, hyperparams_pattern)
                        
                        if os.path.exists(hyperparams_file):
                            try:
                                with open(hyperparams_file, 'r') as f:
                                    hyperparams = json.load(f)
                                
                                logger.info(f"âœ… [HYPERPARAMS_SEARCH] ê¸°ì¡´ íŒŒì¼ì—ì„œ í˜¸í™˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬!")
                                logger.info(f"    ğŸ“ Source file: {existing_file.name}")
                                logger.info(f"    ğŸ“Š Hyperparams file: {hyperparams_pattern}")
                                
                                return {
                                    'hyperparams': hyperparams,
                                    'source_file': str(existing_file),
                                    'extension_info': extension_result,
                                    'period': current_period
                                }
                                
                            except Exception as e:
                                logger.warning(f"ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                        else:
                            # âŒ ì‚­ì œëœ ë¶€ë¶„: ë‹¤ë¥¸ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ì²´ë¡œ ì‚¬ìš©í•˜ëŠ” ë¡œì§ ì œê±°
                            logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] {current_period} ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"íŒŒì¼ í™•ì¥ ê´€ê³„ í™•ì¸ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                continue
        
        logger.info(f"âŒ [HYPERPARAMS_SEARCH] í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± íƒìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None

def find_compatible_cache_file(new_file_path, intended_data_range=None, cached_df=None):
    """
    ìƒˆ íŒŒì¼ê³¼ í˜¸í™˜ë˜ëŠ” ê¸°ì¡´ ìºì‹œë¥¼ ì°¾ëŠ” í•¨ìˆ˜ (ë°ì´í„° ë²”ìœ„ ê³ ë ¤)
    
    ğŸ”§ í•µì‹¬ ê°œì„ :
    - íŒŒì¼ ë‚´ìš© + ì‚¬ìš© ë°ì´í„° ë²”ìœ„ë¥¼ ëª¨ë‘ ê³ ë ¤
    - ê°™ì€ íŒŒì¼ì´ë¼ë„ ë‹¤ë¥¸ ë°ì´í„° ë²”ìœ„ë©´ ìƒˆ ì˜ˆì¸¡ìœ¼ë¡œ ì¸ì‹
    - ì‚¬ìš©ì ì˜ë„ë¥¼ ë°˜ì˜í•œ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ë§¤ì¹­
    - ì¤‘ë³µ ë¡œë”© ë°©ì§€ë¥¼ ìœ„í•œ ìºì‹œëœ DataFrame ì¬ì‚¬ìš©
    
    Parameters:
    -----------
    new_file_path : str
        ìƒˆ íŒŒì¼ ê²½ë¡œ
    intended_data_range : dict, optional
        ì‚¬ìš©ìê°€ ì˜ë„í•œ ë°ì´í„° ë²”ìœ„ {'start_date': 'YYYY-MM-DD', 'cutoff_date': 'YYYY-MM-DD'}
    cached_df : DataFrame, optional
        ì´ë¯¸ ë¡œë”©ëœ DataFrame (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
    
    Returns:
    --------
    dict: {
        'found': bool,
        'cache_type': str,  # 'exact_with_range', 'extension', 'partial', 'range_mismatch'
        'cache_files': list,
        'compatibility_info': dict
    }
    """
    try:
        from app.data.loader import load_data
        # ğŸ”§ ìºì‹œëœ DataFrameì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ë¡œë”©
        if cached_df is not None:
            logger.info(f"ğŸ”„ [CACHE_OPTIMIZATION] Using cached DataFrame (avoiding duplicate load)")
            new_df = cached_df.copy()
        else:
            logger.info(f"ğŸ“ [CACHE_COMPATIBILITY] Loading data for cache check...")
            # ìƒˆ íŒŒì¼ì˜ ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
            file_ext = os.path.splitext(new_file_path.lower())[1]
            if file_ext == '.csv':
                new_df = pd.read_csv(new_file_path)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
                from app.data.loader import load_data
                new_df = load_data(new_file_path)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if new_df.index.name == 'Date':
                    new_df = new_df.reset_index()
        
        if 'Date' not in new_df.columns:
            return {'found': False, 'cache_type': None, 'reason': 'No Date column'}
            
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        new_start_date = new_df['Date'].min()
        new_end_date = new_df['Date'].max()
        new_hash = get_data_content_hash(new_file_path)
        
        logger.info(f"ğŸ” [ENHANCED_CACHE] Analyzing new file:")
        logger.info(f"  ğŸ“… Full date range: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')}")
        logger.info(f"  ğŸ“Š Records: {len(new_df)}")
        logger.info(f"  ğŸ”‘ Hash: {new_hash[:12] if new_hash else 'None'}...")
        
        # ì‚¬ìš©ì ì˜ë„ ë°ì´í„° ë²”ìœ„ í™•ì¸
        if intended_data_range:
            intended_start = pd.to_datetime(intended_data_range.get('start_date', new_start_date))
            intended_cutoff = pd.to_datetime(intended_data_range.get('cutoff_date', new_end_date))
            logger.info(f"  ğŸ¯ Intended range: {intended_start.strftime('%Y-%m-%d')} ~ {intended_cutoff.strftime('%Y-%m-%d')}")
        else:
            intended_start = new_start_date
            intended_cutoff = new_end_date
            logger.info(f"  ğŸ¯ Using full range (no specific intention provided)")
        
        compatible_caches = []
        
        # 1. uploads í´ë”ì˜ íŒŒì¼ë“¤ ê²€ì‚¬ (ë°ì´í„° ë²”ìœ„ ê³ ë ¤)
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = list(upload_dir.glob('*.csv')) + list(upload_dir.glob('*.xlsx')) + list(upload_dir.glob('*.xls'))
        
        logger.info(f"ğŸ” [ENHANCED_CACHE] Checking {len(existing_files)} upload files with range consideration...")
        
        for existing_file in existing_files:
            if existing_file.name == os.path.basename(new_file_path):
                continue
                
            try:
                # íŒŒì¼ í•´ì‹œ í™•ì¸
                existing_hash = get_data_content_hash(str(existing_file))
                if existing_hash == new_hash:
                    logger.info(f"ğŸ“„ [ENHANCED_CACHE] Same file content found: {existing_file.name}")
                    
                    # ğŸ”‘ ê°™ì€ íŒŒì¼ì´ì§€ë§Œ ë°ì´í„° ë²”ìœ„ ì˜ë„ í™•ì¸
                    # ê¸°ì¡´ ìºì‹œì˜ ë°ì´í„° ë²”ìœ„ ì •ë³´ë¥¼ ì°¾ì•„ì•¼ í•¨
                    existing_cache_range = find_existing_cache_range(str(existing_file))
                    
                    if existing_cache_range and intended_data_range:
                        cache_start = existing_cache_range.get('start_date')
                        cache_cutoff = existing_cache_range.get('cutoff_date') 
                        
                        if cache_start and cache_cutoff:
                            cache_start = pd.to_datetime(cache_start)
                            cache_cutoff = pd.to_datetime(cache_cutoff)
                            
                            # ë°ì´í„° ë²”ìœ„ ë¹„êµ
                            range_match = (
                                abs((intended_start - cache_start).days) <= 30 and 
                                abs((intended_cutoff - cache_cutoff).days) <= 30
                            )
                            
                            if range_match:
                                logger.info(f"âœ… [ENHANCED_CACHE] Exact match with same intended range!")
                                return {
                                    'found': True,
                                    'cache_type': 'exact_with_range',
                                    'cache_files': [str(existing_file)],
                                    'compatibility_info': {
                                        'match_type': 'file_hash_and_range',
                                        'cache_range': existing_cache_range,
                                        'intended_range': intended_data_range
                                    }
                                }
                            else:
                                logger.info(f"âš ï¸ [ENHANCED_CACHE] Same file but different intended range:")
                                logger.info(f"    ğŸ’¾ Cached range: {cache_start.strftime('%Y-%m-%d')} ~ {cache_cutoff.strftime('%Y-%m-%d')}")
                                logger.info(f"    ğŸ¯ Intended range: {intended_start.strftime('%Y-%m-%d')} ~ {intended_cutoff.strftime('%Y-%m-%d')}")
                                logger.info(f"    ğŸ”„ Will create new cache for different range")
                                # ê°™ì€ íŒŒì¼ì´ì§€ë§Œ ë‹¤ë¥¸ ë²”ìœ„ ì˜ë„ â†’ ìƒˆ ì˜ˆì¸¡ í•„ìš”
                                continue
                    
                    # ë²”ìœ„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë¡œì§ ì ìš©
                    logger.info(f"âœ… [ENHANCED_CACHE] Exact file match (no range info): {existing_file.name}")
                    return {
                        'found': True,
                        'cache_type': 'exact',
                        'cache_files': [str(existing_file)],
                        'compatibility_info': {'match_type': 'file_hash_only'}
                    }
                
                # í™•ì¥ íŒŒì¼ í™•ì¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€) - ë””ë²„ê¹… ê°•í™”
                logger.info(f"ğŸ” [EXTENSION_CHECK] Testing extension: {existing_file.name} â†’ {os.path.basename(new_file_path)}")
                extension_info = check_data_extension(str(existing_file), new_file_path)
                
                logger.info(f"ğŸ“Š [EXTENSION_RESULT] is_extension: {extension_info['is_extension']}")
                if extension_info.get('validation_details'):
                    logger.info(f"ğŸ“Š [EXTENSION_RESULT] reason: {extension_info['validation_details'].get('reason', 'N/A')}")
                
                if extension_info['is_extension']:
                    logger.info(f"ğŸ“ˆ [ENHANCED_CACHE] Found extension base: {existing_file.name} (+{extension_info.get('new_rows_count', 0)} rows)")
                    return {
                        'found': True,
                        'cache_type': 'extension', 
                        'cache_files': [str(existing_file)],
                        'compatibility_info': extension_info
                    }
                else:
                    logger.info(f"âŒ [EXTENSION_CHECK] Not an extension: {extension_info['validation_details'].get('reason', 'Unknown reason')}")
                    
            except Exception as e:
                logger.warning(f"Error checking upload file {existing_file}: {str(e)}")
                continue
        
        # 2. ğŸ”§ ê°•í™”ëœ ìºì‹œ ë””ë ‰í† ë¦¬ ê²€ì‚¬
        cache_root = Path(CACHE_ROOT_DIR)
        if not cache_root.exists():
            logger.info("âŒ [ENHANCED_CACHE] No cache directory found")
            return {'found': False, 'cache_type': None}
            
        logger.info(f"ğŸ” [ENHANCED_CACHE] Scanning cache directories at: {cache_root}")
        
        all_cache_dirs = list(cache_root.iterdir())
        valid_cache_dirs = [d for d in all_cache_dirs if d.is_dir()]
        
        logger.info(f"ğŸ“Š [ENHANCED_CACHE] Found {len(valid_cache_dirs)} cache directories")
        
        for file_cache_dir in valid_cache_dirs:
            logger.info(f"ğŸ” [ENHANCED_CACHE] Checking directory: {file_cache_dir.name}")
            
            predictions_dir = file_cache_dir / 'predictions'
            if not predictions_dir.exists():
                logger.info(f"    âŒ No predictions directory found")
                continue
                
            # predictions ë””ë ‰í† ë¦¬ ë‚´ íŒŒì¼ í™•ì¸
            pred_files = list(predictions_dir.iterdir())
            csv_files = [f for f in pred_files if f.suffix == '.csv']
            json_files = [f for f in pred_files if f.suffix == '.json']
            
            logger.info(f"    ğŸ“Š Found {len(csv_files)} CSV and {len(json_files)} JSON files")
            
            # ê°„ë‹¨í•œ í™•ì¥ ê°ì§€: íŒŒì¼ í•´ì‹œ ê¸°ë°˜
            if any(new_hash and new_hash[:12] in file_cache_dir.name for _ in [1] if new_hash):
                logger.info(f"    âœ… Hash match detected: {file_cache_dir.name}")
                
                # ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ ì¶”ì •
                cache_files = [str(f) for f in pred_files if f.suffix in ['.csv', '.xlsx', '.xls']]
                if cache_files:
                    compatible_caches.append({
                        'cache_dir': str(file_cache_dir),
                        'predictions_dir': str(predictions_dir),
                        'cache_files': cache_files,
                        'match_type': 'hash_based'
                    })
                    logger.info(f"    ğŸ“ Added to compatible caches")
                
                # predictions_index.cs íŒŒì¼ì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ë“¤ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
                index_file = predictions_dir / 'predictions_index.cs'
                if not index_file.exists():
                    continue
                
            try:
                cache_index = pd.read_csv(index_file)
                if 'data_end_date' not in cache_index.columns:
                    continue
                    
                cache_index['data_end_date'] = pd.to_datetime(cache_index['data_end_date'])
                cache_start = cache_index['data_end_date'].min()
                cache_end = cache_index['data_end_date'].max()
                
                logger.info(f"  ğŸ“ {file_cache_dir.name}: {cache_start.strftime('%Y-%m-%d')} ~ {cache_end.strftime('%Y-%m-%d')} ({len(cache_index)} predictions)")
                
                # ë‚ ì§œ ë²”ìœ„ ì¤‘ë³µ í™•ì¸
                overlap_start = max(new_start_date, cache_start)
                overlap_end = min(new_end_date, cache_end)
                
                if overlap_start <= overlap_end:
                    overlap_days = (overlap_end - overlap_start).days + 1
                    new_total_days = (new_end_date - new_start_date).days + 1
                    coverage_ratio = overlap_days / new_total_days
                    
                    logger.info(f"    ğŸ“Š Overlap: {overlap_days} days ({coverage_ratio:.1%} coverage)")
                    
                    if coverage_ratio >= 0.7:  # 70% ì´ìƒ ê²¹ì¹˜ë©´ í˜¸í™˜ ê°€ëŠ¥
                        compatible_caches.append({
                            'cache_dir': str(file_cache_dir),
                            'predictions_dir': str(predictions_dir),
                            'coverage_ratio': coverage_ratio,
                            'overlap_days': overlap_days,
                            'cache_range': (cache_start, cache_end),
                            'prediction_count': len(cache_index)
                        })
                        
            except Exception as e:
                logger.warning(f"Error analyzing cache {file_cache_dir.name}: {str(e)}")
                continue
        
        # 3. í˜¸í™˜ ê°€ëŠ¥í•œ ìºì‹œ ê²°ê³¼ ì²˜ë¦¬
        if compatible_caches:
            logger.info(f"ğŸ¯ [ENHANCED_CACHE] Found {len(compatible_caches)} compatible cache(s)")
            
            # í•´ì‹œ ê¸°ë°˜ ë§¤ì¹­ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            hash_based_caches = [c for c in compatible_caches if c.get('match_type') == 'hash_based']
            
            if hash_based_caches:
                best_cache = hash_based_caches[0]
                cache_type = 'hash_based'
                logger.info(f"  ğŸ¥‡ Using hash-based match: {Path(best_cache['cache_dir']).name}")
            else:
                # ì»¤ë²„ë¦¬ì§€ ë¹„ìœ¨ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
                compatible_caches.sort(key=lambda x: x.get('coverage_ratio', 0), reverse=True)
                best_cache = compatible_caches[0]
                
                if best_cache.get('coverage_ratio', 0) >= 0.95:  # 95% ì´ìƒì´ë©´ ê±°ì˜ ì™„ì „
                    cache_type = 'near_complete'
                elif len(compatible_caches) > 1:  # ì—¬ëŸ¬ ìºì‹œ ì¡°í•© ê°€ëŠ¥
                    cache_type = 'multi_cache' 
                else:
                    cache_type = 'partial'
                    
                logger.info(f"  ğŸ¥‡ Best: {Path(best_cache['cache_dir']).name} ({best_cache.get('coverage_ratio', 0):.1%} coverage)")
                
            return {
                'found': True,
                'cache_type': cache_type,
                'cache_files': [best_cache['predictions_dir']] if 'predictions_dir' in best_cache else best_cache.get('cache_files', []),
                'compatibility_info': {
                    'best_cache_dir': best_cache['cache_dir'],
                    'predictions_dir': best_cache.get('predictions_dir', ''),
                    'total_compatible_caches': len(compatible_caches),
                    'match_type': best_cache.get('match_type', 'coverage_based')
                }
            }
        
        logger.info("âŒ [ENHANCED_CACHE] No compatible cache found")
        return {'found': False, 'cache_type': None}
        
    except Exception as e:
        logger.error(f"Enhanced cache compatibility check failed: {str(e)}")
        return {'found': False, 'cache_type': None, 'error': str(e)}
    
def save_prediction_simple(prediction_results: dict, prediction_date):
    """ì™„ì „ í†µí•© ì €ì¥ì†Œì— ì €ì¥í•˜ëŠ” ìµœì¢… ë²„ì „ ğŸŒŸ"""
    try:
        # í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ í™•ì¸
        ensure_unified_predictions_dir()
        # í†µí•© ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        storage_dirs = get_unified_storage_dirs()
        
        preds_root = prediction_results.get("predictions")

        # â”€â”€ ì²« ì˜ˆì¸¡ ë ˆì½”ë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict) and preds_root:
            preds_seq = preds_root.get("future") or []
        else:                                   # list í˜¹ì€ None
            preds_seq = preds_root or prediction_results.get("predictions_flat", [])

        if not preds_seq:
            raise ValueError("prediction_results ì•ˆì— ì˜ˆì¸¡ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        first_rec = preds_seq[0]
        first_date = pd.to_datetime(first_rec.get("date") or first_rec.get("Date"))
        if pd.isna(first_date):
            raise ValueError("ì²« ì˜ˆì¸¡ ë ˆì½”ë“œì— ë‚ ì§œ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸŒŸ ì™„ì „ í†µí•© ì €ì¥ì†Œ ì‚¬ìš©
        predictions_dir = storage_dirs['predictions']
        hyperparameters_dir = storage_dirs['hyperparameters']
        plots_dir = storage_dirs['plots']
        
        # ğŸŒŸ í†µí•© ì €ì¥ì†Œ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        base_name = f"prediction_start_{first_date:%Y%m%d}"
        csv_path = predictions_dir / f"{base_name}.cs"
        meta_path = predictions_dir / f"{base_name}_meta.json"
        attention_path = predictions_dir / f"{base_name}_attention.json"
        ma_path = predictions_dir / f"{base_name}_ma.json"
        
        # ğŸ”§ ì¤‘ë³µ í™•ì¸ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€)
        if csv_path.exists() and meta_path.exists():
            logger.info(f"ğŸ”„ [UNIFIED_SAVE] Existing prediction found, preserving with timestamp")
            timestamp = datetime.now().strftime('%H%M%S')
            csv_path = predictions_dir / f"{base_name}_{timestamp}.cs"
            meta_path = predictions_dir / f"{base_name}_{timestamp}_meta.json"
            attention_path = predictions_dir / f"{base_name}_{timestamp}_attention.json"
            ma_path = predictions_dir / f"{base_name}_{timestamp}_ma.json"
            logger.info(f"  ğŸ“ Files will be saved with timestamp: {timestamp}")
        
        logger.info(f"ğŸŒŸ [UNIFIED_SAVE] Saving to unified storage:")
        logger.info(f"  ğŸ“„ Predictions: {csv_path.name}")
        logger.info(f"  ğŸ“„ Meta: {meta_path.name}")
        logger.info(f"  ğŸ“ Directory: {predictions_dir}")
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°/ëª¨ë¸ ì €ì¥ ê²½ë¡œë„ ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
        hyperparameter_path = hyperparameters_dir / f"hyperparameter_{first_date:%Y%m%d}.json"

        # â”€â”€ validation ê°œìˆ˜ ê³„ì‚° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(preds_root, dict):
            validation_cnt = len(preds_root.get("validation", []))
        else:
            validation_cnt = 0

        # â”€â”€ ë©”íƒ€ + ë³¸ë¬¸ êµ¬ì„± (íŒŒì¼ ìºì‹œ ì •ë³´ í¬í•¨) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        current_file_path = prediction_state.get('current_file', None)
        file_content_hash = get_data_content_hash(current_file_path) if current_file_path else None
        
        meta = {
            "prediction_start_date": first_date.strftime("%Y-%m-%d"),
            "data_end_date": str(prediction_date)[:10],
            "created_at": datetime.now().isoformat(),
            "semimonthly_period": prediction_results.get("semimonthly_period"),
            "next_semimonthly_period": prediction_results.get("next_semimonthly_period"),
            "selected_features": prediction_results.get("selected_features", []),
            "total_predictions": len(prediction_results.get("predictions_flat", preds_seq)),
            "validation_points": validation_cnt,
            "is_pure_future_prediction": prediction_results.get("summary", {}).get(
                "is_pure_future_prediction", validation_cnt == 0
            ),
            "metrics": prediction_results.get("metrics"),
            "interval_scores": prediction_results.get("interval_scores", {}),
            # ğŸ”‘ ì›ë³¸ íŒŒì¼ ì •ë³´ (ì°¸ì¡°ìš©)
            "source_file_path": current_file_path,
            "source_file_hash": file_content_hash,
            "model_type": prediction_results.get("model_type", "ImprovedLSTMPredictor"),
            "loss_function": prediction_results.get("loss_function", "DirectionalLoss"),
            "prediction_mode": "ì¼ë°˜ ëª¨ë“œ",
            # ğŸŒŸ í†µí•© ì €ì¥ì†Œ ì •ë³´
            "storage_system": "unified_complete",
            "storage_paths": {
                "predictions": str(csv_path),
                "metadata": str(meta_path),
                "attention": str(attention_path),
                "ma_results": str(ma_path),
                "hyperparameters": str(hyperparameter_path)
            }
        }

        # âœ… CSV íŒŒì¼ ì €ì¥ - NaN ê°’ ì•ˆì „ ì²˜ë¦¬ (í†µí•© ì €ì¥)
        predictions_data = clean_predictions_data(
            prediction_results.get("predictions_flat", preds_seq)
        )
        
        if predictions_data:
            # ğŸ”§ NaN ê°’ ì¶”ê°€ ì •ë¦¬
            for pred in predictions_data:
                for key, value in list(pred.items()):
                    pred[key] = safe_serialize_value(value)
            
            pred_df = pd.DataFrame(predictions_data)
            
            # ğŸŒŸ í†µí•© ì €ì¥ì†Œì— ì €ì¥
            pred_df.to_csv(csv_path, index=False)
            logger.info(f"âœ… [UNIFIED] CSV saved: {csv_path}")

        # âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ - NaN ê°’ ì•ˆì „ ì²˜ë¦¬ (í†µí•© ì €ì¥)
        safe_meta = {}
        for key, value in meta.items():
            if isinstance(value, dict):
                safe_meta[key] = {}
                for k, v in value.items():
                    if isinstance(v, dict):
                        safe_meta[key][k] = {}
                        for kk, vv in v.items():
                            safe_meta[key][k][kk] = safe_serialize_value(vv)
                    else:
                        safe_meta[key][k] = safe_serialize_value(v)
            else:
                safe_meta[key] = safe_serialize_value(value)
        
        # ğŸŒŸ í†µí•© ì €ì¥ì†Œì— ì €ì¥
        with open(meta_path, "w", encoding="utf-8") as fp:
            json.dump(safe_meta, fp, ensure_ascii=False, indent=2)
        logger.info(f"âœ… [UNIFIED] Metadata saved: {meta_path}")

        # âœ… Attention ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°) - NaN ê°’ ì•ˆì „ ì²˜ë¦¬ (í†µí•© ì €ì¥)
        attention_data = prediction_results.get("attention_data")
        if attention_data:
            attention_save_data = {
                "image_base64": safe_serialize_value(attention_data.get("image", "")),
                "feature_importance": {},
                "temporal_importance": {}
            }
            
            # feature_importance ì•ˆì „ ì²˜ë¦¬
            if attention_data.get("feature_importance"):
                for k, v in attention_data["feature_importance"].items():
                    attention_save_data["feature_importance"][k] = safe_serialize_value(v)
            
            # temporal_importance ì•ˆì „ ì²˜ë¦¬  
            if attention_data.get("temporal_importance"):
                for k, v in attention_data["temporal_importance"].items():
                    attention_save_data["temporal_importance"][k] = safe_serialize_value(v)
            
            # ğŸŒŸ í†µí•© ì €ì¥ì†Œì— ì €ì¥
            with open(attention_path, "w", encoding="utf-8") as fp:
                json.dump(attention_save_data, fp, ensure_ascii=False, indent=2)
            logger.info(f"âœ… [UNIFIED] Attention saved: {attention_path}")

        # âœ… ì´ë™í‰ê·  ë°ì´í„° ì €ì¥ (ìˆëŠ” ê²½ìš°) - NaN ê°’ ì•ˆì „ ì²˜ë¦¬ (í†µí•© ì €ì¥)
        ma_results = prediction_results.get("ma_results")
        ma_file = None
        
        if ma_results:
            try:
                # MA ê²°ê³¼ ì•ˆì „ ì²˜ë¦¬
                safe_ma_results = {}
                for window, results in ma_results.items():
                    safe_ma_results[str(window)] = []
                    for result in results:
                        if isinstance(result, dict):
                            safe_result = {}
                            for k, v in result.items():
                                safe_result[k] = safe_serialize_value(v)
                            safe_ma_results[str(window)].append(safe_result)
                        else:
                            safe_ma_results[str(window)].append(safe_serialize_value(result))
                
                # ğŸŒŸ í†µí•© ì €ì¥ì†Œì— ì €ì¥
                with open(ma_path, "w", encoding="utf-8") as fp:
                    json.dump(safe_ma_results, fp, ensure_ascii=False, indent=2)
                logger.info(f"âœ… [UNIFIED] MA results saved: {ma_path}")
                ma_file = str(ma_path)
                
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to save MA results: {str(e)}")
                logger.error(f"MA results error details: {traceback.format_exc()}")

        # âœ… í†µí•© ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_unified_predictions_index(safe_meta)
        
        logger.info(f"âœ… Complete unified prediction save â†’ start date: {meta['prediction_start_date']}")
        return {
            "success": True, 
            # ğŸŒŸ í†µí•© ì €ì¥ì†Œ íŒŒì¼ë“¤
            "csv_file": str(csv_path),
            "meta_file": str(meta_path),
            "attention_file": str(attention_path) if attention_data else None,
            "ma_file": ma_file,
            "hyperparameter_file": str(hyperparameter_path),
            # ê³µí†µ ì •ë³´
            "prediction_start_date": meta["prediction_start_date"],
            "preserved_existing": csv_path.name != f"{base_name}.csv",
            "prediction_count": len(preds_seq),
            "storage_system": "unified_complete",
            "storage_directories": {
                "predictions": str(predictions_dir),
                "hyperparameters": str(hyperparameters_dir),
                "plots": str(plots_dir)
            }
        }

    except Exception as e:
        logger.error(f"âŒ save_prediction_simple ì˜¤ë¥˜: {e}")
        logger.error(traceback.format_exc())
        return {"success": False, "error": str(e)}

# 2. Attention ë°ì´í„°ë¥¼ í¬í•¨í•œ ë¡œë“œ í•¨ìˆ˜
def load_prediction_simple(prediction_start_date):
    """
    ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ í•¨ìˆ˜
    """
    try:
        predictions_dir = Path(PREDICTIONS_DIR)
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        csv_filepath = predictions_dir / f"prediction_{date_str}.csv"
        meta_filepath = predictions_dir / f"prediction_{date_str}_meta.json"
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            # ğŸ”§ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ ì°¾ê¸° (í™•ì¥ëœ íŒŒì¼ì˜ ê²½ìš°)
            current_file = prediction_state.get('current_file')
            if current_file:
                logger.info(f"ğŸ” [PREDICTION_SIMPLE] í˜„ì¬ íŒŒì¼ ìºì‹œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ëŠ” ì¤‘...")
                compatible_predictions = find_compatible_predictions(current_file, prediction_start_date)
                
                if compatible_predictions:
                    logger.info(f"âœ… [PREDICTION_SIMPLE] í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                    logger.info(f"    ğŸ“ Source file: {os.path.basename(compatible_predictions['source_file'])}")
                    return {
                        'success': True,
                        'predictions': compatible_predictions['predictions'],
                        'metadata': compatible_predictions['metadata'],
                        'attention_data': compatible_predictions['attention_data'],
                        'source_info': {
                            'source_file': compatible_predictions['source_file'],
                            'extension_info': compatible_predictions['extension_info']
                        }
                    }
            
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ - ì•ˆì „í•œ fallback ì‚¬ìš©
        from app.data.loader import load_csv_safe_with_fallback
        predictions_df = load_csv_safe_with_fallback(csv_filepath)
        predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        if 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        logger.info(f"Simple prediction load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': {
                'image': metadata.get('attention_map', {}).get('image_base64', ''),
                'feature_importance': metadata.get('attention_map', {}).get('feature_importance', {}),
                'temporal_importance': metadata.get('attention_map', {}).get('temporal_importance', {})
            }
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction: {str(e)}")
        return {'success': False, 'error': str(e)}

def update_predictions_index_simple(metadata):
    """ë‹¨ìˆœí™”ëœ ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ - íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©"""
    try:
        # ğŸ”§ metadataê°€ Noneì¸ ê²½ìš° ì²˜ë¦¬
        if metadata is None:
            logger.warning("âš ï¸ [INDEX] metadataê°€ Noneì…ë‹ˆë‹¤. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
        cache_dirs = get_file_cache_dirs()
        predictions_index_file = cache_dirs['predictions'] / 'predictions_index.cs'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
        index_data = []
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                index_data = list(reader)
        
        # ì¤‘ë³µ ì œê±°
        prediction_start_date = metadata.get('prediction_start_date')
        if not prediction_start_date:
            logger.warning("âš ï¸ [INDEX] metadataì— prediction_start_dateê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        index_data = [row for row in index_data 
                     if row.get('prediction_start_date') != prediction_start_date]
        
        # metricsê°€ Noneì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        metrics = metadata.get('metrics') or {}
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€ (ğŸ”§ í•„ë“œëª… ìˆ˜ì •)
        new_row = {
            'prediction_start_date': metadata.get('prediction_start_date', ''),
            'data_end_date': metadata.get('data_end_date', ''),
            'created_at': metadata.get('created_at', ''),
            'semimonthly_period': metadata.get('semimonthly_period', ''),
            'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
            'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),  # ğŸ”§ ìˆ˜ì •
            'f1_score': metrics.get('f1', 0) if isinstance(metrics, dict) else 0,
            'accuracy': metrics.get('accuracy', 0) if isinstance(metrics, dict) else 0,
            'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else 0,
            'weighted_score': metrics.get('weighted_score', 0) if isinstance(metrics, dict) else 0
        }
        index_data.append(new_row)
        
        # ë‚ ì§œìˆœ ì •ë ¬ í›„ ì €ì¥
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        if index_data:
            fieldnames = new_row.keys()  # ğŸ”§ ì¼ê´€ëœ í•„ë“œëª… ì‚¬ìš©
            with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(index_data)
            
            logger.info(f"âœ… Predictions index updated successfully: {len(index_data)} entries")
            logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
            return True
        else:
            logger.warning("âš ï¸ No data to write to index file")
            return False
        
    except Exception as e:
        logger.error(f"âŒ Error updating simple predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def rebuild_predictions_index_from_existing_files():
    """
    ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼ë“¤ë¡œë¶€í„° predictions_index.csë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
    ğŸ”§ ëˆ„ì  ì˜ˆì¸¡ì´ ê¸°ì¡´ ë‹¨ì¼ ì˜ˆì¸¡ ìºì‹œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•¨
    """
    global predictions_index
    
    try:
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.warning("âš ï¸ No current file set, cannot rebuild index")
            return False
        
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        predictions_index_file = predictions_dir / 'predictions_index.cs'
        
        logger.info(f"ğŸ”„ Rebuilding predictions index from existing files in: {predictions_dir}")
        
        # ê¸°ì¡´ ë©”íƒ€ íŒŒì¼ë“¤ ì°¾ê¸°
        meta_files = list(predictions_dir.glob("*_meta.json"))
        logger.info(f"ğŸ“‹ Found {len(meta_files)} meta files")
        
        if not meta_files:
            logger.warning("âš ï¸ No meta files found to rebuild index")
            return False
        
        index_data = []
        
        for meta_file in meta_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # ì¸ë±ìŠ¤ ë ˆì½”ë“œ ìƒì„± (ë™ì¼í•œ í•„ë“œëª… ì‚¬ìš©)
                new_row = {
                    'prediction_start_date': metadata.get('prediction_start_date', ''),
                    'data_end_date': metadata.get('data_end_date', ''),
                    'created_at': metadata.get('created_at', ''),
                    'semimonthly_period': metadata.get('semimonthly_period', ''),
                    'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
                    'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
                    'f1_score': metadata.get('metrics', {}).get('f1', 0),
                    'accuracy': metadata.get('metrics', {}).get('accuracy', 0),
                    'mape': metadata.get('metrics', {}).get('mape', 0),
                    'weighted_score': metadata.get('metrics', {}).get('weighted_score', 0)
                }
                
                index_data.append(new_row)
                logger.info(f"  âœ… {meta_file.name}: {new_row['prediction_start_date']}")
                
            except Exception as e:
                logger.warning(f"  âš ï¸  Error reading {meta_file.name}: {str(e)}")
                continue
        
        if not index_data:
            logger.error("âŒ No valid metadata found")
            return False
        
        # ë‚ ì§œìˆœ ì •ë ¬
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        # CSV íŒŒì¼ ìƒì„±
        fieldnames = index_data[0].keys()
        
        with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(index_data)
        
        logger.info(f"âœ… Successfully rebuilt predictions_index.cs with {len(index_data)} entries")
        logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def rebuild_predictions_index_from_existing_files():
    """
    ê¸°ì¡´ ì˜ˆì¸¡ íŒŒì¼ë“¤ë¡œë¶€í„° predictions_index.csë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
    ğŸ”§ ëˆ„ì  ì˜ˆì¸¡ì´ ê¸°ì¡´ ë‹¨ì¼ ì˜ˆì¸¡ ìºì‹œë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•¨
    """
    try:
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.warning("âš ï¸ No current file set, cannot rebuild index")
            return False
        
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        predictions_index_file = predictions_dir / 'predictions_index.cs'
        
        logger.info(f"ğŸ”„ Rebuilding predictions index from existing files in: {predictions_dir}")
        
        # ê¸°ì¡´ ë©”íƒ€ íŒŒì¼ë“¤ ì°¾ê¸°
        meta_files = list(predictions_dir.glob("*_meta.json"))
        logger.info(f"ğŸ“‹ Found {len(meta_files)} meta files")
        
        if not meta_files:
            logger.warning("âš ï¸ No meta files found to rebuild index")
            return False
        
        index_data = []
        
        for meta_file in meta_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # ì¸ë±ìŠ¤ ë ˆì½”ë“œ ìƒì„± (ë™ì¼í•œ í•„ë“œëª… ì‚¬ìš©)
                new_row = {
                    'prediction_start_date': metadata.get('prediction_start_date', ''),
                    'data_end_date': metadata.get('data_end_date', ''),
                    'created_at': metadata.get('created_at', ''),
                    'semimonthly_period': metadata.get('semimonthly_period', ''),
                    'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
                    'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
                    'f1_score': metadata.get('metrics', {}).get('f1', 0),
                    'accuracy': metadata.get('metrics', {}).get('accuracy', 0),
                    'mape': metadata.get('metrics', {}).get('mape', 0),
                    'weighted_score': metadata.get('metrics', {}).get('weighted_score', 0)
                }
                
                index_data.append(new_row)
                logger.info(f"  âœ… {meta_file.name}: {new_row['prediction_start_date']}")
                
            except Exception as e:
                logger.warning(f"  âš ï¸  Error reading {meta_file.name}: {str(e)}")
                continue
        
        if not index_data:
            logger.error("âŒ No valid metadata found")
            return False
        
        # ë‚ ì§œìˆœ ì •ë ¬
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        # CSV íŒŒì¼ ìƒì„±
        fieldnames = index_data[0].keys()
        
        with open(predictions_index_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(index_data)
        
        logger.info(f"âœ… Successfully rebuilt predictions_index.cs with {len(index_data)} entries")
        logger.info(f"ğŸ“„ Index file: {predictions_index_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return False
    
def update_cached_prediction_actual_values(prediction_start_date, update_latest_only=True):
    """
    ìºì‹œëœ ì˜ˆì¸¡ì˜ ì‹¤ì œê°’ë§Œ ì„ íƒì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™”ëœ í•¨ìˆ˜
    
    Args:
        prediction_start_date: ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ
        update_latest_only: Trueë©´ ìµœì‹  ë°ì´í„°ë§Œ ì²´í¬í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
    
    Returns:
        dict: ì—…ë°ì´íŠ¸ ê²°ê³¼
    """
    try:
        from app.data.loader import load_data
        current_file = prediction_state.get('current_file')
        if not current_file:
            return {'success': False, 'error': 'No current file context available'}
        
        # ìºì‹œëœ ì˜ˆì¸¡ ë¡œë“œ (ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ì—†ì´)
        cached_result = load_prediction_with_attention_from_csv(prediction_start_date)
        if not cached_result['success']:
            return cached_result
        
        predictions = cached_result['predictions']
        
        # ë°ì´í„° ë¡œë“œ (ìºì‹œ í™œìš©)
        logger.info(f"ğŸ”„ [ACTUAL_UPDATE] Loading data for actual value update...")
        from app.data.loader import load_data
        df = load_data(current_file, use_cache=True)
        
        if df is None or df.empty:
            logger.warning(f"âš ï¸ [ACTUAL_UPDATE] Could not load data file")
            return {'success': False, 'error': 'Could not load data file'}
        
        last_data_date = df.index.max()
        updated_count = 0
        
        # ê° ì˜ˆì¸¡ì— ëŒ€í•´ ì‹¤ì œê°’ í™•ì¸ ë° ì„¤ì •
        for pred in predictions:
            pred_date = pd.to_datetime(pred['Date'])
            
            # ìµœì‹  ë°ì´í„°ë§Œ ì²´í¬í•˜ëŠ” ê²½ìš° ì„±ëŠ¥ ìµœì í™”
            if update_latest_only and pred_date < last_data_date - pd.Timedelta(days=30):
                continue
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, 'MOPJ']) and 
                pred_date <= last_data_date):
                actual_val = float(df.loc[pred_date, 'MOPJ'])
                pred['Actual'] = actual_val
                updated_count += 1
                logger.debug(f"  ğŸ“Š Updated actual value for {pred_date.strftime('%Y-%m-%d')}: {actual_val:.2f}")
            elif 'Actual' not in pred or pred['Actual'] is None:
                pred['Actual'] = None
        
        logger.info(f"âœ… [ACTUAL_UPDATE] Updated {updated_count} actual values")
        
        # ì—…ë°ì´íŠ¸ëœ ê²°ê³¼ ë°˜í™˜
        cached_result['predictions'] = predictions
        cached_result['actual_values_updated'] = True
        cached_result['updated_count'] = updated_count
        
        return cached_result
        
    except Exception as e:
        logger.error(f"âŒ [ACTUAL_UPDATE] Error updating actual values: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_prediction_from_csv(prediction_start_date_or_data_end_date):
    """
    í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ í•¨ìˆ˜ - ìë™ìœ¼ë¡œ ìƒˆë¡œìš´ í•¨ìˆ˜ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
    """
    logger.info("Using compatibility wrapper - redirecting to new smart cache function")
    return load_prediction_with_attention_from_csv(prediction_start_date_or_data_end_date)

# xlwings ëŒ€ì•ˆ ë¡œë” (ë³´ì•ˆí”„ë¡œê·¸ë¨ì´ íŒŒì¼ì„ ì ê·¸ëŠ” ê²½ìš° ì‚¬ìš©)
try:
    import xlwings as xw
    XLWINGS_AVAILABLE = True
    logger.info("âœ… xlwings library available - Excel security bypass enabled")
except ImportError:
    XLWINGS_AVAILABLE = False
    logger.warning("âš ï¸ xlwings not available - falling back to pandas only")

def load_prediction_with_attention_from_csv_in_dir(prediction_start_date, file_predictions_dir):
    """
    íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ì™€ attention ë°ì´í„°ë¥¼ í•¨ê»˜ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ë³„ ë””ë ‰í† ë¦¬ì—ì„œ íŒŒì¼ ê²½ë¡œ ì„¤ì • (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        csv_filepath = file_predictions_dir / f"prediction_start_{date_str}.cs"
        meta_filepath = file_predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = file_predictions_dir / f"prediction_start_{date_str}_attention.json"
        ma_filepath = file_predictions_dir / f"prediction_start_{date_str}_ma.json"
        
        logger.info(f"ğŸ“‚ Loading from file directory: {file_predictions_dir.name}")
        logger.info(f"  ğŸ“„ CSV: {csv_filepath.name}")
        
        if not csv_filepath.exists() or not meta_filepath.exists():
            logger.warning(f"  âŒ Required files missing in {file_predictions_dir.name}")
            
            # ğŸ”§ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ ì°¾ê¸° (í™•ì¥ëœ íŒŒì¼ì˜ ê²½ìš°)
            current_file = prediction_state.get('current_file')
            if current_file:
                logger.info(f"ğŸ” [PREDICTION_DIR] í˜„ì¬ íŒŒì¼ ìºì‹œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ëŠ” ì¤‘...")
                compatible_predictions = find_compatible_predictions(current_file, prediction_start_date)
                
                if compatible_predictions:
                    logger.info(f"âœ… [PREDICTION_DIR] í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                    logger.info(f"    ğŸ“ Source file: {os.path.basename(compatible_predictions['source_file'])}")
                    return {
                        'success': True,
                        'predictions': compatible_predictions['predictions'],
                        'metadata': compatible_predictions['metadata'],
                        'attention_data': compatible_predictions['attention_data'],
                        'ma_results': compatible_predictions['ma_results'],
                        'source_info': {
                            'source_file': compatible_predictions['source_file'],
                            'extension_info': compatible_predictions['extension_info']
                        }
                    }
            
            return {'success': False, 'error': f'Prediction files not found for {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV ë¡œë“œ - .cs íŒŒì¼ í˜¸í™˜ fallback ì‚¬ìš©
        from app.data.loader import load_csv_safe_with_fallback
        predictions_df = load_csv_safe_with_fallback(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
            predictions_df.drop('date', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
            predictions_df.drop('prediction', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
            predictions_df.drop('prediction_from', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        # actual ì»¬ëŸ¼ë„ í˜¸í™˜ì„± ì²˜ë¦¬
        if 'actual' in predictions_df.columns:
            predictions_df['Actual'] = pd.to_numeric(predictions_df['actual'], errors='coerce')
            predictions_df.drop('actual', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        
        logger.info(f"ğŸ“Š [CSV_DIR_LOAD] DataFrame processed: {predictions_df.shape}")
        logger.info(f"ğŸ“‹ [CSV_DIR_LOAD] Final columns: {list(predictions_df.columns)}")
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ Timestamp ê°ì²´ë“¤ì„ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì€ ëª¨ë‘ floatë¡œ ìœ ì§€
                    pred[key] = float(value)
                elif hasattr(value, 'item'):  # numpy scalars
                    pred[key] = value.item()
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (ì„ íƒì  - ì„±ëŠ¥ ìµœì í™”)
        # ğŸ’¡ ìºì‹œëœ ì˜ˆì¸¡ì„ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µ
        # í•„ìš”ì‹œì—ë§Œ ë³„ë„ APIë¡œ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        logger.info(f"ğŸ“¦ [CACHE_FAST] Skipping actual value update for faster cache loading")
        logger.info(f"ğŸ’¡ [CACHE_FAST] Use separate API endpoint if actual value update is needed")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ë¡œë“œ
        attention_data = {}
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    attention_raw = json.load(f)
                attention_data = {
                    'image': attention_raw.get('image_base64', ''),
                    'feature_importance': attention_raw.get('feature_importance', {}),
                    'temporal_importance': attention_raw.get('temporal_importance', {})
                }
                logger.info(f"  ğŸ§  Attention data loaded successfully")
                logger.info(f"  ğŸ§  Image data length: {len(attention_data['image']) if attention_data['image'] else 0}")
                logger.info(f"  ğŸ§  Feature importance keys: {len(attention_data['feature_importance'])}")
                logger.info(f"  ğŸ§  Temporal importance keys: {len(attention_data['temporal_importance'])}")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load attention data: {str(e)}")
                attention_data = {}
        
        # ì´ë™í‰ê·  ë°ì´í„° ë¡œë“œ
        ma_results = {}
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"  ğŸ“Š MA results loaded successfully")
            except Exception as e:
                logger.warning(f"  âš ï¸  Failed to load MA results: {str(e)}")
        
        logger.info(f"âœ… File directory cache load completed: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results
        }
        
    except Exception as e:
        logger.error(f"âŒ Error loading prediction from file directory: {str(e)}")
        return {'success': False, 'error': str(e)}

def load_prediction_with_attention_from_csv(prediction_start_date):
    """
    í†µí•© predictions í´ë”ì—ì„œ ìš°ì„  ë¡œë“œí•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜ ğŸŒŸ
    """
    try:
        # ğŸŒŸ í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ì—ì„œ ìš°ì„  ì‹œë„ (Primary)
        result = load_prediction_from_unified_storage(prediction_start_date)
        if result.get('success'):
            logger.info(f"âœ… [UNIFIED_LOAD] Successfully loaded from unified storage")
            return result
        
        # ğŸ¯ í´ë°±: íŒŒì¼ë³„ ìºì‹œì—ì„œ ë¡œë“œ ì‹œë„ (Secondary)
        logger.info(f"ğŸ”„ [FALLBACK_LOAD] Trying file-specific cache...")
        current_file = prediction_state.get('current_file')
        if not current_file:
            logger.error("âŒ No current file set in prediction_state")
            return {'success': False, 'error': 'No current file context available'}
            
        cache_dirs = get_file_cache_dirs(current_file)
        predictions_dir = cache_dirs['predictions']
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        csv_filepath = predictions_dir / f"prediction_start_{date_str}.cs"
        meta_filepath = predictions_dir / f"prediction_start_{date_str}_meta.json"
        attention_filepath = predictions_dir / f"prediction_start_{date_str}_attention.json"
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not csv_filepath.exists() or not meta_filepath.exists():
            # ğŸ”§ í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ ì°¾ê¸° (í™•ì¥ëœ íŒŒì¼ì˜ ê²½ìš°)
            logger.info(f"ğŸ” [PREDICTION_LOAD] í˜„ì¬ íŒŒì¼ ìºì‹œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì°¾ëŠ” ì¤‘...")
            compatible_predictions = find_compatible_predictions(current_file, prediction_start_date)
            
            if compatible_predictions:
                logger.info(f"âœ… [PREDICTION_LOAD] í˜¸í™˜ ê°€ëŠ¥í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                logger.info(f"    ğŸ“ Source file: {os.path.basename(compatible_predictions['source_file'])}")
                return {
                    'success': True,
                    'predictions': compatible_predictions['predictions'],
                    'metadata': compatible_predictions['metadata'],
                    'attention_data': compatible_predictions['attention_data'],
                    'ma_results': compatible_predictions['ma_results'],
                    'source_info': {
                        'source_file': compatible_predictions['source_file'],
                        'extension_info': compatible_predictions['extension_info']
                    }
                }
            else:
                return {
                    'success': False,
                    'error': f'Prediction files not found for start date {start_date.strftime("%Y-%m-%d")}'
                }
        
        # CSV íŒŒì¼ ì½ê¸° - ì•ˆì „í•œ fallback ì‚¬ìš©
        from app.data.loader import load_csv_safe_with_fallback
        predictions_df = load_csv_safe_with_fallback(csv_filepath)
        
        # ğŸ”§ ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬: ì†Œë¬¸ìë¡œ ì €ì¥ëœ ì»¬ëŸ¼ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # âœ… JSON ì§ë ¬í™”ë¥¼ ìœ„í•´ Timestamp ê°ì²´ë“¤ì„ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì€ ëª¨ë‘ floatë¡œ ìœ ì§€
                    pred[key] = float(value)
                elif hasattr(value, 'item'):  # numpy scalars
                    pred[key] = value.item()
        
        # âœ… ìºì‹œì—ì„œ ë¡œë“œí•  ë•Œ ì‹¤ì œê°’ ë‹¤ì‹œ ì„¤ì • (ì„ íƒì  - ì„±ëŠ¥ ìµœì í™”)
        # ğŸ’¡ ìºì‹œëœ ì˜ˆì¸¡ì„ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µ
        # í•„ìš”ì‹œì—ë§Œ ë³„ë„ APIë¡œ ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ìˆ˜í–‰
        logger.info(f"ğŸ“¦ [CACHE_FAST] Skipping actual value update for faster cache loading")
        logger.info(f"ğŸ’¡ [CACHE_FAST] Use separate API endpoint if actual value update is needed")
        
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        with open(meta_filepath, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        attention_data = None
        if attention_filepath.exists():
            try:
                with open(attention_filepath, 'r', encoding='utf-8') as f:
                    stored_attention = json.load(f)
                
                attention_data = {
                    'image': stored_attention.get('image_base64', ''),
                    'file_path': None,  # ì´ë¯¸ì§€ëŠ” base64ë¡œ ì €ì¥ë¨
                    'feature_importance': stored_attention.get('feature_importance', {}),
                    'temporal_importance': stored_attention.get('temporal_importance', {})
                }
                logger.info(f"Attention data loaded from: {attention_filepath}")
            except Exception as e:
                logger.warning(f"Failed to load attention data: {str(e)}")
                attention_data = None

        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        ma_filepath = predictions_dir / f"prediction_start_{date_str}_ma.json"
        ma_results = None
        if ma_filepath.exists():
            try:
                with open(ma_filepath, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"MA results loaded from: {ma_filepath} ({len(ma_results)} windows)")
            except Exception as e:
                logger.warning(f"Failed to load MA results: {str(e)}")
                ma_results = None
        
        logger.info(f"Complete prediction data loaded: {csv_filepath} ({len(predictions)} predictions)")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results,  # ğŸ”‘ ì´ë™í‰ê·  ë°ì´í„° ì¶”ê°€
            'prediction_start_date': start_date.strftime('%Y-%m-%d'),
            'data_end_date': metadata.get('data_end_date'),
            'semimonthly_period': metadata['semimonthly_period'],
            'next_semimonthly_period': metadata['next_semimonthly_period'],
            'metrics': metadata['metrics'],
            'interval_scores': metadata['interval_scores'],
            'selected_features': metadata['selected_features'],
            'has_cached_attention': attention_data is not None,
            'has_cached_ma': ma_results is not None  # ğŸ”‘ MA ìºì‹œ ì—¬ë¶€ ì¶”ê°€
        }
        
    except Exception as e:
        logger.error(f"Error loading prediction with attention: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def get_saved_predictions_list_for_file(file_path, limit=100):
    """
    íŠ¹ì • íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        predictions_list = []
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ êµ¬ì„±
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = Path(cache_dirs['predictions'])
        predictions_index_file = predictions_dir / 'predictions_index.cs'
        
        logger.info(f"ğŸ” [CACHE] Searching predictions in: {predictions_dir}")
        
        if predictions_index_file.exists():
            with open(predictions_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if len(predictions_list) >= limit:
                        break
                    
                    prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                    data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                    
                    if prediction_start_date and data_end_date:
                        pred_info = {
                            'prediction_start_date': prediction_start_date,
                            'data_end_date': data_end_date,
                            'prediction_date': data_end_date,
                            'first_prediction_date': prediction_start_date,
                            'created_at': row.get('created_at'),
                            'semimonthly_period': row.get('semimonthly_period'),
                            'next_semimonthly_period': row.get('next_semimonthly_period'),
                            'prediction_count': row.get('prediction_count'),
                            'actual_business_days': row.get('actual_business_days'),
                            'csv_file': row.get('csv_file'),
                            'meta_file': row.get('meta_file'),
                            'f1_score': float(row.get('f1_score', 0)),
                            'accuracy': float(row.get('accuracy', 0)),
                            'mape': float(row.get('mape', 0)),
                            'weighted_score': float(row.get('weighted_score', 0)),
                            'naming_scheme': row.get('naming_scheme', 'file_based'),
                            'source_file': os.path.basename(file_path),
                            'cache_system': 'file_based'
                        }
                        predictions_list.append(pred_info)
            
            logger.info(f"ğŸ¯ [CACHE] Found {len(predictions_list)} predictions in file-specific cache")
        else:
            logger.info(f"ğŸ“‚ [CACHE] No predictions index found in {predictions_index_file}")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        return predictions_list
        
    except Exception as e:
        logger.error(f"Error reading file-specific predictions list: {str(e)}")
        return []

def get_saved_predictions_list(limit=100):
    """
    í†µí•© predictions í´ë”ì—ì„œ ìš°ì„  ì¡°íšŒí•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜ ğŸŒŸ
    
    Parameters:
    -----------
    limit : int
        ë°˜í™˜í•  ìµœëŒ€ ê°œìˆ˜
    
    Returns:
    --------
    list : ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡
    """
    try:
        # ğŸŒŸ í†µí•© ì €ì¥ì†Œì—ì„œ ìš°ì„  ì¡°íšŒ (Primary)
        predictions_list = get_unified_predictions_list(limit)
        
        if len(predictions_list) > 0:
            logger.info(f"ğŸŒŸ [UNIFIED_LIST] Retrieved {len(predictions_list)} predictions from unified storage")
            return predictions_list
        
        # ğŸ¯ í´ë°±: íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ê²€ìƒ‰ (Secondary - í˜¸í™˜ì„±)
        logger.info(f"ğŸ”„ [FALLBACK_LIST] No unified predictions, trying file-based cache...")
        
        # 1. íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì˜ˆì¸¡ ê²€ìƒ‰
        cache_root = Path(CACHE_ROOT_DIR)
        if cache_root.exists():
            for file_dir in cache_root.iterdir():
                if not file_dir.is_dir():
                    continue
                
                predictions_dir = file_dir / 'predictions'
                predictions_index_file = predictions_dir / 'predictions_index.cs'
                
                if predictions_index_file.exists():
                    with open(predictions_index_file, 'r', encoding='utf-8') as f:
                        reader = csv.DictReader(f)
                        for row in reader:
                            if len(predictions_list) >= limit:
                                break
                            
                            prediction_start_date = row.get('prediction_start_date', row.get('first_prediction_date'))
                            data_end_date = row.get('data_end_date', row.get('prediction_base_date', row.get('prediction_date')))
                            
                            if prediction_start_date and data_end_date:
                                pred_info = {
                                    'prediction_start_date': prediction_start_date,
                                    'data_end_date': data_end_date,
                                    'prediction_date': data_end_date,
                                    'first_prediction_date': prediction_start_date,
                                    'created_at': row.get('created_at'),
                                    'semimonthly_period': row.get('semimonthly_period'),
                                    'next_semimonthly_period': row.get('next_semimonthly_period'),
                                    'prediction_count': row.get('prediction_count'),
                                    'actual_business_days': row.get('actual_business_days'),
                                    'csv_file': row.get('csv_file'),
                                    'meta_file': row.get('meta_file'),
                                    'f1_score': float(row.get('f1_score', 0)),
                                    'accuracy': float(row.get('accuracy', 0)),
                                    'mape': float(row.get('mape', 0)),
                                    'weighted_score': float(row.get('weighted_score', 0)),
                                    'naming_scheme': row.get('naming_scheme', 'file_based'),
                                    'source_file': file_dir.name,
                                    'cache_system': 'file_based',
                                    'storage_system': 'file_based'  # êµ¬ë¶„ì„ ìœ„í•´ ì¶”ê°€
                                }
                                predictions_list.append(pred_info)
        
        if len(predictions_list) == 0:
            logger.info("âŒ [FALLBACK_LIST] No predictions found in file-based cache system")
        else:
            logger.info(f"ğŸ¯ [FALLBACK_LIST] Retrieved {len(predictions_list)} predictions from file-based cache")
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
        predictions_list.sort(key=lambda x: x['data_end_date'], reverse=True)
        
        return predictions_list
        
    except Exception as e:
        logger.error(f"âŒ Error reading predictions list: {str(e)}")
        return []

def load_accumulated_predictions_from_csv(start_date, end_date=None, limit=None, file_path=None):
    """
    CSVì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¹ ë¥´ê²Œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    - ì£¼ì–´ì§„ file_pathì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ë‚´ì—ì„œë§Œ ê²€ìƒ‰í•˜ì—¬ ëª…í™•ì„± í™•ë³´
    """
    try:
        if not file_path:
            logger.warning("âš ï¸ load_accumulated_predictions_from_csv: file_path is required for accurate cache loading.")
            return []

        logger.info(f"ğŸ” [CACHE_LOAD] Loading predictions for '{os.path.basename(file_path)}' from {start_date} to {end_date or 'latest'}")

        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)

        # âœ… 1. í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ëª©ë¡ë§Œ ê°€ì ¸ì˜¤ê¸°
        predictions_list = get_saved_predictions_list_for_file(file_path, limit=1000)

        # âœ… 2. ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
        filtered_predictions_info = []
        for pred_info in predictions_list:
            data_end_date = pd.to_datetime(pred_info.get('data_end_date'))
            if data_end_date >= start_date and (end_date is None or data_end_date <= end_date):
                filtered_predictions_info.append(pred_info)
        
        logger.info(f"ğŸ“‹ [CACHE] Found {len(filtered_predictions_info)} matching prediction files in the specified date range.")

        # âœ… 3. ê° ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
        accumulated_results = []
        cache_dirs = get_file_cache_dirs(file_path)
        predictions_dir = cache_dirs['predictions']

        for pred_info in filtered_predictions_info:
            try:
                # data_end_dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡ ì‹œì‘ì¼ì„ ë‹¤ì‹œ ê³„ì‚°í•˜ì—¬ ì •í™•í•œ íŒŒì¼ ë¡œë“œ
                data_end_date = pd.to_datetime(pred_info.get('data_end_date'))
                prediction_start_date = data_end_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
                    prediction_start_date += pd.Timedelta(days=1)

                loaded_result = load_prediction_with_attention_from_csv_in_dir(prediction_start_date, predictions_dir)

                if loaded_result.get('success'):
                    metadata = loaded_result.get('metadata', {})
                    predictions = loaded_result.get('predictions', [])
                    
                    # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    accumulated_item = {
                        'date': data_end_date.strftime('%Y-%m-%d'),
                        'predictions': predictions,
                        'metrics': metadata.get('metrics', {}),
                        'interval_scores': metadata.get('interval_scores', {}),
                        # ... ê¸°íƒ€ í•„ìš”í•œ ì •ë³´ ...
                    }
                    accumulated_results.append(accumulated_item)
                else:
                    logger.warning(f"  âŒ [CACHE] Failed to load prediction for {data_end_date.strftime('%Y-%m-%d')}: {loaded_result.get('error')}")
            except Exception as e:
                logger.error(f"  âŒ Error loading individual prediction file: {str(e)}")
        
        logger.info(f"ğŸ¯ [CACHE] Successfully loaded {len(accumulated_results)} predictions from CSV cache files.")
        return accumulated_results

    except Exception as e:
        logger.error(f"Error loading accumulated predictions from CSV: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return []

def delete_saved_prediction(prediction_date):
    """
    ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    prediction_date : str or datetime
        ì‚­ì œí•  ì˜ˆì¸¡ ë‚ ì§œ
    
    Returns:
    --------
    dict : ì‚­ì œ ê²°ê³¼
    """
    try:
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(prediction_date, str):
            pred_date = pd.to_datetime(prediction_date)
        else:
            pred_date = prediction_date
        
        date_str = pred_date.strftime('%Y%m%d')
        
        # íŒŒì¼ ê²½ë¡œë“¤ (TARGET_DATE ë°©ì‹)
        csv_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}.csv")
        meta_filepath = os.path.join(PREDICTIONS_DIR, f"prediction_target_{date_str}_meta.json")
        
        # íŒŒì¼ ì‚­ì œ
        deleted_files = []
        if os.path.exists(csv_filepath):
            os.remove(csv_filepath)
            deleted_files.append(csv_filepath)
        
        if os.path.exists(meta_filepath):
            os.remove(meta_filepath)
            deleted_files.append(meta_filepath)
        
        # ğŸš« ë ˆê±°ì‹œ ì¸ë±ìŠ¤ ì œê±° ê¸°ëŠ¥ì€ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œì—ì„œ ì œê±°ë¨
        # íŒŒì¼ë³„ ìºì‹œì—ì„œëŠ” ê° íŒŒì¼ì˜ predictions_index.csê°€ ìë™ìœ¼ë¡œ ê´€ë¦¬ë¨
        logger.info("âš ï¸ Legacy delete_saved_prediction function called - not supported in file-based cache system")
        
        return {
            'success': True,
            'deleted_files': deleted_files,
            'message': f'Prediction for {pred_date.strftime("%Y-%m-%d")} deleted successfully'
        }
        
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }
    
def save_varmax_prediction(prediction_results: dict, prediction_date):
    """
    VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction save")
            return False
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        # JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        clean_results = {}
        for key, value in prediction_results.items():
            try:
                clean_results[key] = safe_serialize_value(value)
            except Exception as e:
                logger.warning(f"Failed to serialize {key}: {e}")
                continue
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        clean_results['metadata'] = {
            'prediction_date': prediction_date,
            'created_at': datetime.now().isoformat(),
            'file_path': file_path,
            'model_type': 'VARMAX'
        }
        
        # íŒŒì¼ì— ì €ì¥
        with open(prediction_file, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, ensure_ascii=False, indent=2)
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_varmax_predictions_index({
            'prediction_date': prediction_date,
            'file_path': str(prediction_file),
            'created_at': datetime.now().isoformat(),
            'original_file': file_path
        })
        
        logger.info(f"âœ… VARMAX prediction saved: {prediction_file}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to save VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return False

def load_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction load")
            return None
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        if not prediction_file.exists():
            logger.info(f"VARMAX prediction file not found: {prediction_file}")
            return None
            
        # íŒŒì¼ì—ì„œ ë¡œë“œ
        with open(prediction_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # ğŸ” ë¡œë“œëœ ë°ì´í„° íƒ€ì… ë° êµ¬ì¡° í™•ì¸
        logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data type: {type(results)}")
        if isinstance(results, dict):
            logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data keys: {list(results.keys())}")
            
            # ğŸ”§ ma_results í•„ë“œ íƒ€ì… í™•ì¸ ë° ìˆ˜ì •
            if 'ma_results' in results:
                ma_results = results['ma_results']
                logger.info(f"ğŸ” [VARMAX_LOAD] MA results type: {type(ma_results)}")
                
                if isinstance(ma_results, str):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results is string, attempting to parse as JSON...")
                    try:
                        results['ma_results'] = json.loads(ma_results)
                        logger.info(f"ğŸ”§ [VARMAX_LOAD] Successfully parsed ma_results from string to dict")
                    except Exception as e:
                        logger.error(f"âŒ [VARMAX_LOAD] Failed to parse ma_results string as JSON: {e}")
                        results['ma_results'] = {}
                elif not isinstance(ma_results, dict):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results has unexpected type: {type(ma_results)}, setting empty dict")
                    results['ma_results'] = {}
                    
        elif isinstance(results, str):
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Loaded data is string, not dict: {results[:100]}...")
            # ë¬¸ìì—´ì¸ ê²½ìš° ë‹¤ì‹œ JSON íŒŒì‹± ì‹œë„
            try:
                results = json.loads(results)
                logger.info(f"ğŸ”§ [VARMAX_LOAD] Re-parsed string as JSON: {type(results)}")
            except:
                logger.error(f"âŒ [VARMAX_LOAD] Failed to re-parse string as JSON")
                return None
        else:
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Unexpected data type: {type(results)}")
        
        logger.info(f"âœ… VARMAX prediction loaded: {prediction_file}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to load VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return None

def update_varmax_predictions_index(metadata):
    """
    VARMAX ì˜ˆì¸¡ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        index_file = varmax_dir / 'varmax_index.json'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
        else:
            index = {'predictions': []}
        
        # ìƒˆ ì˜ˆì¸¡ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        prediction_date = metadata['prediction_date']
        index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
        index['predictions'].append(metadata)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        index['predictions'].sort(key=lambda x: x['prediction_date'], reverse=True)
        
        # ìµœëŒ€ 100ê°œ ìœ ì§€
        index['predictions'] = index['predictions'][:100]
        
        # ì¸ë±ìŠ¤ ì €ì¥
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to update VARMAX predictions index: {e}")
        return False

def get_saved_varmax_predictions_list(limit=100):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX predictions list")
            return []
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        index_file = varmax_dir / 'varmax_index.json'
        
        if not index_file.exists():
            return []
            
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        predictions = index.get('predictions', [])[:limit]
        
        logger.info(f"âœ… Found {len(predictions)} saved VARMAX predictions")
        return predictions
        
    except Exception as e:
        logger.error(f"âŒ Failed to get saved VARMAX predictions list: {e}")
        return []

def delete_saved_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ì˜ˆì¸¡ íŒŒì¼ ì‚­ì œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        if prediction_file.exists():
            prediction_file.unlink()
        
        # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        index_file = varmax_dir / 'varmax_index.json'
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
            
            index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
            
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… VARMAX prediction deleted: {prediction_date}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete VARMAX prediction: {e}")
        return False

# âœ… ìœ„ì—ì„œ í†µí•© ì €ì¥ì†Œë¡œ í†µí•©ë¨

def load_prediction_from_unified_storage(prediction_start_date):
    """
    í†µí•© ì €ì¥ì†Œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ ğŸŒŸ
    """
    try:
        prediction_date = pd.to_datetime(prediction_start_date)
        base_name = f"prediction_start_{prediction_date:%Y%m%d}"
        
        storage_dirs = get_unified_storage_dirs()
        predictions_dir = storage_dirs['predictions']
        
        csv_path = predictions_dir / f"{base_name}.cs"
        meta_path = predictions_dir / f"{base_name}_meta.json"
        attention_path = predictions_dir / f"{base_name}_attention.json"
        ma_path = predictions_dir / f"{base_name}_ma.json"
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¶™ì€ íŒŒì¼ì´ ìˆëŠ”ì§€ë„ í™•ì¸
        if not csv_path.exists() or not meta_path.exists():
            # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ë¶™ì€ íŒŒì¼ë“¤ ì°¾ê¸°
            pattern_csv = f"{base_name}_*.cs"
            pattern_meta = f"{base_name}_*_meta.json"
            
            csv_files = list(predictions_dir.glob(pattern_csv))
            meta_files = list(predictions_dir.glob(pattern_meta))
            
            if csv_files and meta_files:
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
                csv_path = sorted(csv_files, key=lambda x: x.stat().st_mtime)[-1]
                # meta íŒŒì¼ëª…ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
                timestamp = csv_path.stem.split('_')[-1]
                if timestamp.isdigit() and len(timestamp) == 6:
                    meta_path = predictions_dir / f"{base_name}_{timestamp}_meta.json"
                    attention_path = predictions_dir / f"{base_name}_{timestamp}_attention.json"
                    ma_path = predictions_dir / f"{base_name}_{timestamp}_ma.json"
                else:
                    meta_path = sorted(meta_files, key=lambda x: x.stat().st_mtime)[-1]
                    
                logger.info(f"ğŸ” [UNIFIED_LOAD] Using timestamped files: {csv_path.name}")
        
        if not csv_path.exists() or not meta_path.exists():
            logger.warning(f"âš ï¸ [UNIFIED_LOAD] Prediction files not found: {base_name}")
            return None
        
        # CSV ë¡œë“œ
        from app.data.loader import load_csv_safe_with_fallback
        predictions_df = load_csv_safe_with_fallback(csv_path)
        
        # ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
            predictions_df.drop('date', axis=1, inplace=True)
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
            predictions_df.drop('prediction', axis=1, inplace=True)
        
        if 'actual' in predictions_df.columns:
            predictions_df['Actual'] = pd.to_numeric(predictions_df['actual'], errors='coerce')
            predictions_df.drop('actual', axis=1, inplace=True)
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
            predictions_df.drop('prediction_from', axis=1, inplace=True)
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        predictions = predictions_df.to_dict('records')
        
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì•ˆì „ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    pred[key] = float(value)
                elif hasattr(value, 'item'):
                    pred[key] = value.item()
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(meta_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ë¡œë“œ
        attention_data = None
        if attention_path.exists():
            try:
                with open(attention_path, 'r', encoding='utf-8') as f:
                    stored_attention = json.load(f)
                
                attention_data = {
                    'image': stored_attention.get('image_base64', ''),
                    'file_path': None,
                    'feature_importance': stored_attention.get('feature_importance', {}),
                    'temporal_importance': stored_attention.get('temporal_importance', {})
                }
                logger.info(f"âœ… [UNIFIED_LOAD] Attention data loaded")
            except Exception as e:
                logger.warning(f"âš ï¸ [UNIFIED_LOAD] Failed to load attention data: {str(e)}")
        
        # MA ê²°ê³¼ ë¡œë“œ
        ma_results = None
        if ma_path.exists():
            try:
                with open(ma_path, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"âœ… [UNIFIED_LOAD] MA results loaded ({len(ma_results)} windows)")
            except Exception as e:
                logger.warning(f"âš ï¸ [UNIFIED_LOAD] Failed to load MA results: {str(e)}")
        
        logger.info(f"âœ… [UNIFIED_LOAD] Loaded prediction: {len(predictions)} records from unified storage")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results,
            'storage_system': 'unified_complete',
            'source_files': {
                'csv': str(csv_path),
                'meta': str(meta_path),
                'attention': str(attention_path) if attention_path.exists() else None,
                'ma': str(ma_path) if ma_path.exists() else None
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_LOAD] Error loading prediction: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def update_unified_predictions_index(metadata):
    """í†µí•© ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ - app/predictions/predictions_index.cs"""
    try:
        # í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ í™•ì¸
        ensure_unified_predictions_dir()
        if metadata is None:
            logger.warning("âš ï¸ [UNIFIED_INDEX] metadataê°€ Noneì…ë‹ˆë‹¤. ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return False
            
        # í†µí•© ì €ì¥ì†Œ ê²½ë¡œ ì‚¬ìš©
        storage_dirs = get_unified_storage_dirs()
        unified_index_file = storage_dirs['predictions'] / 'predictions_index.cs'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ì½ê¸°
        index_data = []
        if unified_index_file.exists():
            with open(unified_index_file, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                index_data = list(reader)
        
        # ì¤‘ë³µ ì œê±°
        prediction_start_date = metadata.get('prediction_start_date')
        if not prediction_start_date:
            logger.warning("âš ï¸ [UNIFIED_INDEX] metadataì— prediction_start_dateê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        index_data = [row for row in index_data 
                     if row.get('prediction_start_date') != prediction_start_date]
        
        # metricsê°€ Noneì¼ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
        metrics = metadata.get('metrics') or {}
        
        # ìƒˆ ë°ì´í„° ì¶”ê°€
        new_row = {
            'prediction_start_date': metadata.get('prediction_start_date', ''),
            'data_end_date': metadata.get('data_end_date', ''),
            'created_at': metadata.get('created_at', ''),
            'semimonthly_period': metadata.get('semimonthly_period', ''),
            'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
            'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
            'f1_score': metrics.get('f1', 0) if isinstance(metrics, dict) else 0,
            'accuracy': metrics.get('accuracy', 0) if isinstance(metrics, dict) else 0,
            'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else 0,
            'weighted_score': metrics.get('weighted_score', 0) if isinstance(metrics, dict) else 0,
            'file_content_hash': metadata.get('file_content_hash', ''),
            'source_file': metadata.get('file_path', ''),
            'storage_system': metadata.get('storage_system', 'unified'),
            'unified_storage_path': metadata.get('unified_storage_path', ''),
            'file_cache_path': metadata.get('file_cache_path', '')
        }
        index_data.append(new_row)
        
        # ë‚ ì§œìˆœ ì •ë ¬ í›„ ì €ì¥
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        if index_data:
            fieldnames = new_row.keys()
            with open(unified_index_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(index_data)
            
            logger.info(f"âœ… [UNIFIED_INDEX] Updated successfully: {len(index_data)} entries")
            logger.info(f"ğŸ“„ [UNIFIED_INDEX] File: {unified_index_file}")
            return True
        else:
            logger.warning("âš ï¸ [UNIFIED_INDEX] No data to write to index file")
            return False
        
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_INDEX] Error updating unified index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def get_unified_predictions_list(limit=1000):
    from app.data.loader import load_csv_safe_with_fallback
    """
    í†µí•© ì¸ë±ìŠ¤ íŒŒì¼ì—ì„œ ì˜ˆì¸¡ ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (prediction_start_*.csì™€ ë™ì¼í•œ xlwings í¬í•¨ ë¡œì§ ì ìš©)
    """
    try:
        # ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ (.cs ì‚¬ìš©)
        unified_index_file = UNIFIED_PREDICTIONS_DIR / 'predictions_index.cs'
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸ ë° ì¬ìƒì„± ì‹œë„
        if not unified_index_file.exists():
            logger.warning(f"[UNIFIED_INDEX] Index file not found: {unified_index_file}. Rebuilding...")
            rebuild_unified_predictions_index()  # ì¬ìƒì„±
        
        # load_csv_safe_with_fallbackìœ¼ë¡œ .cs íŒŒì¼ ì½ê¸°
        logger.info(f"[UNIFIED_INDEX] Loading index file with xlwings fallback: {unified_index_file}")
        predictions_df = load_csv_safe_with_fallback(unified_index_file)
        
        # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ê¸°ì¡´ ë¡œì§ê³¼ í˜¸í™˜)
        predictions = predictions_df.to_dict('records')
        
        # .cs ì½ê¸° ì‹¤íŒ¨ ì‹œ ë ˆê±°ì‹œ .csv ëŒ€ì²´
        if not predictions:
            legacy_index_file = UNIFIED_PREDICTIONS_DIR / 'predictions_index.csv'
            if legacy_index_file.exists():
                logger.info(f"[UNIFIED_INDEX] Falling back to legacy .csv index file")
                predictions_df = load_csv_safe_with_fallback(legacy_index_file)
                predictions = predictions_df.to_dict('records')
        
        # ì •ë ¬ ë° ì œí•œ ì ìš© (ìµœì‹  ìˆœ)
        predictions.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        return predictions[:limit] if limit else predictions
    
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_INDEX] Error loading index: {str(e)}")
        logger.error(traceback.format_exc())
        return []

def rebuild_unified_predictions_index():
    """
    ê¸°ì¡´ ë©”íƒ€íŒŒì¼ë“¤ë¡œë¶€í„° í†µí•© ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ğŸŒŸ
    """
    try:
        # í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ í™•ì¸
        ensure_unified_predictions_dir()
        
        # ê¸°ì¡´ ë©”íƒ€íŒŒì¼ë“¤ ìˆ˜ì§‘ (JSON íŒŒì¼ë§Œ)
        meta_files = list(UNIFIED_PREDICTIONS_DIR.glob("*_meta.json"))
        logger.info(f"ğŸ“Š [UNIFIED_REBUILD] Found {len(meta_files)} meta files in unified storage")
        
        if not meta_files:
            logger.warning("[UNIFIED_REBUILD] No meta files found - nothing to rebuild")
            return False
        
        index_data = []
        
        for meta_file in meta_files:
            try:
                with open(meta_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                metrics = metadata.get('metrics', {})
                if not isinstance(metrics, dict):
                    metrics = {}
                
                new_row = {
                    'prediction_start_date': metadata.get('prediction_start_date', ''),
                    'data_end_date': metadata.get('data_end_date', ''),
                    'semimonthly_period': metadata.get('semimonthly_period', ''),
                    'next_semimonthly_period': metadata.get('next_semimonthly_period', ''),
                    'prediction_count': metadata.get('total_predictions', metadata.get('prediction_count', 0)),
                    'f1_score': metrics.get('f1', 0) if isinstance(metrics, dict) else 0,
                    'accuracy': metrics.get('accuracy', 0) if isinstance(metrics, dict) else 0,
                    'mape': metrics.get('mape', 0) if isinstance(metrics, dict) else 0,
                    'weighted_score': metrics.get('weighted_score', 0) if isinstance(metrics, dict) else 0,
                    'file_content_hash': metadata.get('file_content_hash', ''),
                    'source_file': metadata.get('file_path', ''),
                    'storage_system': metadata.get('storage_system', 'unified'),
                    'unified_storage_path': metadata.get('unified_storage_path', ''),
                    'file_cache_path': metadata.get('file_cache_path', '')
                }
                
                index_data.append(new_row)
                logger.info(f"  âœ… [UNIFIED_REBUILD] {meta_file.name}: {new_row['prediction_start_date']}")
                
            except Exception as e:
                logger.warning(f"  âš ï¸ [UNIFIED_REBUILD] Error reading {meta_file.name}: {str(e)}")
                continue
        
        if not index_data:
            logger.error("âŒ [UNIFIED_REBUILD] No valid metadata found")
            return False
        
        # ë‚ ì§œìˆœ ì •ë ¬
        index_data.sort(key=lambda x: x.get('data_end_date', ''), reverse=True)
        
        # CSV íŒŒì¼ ìƒì„± (í™•ì¥ìë¥¼ .csë¡œ ë³€ê²½)
        unified_index_file = UNIFIED_PREDICTIONS_DIR / 'predictions_index.cs'
        
        fieldnames = index_data[0].keys()
        
        with open(unified_index_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(index_data)
        
        logger.info(f"âœ… [UNIFIED_REBUILD] Successfully rebuilt unified index with {len(index_data)} entries")
        logger.info(f"ğŸ“„ [UNIFIED_REBUILD] Index file: {unified_index_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_REBUILD] Error rebuilding unified index: {str(e)}")
        logger.error(traceback.format_exc())
        return False

def load_prediction_from_unified_storage(prediction_start_date):
    """
    í†µí•© ì˜ˆì¸¡ ì €ì¥ì†Œì—ì„œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ ğŸŒŸ
    
    Parameters:
    -----------
    prediction_start_date : str or datetime
        ë¡œë“œí•  ì˜ˆì¸¡ ì‹œì‘ ë‚ ì§œ
        
    Returns:
    --------
    dict : ë¡œë“œ ê²°ê³¼
    """
    try:
        # í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ í™•ì¸
        ensure_unified_predictions_dir()
        
        if isinstance(prediction_start_date, str):
            start_date = pd.to_datetime(prediction_start_date)
        else:
            start_date = prediction_start_date
        
        date_str = start_date.strftime('%Y%m%d')
        
        # í†µí•© í´ë”ì˜ íŒŒì¼ ê²½ë¡œë“¤ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        unified_csv_path = UNIFIED_PREDICTIONS_DIR / f"prediction_start_{date_str}.cs"
        unified_meta_path = UNIFIED_PREDICTIONS_DIR / f"prediction_start_{date_str}_meta.json"
        unified_attention_path = UNIFIED_PREDICTIONS_DIR / f"prediction_start_{date_str}_attention.json"
        unified_ma_path = UNIFIED_PREDICTIONS_DIR / f"prediction_start_{date_str}_ma.json"
        
        logger.info(f"ğŸŒŸ [UNIFIED_LOAD] Loading from unified storage:")
        logger.info(f"  ğŸ“„ CSV: {unified_csv_path.name}")
        logger.info(f"  ğŸ“„ Meta: {unified_meta_path.name}")
        
        # í•„ìˆ˜ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not unified_csv_path.exists() or not unified_meta_path.exists():
            logger.info(f"âŒ [UNIFIED_LOAD] Required files not found in unified storage")
            return {'success': False, 'error': f'Prediction files not found for start date {start_date.strftime("%Y-%m-%d")}'}
        
        # CSV íŒŒì¼ ì½ê¸° - ì•ˆì „í•œ fallback ì‚¬ìš©
        from app.data.loader import load_csv_safe_with_fallback
        predictions_df = load_csv_safe_with_fallback(unified_csv_path)
        
        # ì»¬ëŸ¼ëª… í˜¸í™˜ì„± ì²˜ë¦¬ ë° ì¤‘ë³µ ì œê±°
        if 'date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['date'])
            predictions_df.drop('date', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        elif 'Date' in predictions_df.columns:
            predictions_df['Date'] = pd.to_datetime(predictions_df['Date'])
        
        if 'prediction' in predictions_df.columns:
            predictions_df['Prediction'] = predictions_df['prediction']
            predictions_df.drop('prediction', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        
        if 'prediction_from' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['prediction_from'])
            predictions_df.drop('prediction_from', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        elif 'Prediction_From' in predictions_df.columns:
            predictions_df['Prediction_From'] = pd.to_datetime(predictions_df['Prediction_From'])
        
        # actual ì»¬ëŸ¼ë„ í˜¸í™˜ì„± ì²˜ë¦¬
        if 'actual' in predictions_df.columns:
            # actual ê°’ì´ ìˆ«ìì¸ì§€ í™•ì¸í•˜ê³  ë³€í™˜
            predictions_df['Actual'] = pd.to_numeric(predictions_df['actual'], errors='coerce')
            predictions_df.drop('actual', axis=1, inplace=True)  # ì›ë³¸ ì†Œë¬¸ì ì»¬ëŸ¼ ì œê±°
        
        logger.info(f"ğŸ“Š [UNIFIED_LOAD] DataFrame processed: {predictions_df.shape}")
        logger.info(f"ğŸ“‹ [UNIFIED_LOAD] Final columns: {list(predictions_df.columns)}")
        
        predictions = predictions_df.to_dict('records')
        
        # JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ì•ˆì „í•œ ë³€í™˜
        for pred in predictions:
            for key, value in list(pred.items()):
                if pd.isna(value):
                    pred[key] = None
                elif isinstance(value, pd.Timestamp):
                    pred[key] = value.strftime('%Y-%m-%d')
                elif isinstance(value, (np.int64, np.float64)):
                    pred[key] = float(value)
                elif hasattr(value, 'item'):
                    pred[key] = value.item()
        
        # ë©”íƒ€ë°ì´í„° ì½ê¸°
        with open(unified_meta_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Attention ë°ì´í„° ì½ê¸° (ìˆëŠ” ê²½ìš°)
        attention_data = None
        if unified_attention_path.exists():
            try:
                with open(unified_attention_path, 'r', encoding='utf-8') as f:
                    stored_attention = json.load(f)
                
                attention_data = {
                    'image': stored_attention.get('image_base64', ''),
                    'file_path': None,  # ì´ë¯¸ì§€ëŠ” base64ë¡œ ì €ì¥ë¨
                    'feature_importance': stored_attention.get('feature_importance', {}),
                    'temporal_importance': stored_attention.get('temporal_importance', {})
                }
                logger.info(f"  ğŸ§  Attention data loaded from unified storage")
            except Exception as e:
                logger.warning(f"  âš ï¸ Failed to load attention data: {str(e)}")
                attention_data = None

        # MA ê²°ê³¼ ì½ê¸° (ìˆëŠ” ê²½ìš°)
        ma_results = None
        if unified_ma_path.exists():
            try:
                with open(unified_ma_path, 'r', encoding='utf-8') as f:
                    ma_results = json.load(f)
                logger.info(f"  ğŸ“Š MA results loaded from unified storage ({len(ma_results)} windows)")
            except Exception as e:
                logger.warning(f"  âš ï¸ Failed to load MA results: {str(e)}")
                ma_results = None
        
        logger.info(f"âœ… [UNIFIED_LOAD] Complete unified prediction data loaded: {len(predictions)} predictions")
        
        return {
            'success': True,
            'predictions': predictions,
            'metadata': metadata,
            'attention_data': attention_data,
            'ma_results': ma_results,
            'prediction_start_date': start_date.strftime('%Y-%m-%d'),
            'data_end_date': metadata.get('data_end_date'),
            'semimonthly_period': metadata.get('semimonthly_period'),
            'next_semimonthly_period': metadata.get('next_semimonthly_period'),
            'metrics': metadata.get('metrics'),
            'interval_scores': metadata.get('interval_scores'),
            'selected_features': metadata.get('selected_features'),
            'has_cached_attention': attention_data is not None,
            'has_cached_ma': ma_results is not None,
            'storage_system': metadata.get('storage_system', 'unified'),
            'loaded_from': 'unified_storage'
        }
        
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_LOAD] Error loading from unified storage: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e),
            'loaded_from': 'unified_storage_failed'
        }

def load_accumulated_predictions_from_csv(start_date, end_date=None, limit=None, file_path=None):
    """
    í†µí•© predictions í´ë”ì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” ê°œì„ ëœ í•¨ìˆ˜ ğŸŒŸ
    
    Parameters:
    -----------
    start_date : str or datetime
        ì‹œì‘ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    end_date : str or datetime, optional
        ì¢…ë£Œ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    limit : int, optional
        ìµœëŒ€ ë¡œë“œí•  ì˜ˆì¸¡ ê°œìˆ˜
    file_path : str, optional
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€í•˜ì§€ë§Œ í†µí•© ì €ì¥ì†Œì—ì„œëŠ” ë¬´ì‹œ)
    
    Returns:
    --------
    list : ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # í†µí•© ì˜ˆì¸¡ ë””ë ‰í† ë¦¬ í™•ì¸
        ensure_unified_predictions_dir()
        logger.info(f"ğŸŒŸ [UNIFIED_ACCUMULATED] Loading predictions from {start_date} to {end_date or 'latest'}")
        
        # ë‚ ì§œ í˜•ì‹ í†µì¼
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        if end_date and isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)
        
        # ğŸŒŸ í†µí•© ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (Primary)
        all_predictions = get_unified_predictions_list(limit=1000)
        logger.info(f"ğŸŒŸ [UNIFIED_ACCUMULATED] Found {len(all_predictions)} predictions in unified storage")
        
        # ğŸ¯ í´ë°±: ê¸°ì¡´ íŒŒì¼ë³„ ìºì‹œì—ì„œë„ ê²€ìƒ‰ (Secondary - í˜¸í™˜ì„±)
        if len(all_predictions) == 0 and file_path:
            logger.info(f"ğŸ”„ [FALLBACK_ACCUMULATED] No unified predictions, trying file-specific cache...")
            try:
                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
                all_predictions = get_saved_predictions_list_for_file(file_path, limit=1000)
                logger.info(f"ğŸ¯ [FALLBACK_ACCUMULATED] Found {len(all_predictions)} predictions in file cache")
            except Exception as e:
                logger.warning(f"âš ï¸ [FALLBACK_ACCUMULATED] Error in file-specific search: {str(e)}")
                return []
        
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        filtered_predictions = []
        for pred_info in all_predictions:
            # ì¸ë±ìŠ¤ì—ì„œ ë°ì´í„° ê¸°ì¤€ì¼ í™•ì¸
            data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
            
            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            if data_end_date >= start_date:
                if end_date is None or data_end_date <= end_date:
                    filtered_predictions.append(pred_info)
            
            # ì œí•œ ê°œìˆ˜ í™•ì¸
            if limit and len(filtered_predictions) >= limit:
                break
        
        logger.info(f"ğŸ“‹ [UNIFIED_ACCUMULATED] Found {len(filtered_predictions)} matching prediction files in date range")
        if len(filtered_predictions) > 0:
            logger.info(f"ğŸ“… [UNIFIED_ACCUMULATED] Available cached dates:")
            for pred in filtered_predictions:
                data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
                logger.info(f"    - {data_end_date}")
        
        # ê° ì˜ˆì¸¡ ê²°ê³¼ ë¡œë“œ
        accumulated_results = []
        for i, pred_info in enumerate(filtered_predictions):
            try:
                # ë°ì´í„° ê¸°ì¤€ì¼ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                data_end_date = pd.to_datetime(pred_info.get('data_end_date', pred_info.get('prediction_date')))
                
                # ë°ì´í„° ê¸°ì¤€ì¼ë¡œë¶€í„° ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
                prediction_start_date = data_end_date + pd.Timedelta(days=1)
                while prediction_start_date.weekday() >= 5:  # ì£¼ë§ ìŠ¤í‚µ
                    prediction_start_date += pd.Timedelta(days=1)
                
                # ğŸŒŸ í†µí•© ì €ì¥ì†Œì—ì„œ ë¡œë“œ ì‹œë„ (Primary)
                loaded_result = load_prediction_from_unified_storage(prediction_start_date)
                
                # ğŸ¯ í´ë°±: íŒŒì¼ë³„ ìºì‹œì—ì„œ ë¡œë“œ (Secondary)
                if not loaded_result.get('success') and file_path:
                    cache_dirs = get_file_cache_dirs(file_path)
                    loaded_result = load_prediction_with_attention_from_csv_in_dir(prediction_start_date, cache_dirs['predictions'])
                
                if loaded_result.get('success'):
                    logger.info(f"  âœ… [UNIFIED_ACCUMULATED] Successfully loaded cached prediction for {data_end_date.strftime('%Y-%m-%d')}")
                    
                    # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
                    predictions = loaded_result.get('predictions', [])
                    
                    # ì˜ˆì¸¡ ë°ì´í„°ê°€ ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ êµ¬ì¡°ì¸ ê²½ìš° ì²˜ë¦¬
                    if isinstance(predictions, dict):
                        if 'future' in predictions:
                            predictions = predictions['future']
                        elif 'predictions' in predictions:
                            predictions = predictions['predictions']
                    
                    if not isinstance(predictions, list):
                        logger.warning(f"Loaded predictions is not a list for {data_end_date.strftime('%Y-%m-%d')}: {type(predictions)}")
                        predictions = []
                    
                    metadata = loaded_result.get('metadata', {})
                    if not isinstance(metadata, dict):
                        metadata = {}
                    
                    # ğŸ”§ metrics ì•ˆì „ì„± ì²˜ë¦¬: Noneì´ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    cached_metrics = metadata.get('metrics')
                    if not cached_metrics or not isinstance(cached_metrics, dict):
                        cached_metrics = {'f1': 0.0, 'accuracy': 0.0, 'mape': 0.0, 'weighted_score': 0.0}
                    
                    accumulated_item = {
                        'date': data_end_date.strftime('%Y-%m-%d'),  # ë°ì´í„° ê¸°ì¤€ì¼
                        'prediction_start_date': loaded_result.get('prediction_start_date'),  # ì˜ˆì¸¡ ì‹œì‘ì¼
                        'predictions': predictions,
                        'metrics': cached_metrics,
                        'interval_scores': metadata.get('interval_scores', {}),
                        'next_semimonthly_period': metadata.get('next_semimonthly_period'),
                        'actual_business_days': metadata.get('actual_business_days'),
                        'original_interval_scores': metadata.get('interval_scores', {}),
                        'has_attention': loaded_result.get('has_cached_attention', False),
                        'storage_system': metadata.get('storage_system', 'unified'),
                        'loaded_from': loaded_result.get('loaded_from', 'unified_storage')
                    }
                    accumulated_results.append(accumulated_item)
                    logger.info(f"  âœ… [UNIFIED_ACCUMULATED] Added to results {i+1}/{len(filtered_predictions)}: {data_end_date.strftime('%Y-%m-%d')}")
                else:
                    logger.warning(f"  âŒ [UNIFIED_ACCUMULATED] Failed to load prediction {i+1}/{len(filtered_predictions)}: {loaded_result.get('error')}")
                    
            except Exception as e:
                logger.error(f"  âŒ Error loading prediction {i+1}/{len(filtered_predictions)}: {str(e)}")
                continue
        
        logger.info(f"ğŸ¯ [UNIFIED_ACCUMULATED] Successfully loaded {len(accumulated_results)} predictions from unified storage")
        return accumulated_results
        
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_ACCUMULATED] Error loading accumulated predictions: {str(e)}")
        logger.error(traceback.format_exc())
        return []

# ğŸŒŸ ì™„ì „ í†µí•© ì €ì¥ì†Œ ì‹œìŠ¤í…œ
UNIFIED_PREDICTIONS_DIR = Path("app/predictions")
UNIFIED_HYPERPARAMETERS_DIR = Path("app/hyperparameters") 
UNIFIED_PLOTS_DIR = Path("app/plots")

# í†µí•© ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ë“¤ ì´ˆê¸°í™”
def ensure_unified_storage_dirs():
    """ëª¨ë“  í†µí•© ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±"""
    try:
        dirs_to_create = [
            UNIFIED_PREDICTIONS_DIR,
            UNIFIED_HYPERPARAMETERS_DIR, 
            UNIFIED_PLOTS_DIR,
            UNIFIED_PLOTS_DIR / 'attention',
            UNIFIED_PLOTS_DIR / 'ma_plots'
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
            logger.debug(f"âœ… Unified directory ensured: {dir_path}")
        
        logger.info(f"ğŸŒŸ All unified storage directories initialized")
        return True
    except Exception as e:
        logger.error(f"âŒ Failed to create unified storage directories: {e}")
        return False

def get_unified_storage_dirs():
    """
    í†µí•© ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ğŸŒŸ
    ğŸ”§ ëª¨ë“  ì˜ˆì¸¡ ê´€ë ¨ ì‚°ì¶œë¬¼ì„ í†µí•© ê´€ë¦¬
    """
    try:
        # í†µí•© ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ë“¤ í™•ì¸
        ensure_unified_storage_dirs()
        
        dirs = {
            'root': Path("app"),  # ğŸ”§ VARMAX í˜¸í™˜ì„±ì„ ìœ„í•œ root ë””ë ‰í† ë¦¬
            'predictions': UNIFIED_PREDICTIONS_DIR,
            'models': UNIFIED_HYPERPARAMETERS_DIR,  # í˜¸í™˜ì„±ì„ ìœ„í•´ 'models' í‚¤ ìœ ì§€
            'hyperparameters': UNIFIED_HYPERPARAMETERS_DIR,  # ìƒˆë¡œìš´ ëª…ì‹œì  í‚¤
            'plots': UNIFIED_PLOTS_DIR,
            'attention_plots': UNIFIED_PLOTS_DIR / 'attention',
            'ma_plots': UNIFIED_PLOTS_DIR / 'ma_plots',
            'accumulated': UNIFIED_PREDICTIONS_DIR / 'accumulated'  # ëˆ„ì  ì˜ˆì¸¡ìš©
        }
        
        logger.debug(f"ğŸŒŸ [UNIFIED_STORAGE] Using unified storage system")
        return dirs
        
    except Exception as e:
        logger.error(f"âŒ Error in get_unified_storage_dirs: {str(e)}")
        logger.error(traceback.format_exc())
        raise e

# ëª¨ë“ˆ ë¡œë“œ ì‹œ í†µí•© ë””ë ‰í† ë¦¬ ìƒì„±
ensure_unified_storage_dirs()

def ensure_unified_predictions_dir():
    """í†µí•© ì˜ˆì¸¡ ì €ì¥ì†Œ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ìƒì„±"""
    try:
        UNIFIED_PREDICTIONS_DIR.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… [UNIFIED_DIR] Ensured unified predictions directory: {UNIFIED_PREDICTIONS_DIR}")
        return True
    except Exception as e:
        logger.error(f"âŒ [UNIFIED_DIR] Failed to create unified predictions directory: {str(e)}")
        return False

# Cache directory exports  
CACHE_BASE_DIR = Path("app/cache")