import pandas as pd
import os
import logging
import warnings
import numpy as np
import shutil
import logging
from pathlib import Path
import time
import traceback
import json

logger = logging.getLogger(__name__)

# í•„ìš”í•œ í•¨ìˆ˜ë“¤ import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ ì¡°ê±´ë¶€)
def get_file_cache_dirs(file_path):
    """
    íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•´ local import)
    """
    try:
        from app.data.cache_manager import get_file_cache_dirs as _get_file_cache_dirs
        return _get_file_cache_dirs(file_path)
    except ImportError:
        # cache_managerê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ìºì‹œ êµ¬ì¡° ìƒì„±
        import hashlib
        file_hash = hashlib.md5(file_path.encode()).hexdigest()[:12]
        file_name = Path(file_path).stem
        cache_dir_name = f"{file_hash}_{file_name}"
        cache_root = Path(CACHE_ROOT_DIR) / cache_dir_name
        
        # ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
        return {
            'root': cache_root,
            'models': cache_root / 'models',
            'predictions': cache_root / 'predictions',
            'plots': cache_root / 'static' / 'plots',
            'ma_plots': cache_root / 'static' / 'ma_plots',
            'accumulated': cache_root / 'accumulated'
        }

_dataframe_cache = {}
_cache_expiry_seconds = 3600

# ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ ìºì‹œ ìƒì„± í”Œë˜ê·¸
_cache_creation_in_progress = False

# xlwings ê´€ë ¨ ì „ì—­ ë³€ìˆ˜ ì²˜ë¦¬ (app_rev.pyì—ì„œ ê°€ì ¸ì˜´)
try:
    import xlwings as xw
    XLWINGS_AVAILABLE = True
except ImportError:
    XLWINGS_AVAILABLE = False
    logging.warning("xlwings not available - falling back to pandas only")

# ì „ì—­ ë³€ìˆ˜
from app.config import CACHE_ROOT_DIR, UPLOAD_FOLDER
from app.data.cache_manager import get_data_content_hash # load_dataì—ì„œ ì‚¬ìš©

def safe_read_excel(file_path, **kwargs):
    """
    Excel íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì½ëŠ” í—¬í¼ í•¨ìˆ˜
    ë‹¤ì–‘í•œ ì—”ì§„ì„ ì‹œë„í•˜ì—¬ í˜¸í™˜ì„± ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ
        **kwargs: pandas.read_excelì— ì „ë‹¬í•  ì¶”ê°€ ì¸ìë“¤
    
    Returns:
        pd.DataFrame: ì½ì–´ì˜¨ ë°ì´í„°í”„ë ˆì„
    
    Raises:
        ValueError: ëª¨ë“  ì—”ì§„ìœ¼ë¡œ ì½ê¸°ì— ì‹¤íŒ¨í•œ ê²½ìš°
    """
    engines = ['openpyxl', 'xlrd']  # ì‹œë„í•  ì—”ì§„ ìˆœì„œ
    last_error = None
    
    for engine in engines:
        try:
            logger.info(f"ğŸ“– Excel íŒŒì¼ ì½ê¸° ì‹œë„ (ì—”ì§„: {engine}): {os.path.basename(file_path)}")
            df = pd.read_excel(file_path, engine=engine, **kwargs)
            logger.info(f"âœ… Excel íŒŒì¼ ì½ê¸° ì„±ê³µ (ì—”ì§„: {engine})")
            return df
        except Exception as e:
            last_error = e
            logger.warning(f"âš ï¸ ì—”ì§„ {engine}ìœ¼ë¡œ ì½ê¸° ì‹¤íŒ¨: {str(e)}")
            continue
    
    # ëª¨ë“  ì—”ì§„ì´ ì‹¤íŒ¨í•œ ê²½ìš°
    error_msg = f"ëª¨ë“  ì—”ì§„ìœ¼ë¡œ Excel íŒŒì¼ ì½ê¸° ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {str(last_error)}"
    logger.error(f"âŒ {error_msg}")
    raise ValueError(error_msg)

def load_data_with_xlwings(file_path, model_type=None):
    """
    xlwingsë¥¼ ì‚¬ìš©í•˜ì—¬ DRM ë³´í˜¸ íŒŒì¼ ë° ë³´ì•ˆ í™•ì¥ì íŒŒì¼ì„ Excel í”„ë¡œì„¸ìŠ¤ ê²½ìœ ë¡œ ì½ëŠ” í•¨ìˆ˜
    ğŸ”‘ í•µì‹¬: Excel.exeë¥¼ ì¤‘ê³„ìë¡œ ì‚¬ìš©í•˜ì—¬ DRM ì¸ì¦ ë° ë³´ì•ˆ ì •ì±… ìš°íšŒ
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ (DRM ë³´í˜¸ ë˜ëŠ” ë³´ì•ˆ í™•ì¥ì í¬í•¨)
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available. Please install it with: pip install xlwings")
    
    # ğŸ”’ ë³´ì•ˆ í™•ì¥ì í™•ì¸
    actual_file_type, is_security_file = normalize_security_extension(file_path)
    
    if is_security_file:
        logger.info(f"ğŸ”“ [XLWINGS_SECURITY] Security extension detected: {os.path.basename(file_path)} (.{os.path.splitext(file_path)[1][1:]} -> {actual_file_type})")
        logger.info(f"ğŸ”“ [XLWINGS_SECURITY] Using Excel process to bypass security policy")
    else:
        logger.info(f"ğŸ”“ [XLWINGS_DRM] DRM ìš°íšŒ: Excel í”„ë¡œì„¸ìŠ¤ ê²½ìœ ë¡œ íŒŒì¼ ë¡œë”©")
    
    logger.info(f"ğŸ“ [XLWINGS] Target: {os.path.basename(file_path)}")
    
    app = None
    wb = None
    
    try:
        # ğŸ”‘ í•µì‹¬: Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤ì œ Excel.exeë¡œ ì‹œì‘
        # DRMì€ Excel.exeë¥¼ ì‹ ë¢°í•˜ë¯€ë¡œ ì´ë¥¼ í†µí•´ ìš°íšŒ ê°€ëŠ¥
        app = xw.App(visible=False, add_book=False)
        app.display_alerts = False  # DRM ê´€ë ¨ ê²½ê³ ë„ ë¬´ì‹œ
        app.screen_updating = False 
        
        logger.info(f"ğŸ“± [XLWINGS_DRM] Excel.exe process started (PID: {app.pid})")
        logger.info(f"ğŸ” [XLWINGS_DRM] DRM will recognize this as trusted Excel access")
        
        # ğŸ”’ DRM ë³´í˜¸ íŒŒì¼ ì—´ê¸° ì‹œë„
        try:
            # read_only=Trueë¡œ DRM ê²½ê³  ìµœì†Œí™”
            # update_links=Falseë¡œ ì™¸ë¶€ ë§í¬ ì—…ë°ì´íŠ¸ ë°©ì§€ (ë³´ì•ˆ ì´ìŠˆ íšŒí”¼)
            wb = app.books.open(file_path, read_only=True, update_links=False, password=None)
            logger.info(f"âœ… [XLWINGS_DRM] DRM ë³´í˜¸ íŒŒì¼ ì„±ê³µì ìœ¼ë¡œ ì—´ë¦¼: {wb.name}")
        except Exception as open_error:
            logger.error(f"âŒ [XLWINGS_DRM] DRM íŒŒì¼ ì—´ê¸° ì‹¤íŒ¨: {str(open_error)}")
            # ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•œ ê²½ìš°ë‚˜ ë‹¤ë¥¸ DRM ì´ìŠˆ
            if "password" in str(open_error).lower():
                raise ValueError("ğŸ” DRM íŒŒì¼ì— ë¹„ë°€ë²ˆí˜¸ê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ ì œê³µì—…ì²´ì— ë¹„ë°€ë²ˆí˜¸ë¥¼ ìš”ì²­í•˜ì„¸ìš”.")
            elif "permission" in str(open_error).lower() or "access" in str(open_error).lower():
                raise ValueError("ğŸš« DRM ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. IT ê´€ë¦¬ìì—ê²Œ ê¶Œí•œ ìš”ì²­ì„ í•˜ê±°ë‚˜ ì¸ì¦ëœ í™˜ê²½ì—ì„œ íŒŒì¼ì„ ì—¬ì„¸ìš”.")
            else:
                raise ValueError(f"ğŸ”’ DRM ë³´í˜¸ë¡œ ì¸í•´ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(open_error)}")
        
        # ì›Œí¬ì‹œíŠ¸ ì •ë³´ í™•ì¸
        sheet_names = [sheet.name for sheet in wb.sheets]
        logger.info(f"ğŸ“‹ [XLWINGS_DRM] Available sheets: {sheet_names}")
        
        # ì ì ˆí•œ ì‹œíŠ¸ ì„ íƒ
        target_sheet_name = '29 Nov 2010 till todate'
        if target_sheet_name in sheet_names:
            sheet = wb.sheets[target_sheet_name]
            logger.info(f"ğŸ¯ [XLWINGS_DRM] Using target sheet: {target_sheet_name}")
        else:
            sheet = wb.sheets[0]  # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
            logger.info(f"ğŸ¯ [XLWINGS_DRM] Using first sheet: {sheet.name}")
        
        # ğŸ”“ DRM ìš°íšŒ ë°ì´í„° ì¶”ì¶œ
        try:
            # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
            used_range = sheet.used_range
            if used_range is None:
                raise ValueError("Sheet appears to be empty")
            
            logger.info(f"ğŸ“ [XLWINGS_DRM] Used range: {used_range.address}")
            
            # ğŸš€ í•µì‹¬: Excel í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ë¡œ ë³µì‚¬
            # ì´ ê³¼ì •ì—ì„œ DRM ë³´í˜¸ê°€ í•´ì œë¨
            df_raw = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
            
            logger.info(f"ğŸ“Š [XLWINGS_DRM] Raw DRM ë°ì´í„° ì¶”ì¶œ: {df_raw.shape}")
            logger.info(f"ğŸ“‹ [XLWINGS_DRM] Raw columns: {list(df_raw.columns)}")
            
            # ğŸ§¹ Excel DRM í…ìŠ¤íŠ¸ í•„í„°ë§ ë° ë°ì´í„° ì •ì œ
            df = clean_drm_from_excel(df_raw)
            
            if df.empty:
                logger.warning(f"âš ï¸ [XLWINGS_DRM] DRM ì •ì œ í›„ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
                raise ValueError("ğŸš« DRM ì •ì œ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            logger.info(f"âœ… [XLWINGS_DRM] DRM Excel ì •ì œ ì™„ë£Œ: {df.shape}")
            logger.info(f"ğŸ“‹ [XLWINGS_DRM] Cleaned columns: {list(df.columns)}")
            
        except Exception as extract_error:
            logger.error(f"âŒ [XLWINGS_DRM] ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {str(extract_error)}")
            raise ValueError(f"ğŸ”’ DRM ë³´í˜¸ë¡œ ì¸í•´ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(extract_error)}")
        
        # ë°ì´í„° ê²€ì¦
        if df is None or df.empty:
            raise ValueError("ğŸš« DRM íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # Date ì»¬ëŸ¼ ì²˜ë¦¬
        if 'Date' not in df.columns:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œì¼ ê°€ëŠ¥ì„± í™•ì¸
            first_col = df.columns[0]
            if 'date' in first_col.lower() or df[first_col].dtype == 'datetime64[ns]':
                df = df.rename(columns={first_col: 'Date'})
                logger.info(f"ğŸ”„ [XLWINGS_DRM] Renamed '{first_col}' to 'Date'")
            else:
                # Date ì»¬ëŸ¼ì´ ì—†ì–´ë„ ì§„í–‰ (DRM íŒŒì¼ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
                logger.warning(f"âš ï¸ [XLWINGS_DRM] Date column not found, using data as-is")
                return df
        
        # Date ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        try:
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
            logger.info(f"ğŸ“… [XLWINGS_DRM] Date range: {df.index.min()} to {df.index.max()}")
        except Exception as date_error:
            logger.warning(f"âš ï¸ [XLWINGS_DRM] Date conversion failed: {str(date_error)}")
            # Date ë³€í™˜ì— ì‹¤íŒ¨í•´ë„ ì›ë³¸ ë°ì´í„° ë°˜í™˜
            return df
        
        # ëª¨ë¸ íƒ€ì…ë³„ ë°ì´í„° í•„í„°ë§
        if model_type == 'lstm':
            cutoff_date = pd.to_datetime('2022-01-01')
            original_shape = df.shape
            df = df[df.index >= cutoff_date]
            logger.info(f"ğŸ” [XLWINGS_DRM] LSTM filter: {original_shape[0]} -> {df.shape[0]} records")
            
            if df.empty:
                raise ValueError("No data available after 2022-01-01 filter for LSTM model")
        
        # ê¸°ë³¸ ë°ì´í„° ì •ì œ
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill().bfill()
        
        logger.info(f"âœ… [XLWINGS_DRM] DRM ìš°íšŒ ì™„ë£Œ - ë°ì´í„° ì„±ê³µì ìœ¼ë¡œ ë¡œë”©: {df.shape}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [XLWINGS_DRM] DRM ìš°íšŒ ì‹¤íŒ¨: {str(e)}")
        # DRM ê´€ë ¨ êµ¬ì²´ì  ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
        if "password" in str(e).lower():
            raise ValueError("ğŸ” DRM íŒŒì¼ì— ë¹„ë°€ë²ˆí˜¸ ë³´í˜¸ê°€ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. íŒŒì¼ ì œê³µì—…ì²´ì— ë¹„ë°€ë²ˆí˜¸ë¥¼ ìš”ì²­í•˜ì„¸ìš”.")
        elif "permission" in str(e).lower() or "access" in str(e).lower():
            raise ValueError("ğŸš« í˜„ì¬ ì‚¬ìš©ì ê³„ì •ì— DRM íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. IT ê´€ë¦¬ìì—ê²Œ ê¶Œí•œì„ ìš”ì²­í•˜ì„¸ìš”.")
        else:
            raise e
        
    finally:
        # ğŸ§¹ Excel í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¤‘ìš”: DRM ì„¸ì…˜ ì¢…ë£Œ)
        try:
            if wb is not None:
                wb.close()
                logger.info("ğŸ“– [XLWINGS_DRM] DRM workbook closed")
        except:
            pass
        
        try:
            if app is not None:
                app.quit()
                logger.info("ğŸ“± [XLWINGS_DRM] Excel.exe process terminated")
        except:
            pass

def load_data_safe_holidays(file_path):
    """
    íœ´ì¼ íŒŒì¼ ì „ìš© xlwings ë¡œë”© í•¨ìˆ˜ - ë³´ì•ˆí”„ë¡œê·¸ë¨ ìš°íšŒ
    
    Args:
        file_path (str): íœ´ì¼ Excel íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: íœ´ì¼ ë°ì´í„°í”„ë ˆì„ (date, description ì»¬ëŸ¼)
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available for holiday file loading")
    
    logger.info(f"ğŸ”“ [HOLIDAYS_XLWINGS] Loading holiday file with security bypass: {os.path.basename(file_path)}")
    
    app = None
    wb = None
    
    try:
        # Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
        app = xw.App(visible=False, add_book=False)
        app.display_alerts = False
        app.screen_updating = False
        
        logger.info(f"ğŸ“± [HOLIDAYS_XLWINGS] Excel app started for holidays")
        
        # Excel íŒŒì¼ ì—´ê¸°
        wb = app.books.open(file_path, read_only=True, update_links=False)
        logger.info(f"ğŸ“– [HOLIDAYS_XLWINGS] Holiday workbook opened: {wb.name}")
        
        # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš© (íœ´ì¼ íŒŒì¼ì€ ë³´í†µ ë‹¨ìˆœ êµ¬ì¡°)
        sheet = wb.sheets[0]
        logger.info(f"ğŸ¯ [HOLIDAYS_XLWINGS] Using sheet: {sheet.name}")
        
        # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
        used_range = sheet.used_range
        if used_range is None:
            raise ValueError("Holiday sheet appears to be empty")
        
        logger.info(f"ğŸ“ [HOLIDAYS_XLWINGS] Used range: {used_range.address}")
        
        # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸° (í—¤ë” í¬í•¨)
        df = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
        
        logger.info(f"ğŸ“Š [HOLIDAYS_XLWINGS] Holiday data loaded: {df.shape}")
        logger.info(f"ğŸ“‹ [HOLIDAYS_XLWINGS] Columns: {list(df.columns)}")
        
        # ë°ì´í„° ê²€ì¦
        if df is None or df.empty:
            raise ValueError("No holiday data found in the Excel file")
        
        # ì»¬ëŸ¼ëª… ì •ê·œí™” (case-insensitive)
        df.columns = df.columns.str.lower()
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        if 'date' not in df.columns:
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ ë‚ ì§œë¡œ ê°€ì •
            first_col = df.columns[0]
            df = df.rename(columns={first_col: 'date'})
            logger.info(f"ğŸ”„ [HOLIDAYS_XLWINGS] Renamed '{first_col}' to 'date'")
        
        # description ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        if 'description' not in df.columns:
            df['description'] = 'Holiday'
            logger.info(f"â• [HOLIDAYS_XLWINGS] Added default 'description' column")
        
        logger.info(f"âœ… [HOLIDAYS_XLWINGS] Holiday data loaded successfully: {len(df)} holidays")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAYS_XLWINGS] Error loading holiday file: {str(e)}")
        raise e
        
    finally:
        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        try:
            if wb is not None:
                wb.close()
                logger.info("ğŸ“– [HOLIDAYS_XLWINGS] Holiday workbook closed")
        except:
            pass
        
        try:
            if app is not None:
                app.quit()
                logger.info("ğŸ“± [HOLIDAYS_XLWINGS] Excel app closed")
        except:
            pass

def load_data_safe(file_path, model_type=None, use_cache=True, use_xlwings_fallback=True):
    """
    ì•ˆì „í•œ ë°ì´í„° ë¡œë”© í•¨ìˆ˜ - ë³´ì•ˆ í™•ì¥ì ì§€ì› ë° xlwingsë¥¼ ìš°ì„  ì‹œë„í•˜ê³  ë³´ì•ˆ ë¬¸ì œ ì‹œ pandasë¡œ ìë™ ì „í™˜
    
    Args:
        file_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
        use_cache (bool): ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        use_xlwings_fallback (bool): ì´ í•¨ìˆ˜ì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ğŸ”’ ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë¬¼ë¦¬ì  ë³€ê²½ í¬í•¨)
    processed_file_path, actual_ext, is_security_file = process_security_file_in_loader(file_path)
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
    file_path = processed_file_path
    
    # íŒŒì¼ íƒ€ì… ì¬í™•ì¸
    actual_file_type, _ = normalize_security_extension(file_path)
    
    if is_security_file:
        logger.info(f"ğŸ”’ [SECURITY_SAFE] Security file processed: {os.path.basename(file_path)} -> {actual_file_type}")
    
    # âœ… xlwings ìš°ì„  ì‹œë„ (Excel íƒ€ì…ì¸ ê²½ìš°)
    if XLWINGS_AVAILABLE and actual_file_type in ['xlsx', 'xls', 'excel']:
        try:
            logger.info("ğŸ”“ [SECURITY_BYPASS] Attempting xlwings bypass first...")
            return load_data_with_xlwings(file_path, model_type)
        except Exception as xlwings_error:
            logger.warning(f"âš ï¸ [SECURITY_BYPASS] xlwings failed: {str(xlwings_error)}")
            logger.info("ğŸ”„ Falling back to standard pandas loading...")
            # xlwings ì‹¤íŒ¨ ì‹œ ì•„ë˜ì˜ í‘œì¤€ load_dataë¡œ ë„˜ì–´ê°
    
    # xlwingsë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°, í‘œì¤€ load_data í•¨ìˆ˜ ì‹œë„
    try:
        return load_data(file_path, model_type, use_cache)
    except Exception as e:
        logger.error(f"âŒ Both xlwings and standard loading failed: {str(e)}")
        raise e

def load_csv_with_xlwings(csv_path, max_retries=3, retry_delay=1):
    """
    xlwingsë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ì½ëŠ” í•¨ìˆ˜ - ë³´ì•ˆí”„ë¡œê·¸ë¨ ìš°íšŒ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    
    Args:
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
        max_retries (int): ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        retry_delay (int): ì¬ì‹œë„ ê°„ê²© (ì´ˆ)
    
    Returns:
        pd.DataFrame: CSV ë°ì´í„°í”„ë ˆì„
    """
    if not XLWINGS_AVAILABLE:
        raise ImportError("xlwings is not available for CSV loading")
    
    logger.info(f"ğŸ”“ [XLWINGS_CSV] Loading CSV file with security bypass: {os.path.basename(csv_path)}")
    
    for attempt in range(max_retries):
        app = None
        wb = None
        
        try:
            # Excel ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘
            app = xw.App(visible=False, add_book=False)
            app.display_alerts = False
            app.screen_updating = False
            
            logger.info(f"ğŸ“± [XLWINGS_CSV] Excel app started for CSV (attempt {attempt + 1}/{max_retries})")
            
            # CSV íŒŒì¼ì„ Excelë¡œ ì—´ê¸° (CSVëŠ” ìë™ìœ¼ë¡œ íŒŒì‹±ë¨)
            wb = app.books.open(csv_path, read_only=True, update_links=False)
            logger.info(f"ğŸ“– [XLWINGS_CSV] CSV workbook opened: {wb.name}")
            
            # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš© (CSVëŠ” í•­ìƒ í•˜ë‚˜ì˜ ì‹œíŠ¸ë§Œ ê°€ì§)
            sheet = wb.sheets[0]
            
            # ì‚¬ìš©ëœ ë²”ìœ„ í™•ì¸
            used_range = sheet.used_range
            if used_range is None:
                raise ValueError("CSV file appears to be empty")
            
            logger.info(f"ğŸ“ [XLWINGS_CSV] Used range: {used_range.address}")
            
            # ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ì½ê¸° (í—¤ë” í¬í•¨)
            df = sheet['A1'].options(pd.DataFrame, index=False, expand='table').value
            
            logger.info(f"ğŸ“Š [XLWINGS_CSV] CSV data loaded: {df.shape}")
            logger.info(f"ğŸ“‹ [XLWINGS_CSV] Columns: {list(df.columns)}")
            
            # ë°ì´í„° ê²€ì¦
            if df is None or df.empty:
                raise ValueError("No data found in the CSV file")
            
            logger.info(f"âœ… [XLWINGS_CSV] CSV loaded successfully: {df.shape}")
            return df
            
        except Exception as e:
            # RPC ì˜¤ë¥˜ ì½”ë“œ í™•ì¸ (Windows COM ì˜¤ë¥˜)
            is_rpc_error = (
                hasattr(e, 'args') and 
                len(e.args) > 0 and 
                isinstance(e.args[0], tuple) and 
                len(e.args[0]) > 0 and 
                e.args[0][0] in [-2147023174, -2147023170, -2147023173]  # RPC ì„œë²„ ì˜¤ë¥˜ë“¤
            )
            
            if is_rpc_error and attempt < max_retries - 1:
                logger.warning(f"âš ï¸ [XLWINGS_CSV] RPC error detected (attempt {attempt + 1}/{max_retries}): {e}")
                logger.info(f"ğŸ”„ [XLWINGS_CSV] Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                continue
            else:
                logger.error(f"âŒ [XLWINGS_CSV] Error loading CSV file (attempt {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    raise e
                
        finally:
            # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ë” ì•ˆì „í•œ ë°©ì‹
            try:
                if wb is not None:
                    wb.close()
                    logger.info("ğŸ“– [XLWINGS_CSV] CSV workbook closed")
            except Exception as cleanup_error:
                logger.debug(f"ğŸ”§ [XLWINGS_CSV] Workbook cleanup error: {cleanup_error}")
            
            try:
                if app is not None:
                    app.quit()
                    logger.info("ğŸ“± [XLWINGS_CSV] Excel app closed")
            except Exception as cleanup_error:
                logger.debug(f"ğŸ”§ [XLWINGS_CSV] App cleanup error: {cleanup_error}")
    
    # ëª¨ë“  ì¬ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
    raise RuntimeError(f"Failed to load CSV after {max_retries} attempts")

def load_csv_safe_with_fallback(csv_path):
    """
    ì•ˆì „í•œ CSV ë¡œë“œ í•¨ìˆ˜ - .cs íŒŒì¼ì€ pandas ìš°ì„ , ì¼ë°˜ CSVëŠ” xlwings ìš°ì„ 
    
    Args:
        csv_path (str): CSV íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: CSV ë°ì´í„°í”„ë ˆì„
    """
    # ê²½ë¡œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (Path ê°ì²´ ëŒ€ì‘)
    csv_path_str = str(csv_path)
    
    logger.info(f"ğŸ“Š [SAFE_CSV] Loading CSV safely: {os.path.basename(csv_path_str)}")
    
    # .cs í™•ì¥ì íŒŒì¼ì¸ì§€ í™•ì¸
    is_cs_file = csv_path_str.lower().endswith('.cs')
    
    if is_cs_file:
        logger.info(f"ğŸ” [SAFE_CSV] Detected .cs file - using pandas first to avoid C# parsing issues...")
        
        # .cs íŒŒì¼ì˜ ê²½ìš° pandasë¥¼ ë¨¼ì € ì‹œë„
        separators = [',', ';', '\t']
        df = None
        
        for sep in separators:
            try:
                df_test = pd.read_csv(csv_path_str, sep=sep, encoding='utf-8')
                
                # ë‹¨ì¼ ì»¬ëŸ¼ì— êµ¬ë¶„ìê°€ í¬í•¨ëœ ê²½ìš° (ì˜ëª»ëœ íŒŒì‹±) ì²´í¬
                if len(df_test.columns) == 1:
                    col_name = df_test.columns[0]
                    if ',' in col_name or ';' in col_name or '\t' in col_name:
                        logger.warning(f"âš ï¸ [SAFE_CSV] Detected incorrect parsing with '{sep}' - single column contains separators")
                        continue
                
                df = df_test
                logger.info(f"âœ… [SAFE_CSV] Successfully loaded .cs file with pandas using separator '{sep}': {df.shape}")
                logger.info(f"ğŸ“‹ [SAFE_CSV] Columns: {list(df.columns)}")
                return df
                
            except Exception as sep_error:
                logger.warning(f"âš ï¸ [SAFE_CSV] Failed with separator '{sep}': {str(sep_error)}")
                continue
        
        # .cs íŒŒì¼ì˜ ê²½ìš° xlwings ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (C# íŒŒì¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆìŒ)
        if df is None:
            logger.warning(f"âš ï¸ [SAFE_CSV] All pandas attempts failed for .cs file")
        
    else:
        # ì¼ë°˜ CSV íŒŒì¼ì˜ ê²½ìš° ê¸°ì¡´ ë¡œì§ (xlwings ìš°ì„ )
        try:
            # xlwingsë¡œ ë¨¼ì € ì‹œë„
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [SAFE_CSV] Attempting xlwings first...")
                try:
                    df = load_csv_with_xlwings(csv_path_str)
                    
                    # xlwings ê²°ê³¼ ê²€ì¦ (ë‹¨ì¼ ì»¬ëŸ¼ ë¬¸ì œ í™•ì¸)
                    if len(df.columns) == 1 and (',' in df.columns[0] or ';' in df.columns[0]):
                        logger.warning(f"âš ï¸ [SAFE_CSV] xlwings returned single column issue, falling back to pandas...")
                        raise ValueError("xlwings parsing issue - single column detected")
                    
                    logger.info(f"âœ… [SAFE_CSV] Successfully loaded CSV with xlwings: {df.shape}")
                    logger.info(f"ğŸ“‹ [SAFE_CSV] Columns: {list(df.columns)}")
                    return df
                    
                except Exception as xlwings_error:
                    logger.warning(f"âš ï¸ [SAFE_CSV] xlwings failed: {str(xlwings_error)}")
                    logger.info(f"ğŸ”„ [SAFE_CSV] Falling back to pandas with multiple separators...")
            
            # xlwings ì‹¤íŒ¨ ë˜ëŠ” ì—†ëŠ” ê²½ìš° pandasë¡œ fallback (ì—¬ëŸ¬ êµ¬ë¶„ì í…ŒìŠ¤íŠ¸)
            logger.info(f"ğŸ“Š [SAFE_CSV] Attempting pandas with multiple separators...")
            
            separators = [',', ';', '\t']
            df = None
            
            for sep in separators:
                try:
                    df_test = pd.read_csv(csv_path_str, sep=sep, encoding='utf-8')
                    
                    # ë‹¨ì¼ ì»¬ëŸ¼ì— êµ¬ë¶„ìê°€ í¬í•¨ëœ ê²½ìš° (ì˜ëª»ëœ íŒŒì‹±) ì²´í¬
                    if len(df_test.columns) == 1:
                        col_name = df_test.columns[0]
                        if ',' in col_name or ';' in col_name or '\t' in col_name:
                            logger.warning(f"âš ï¸ [SAFE_CSV] Detected incorrect parsing with '{sep}' - single column contains separators")
                            continue
                    
                    df = df_test
                    logger.info(f"âœ… [SAFE_CSV] Successfully loaded with pandas using separator '{sep}': {df.shape}")
                    logger.info(f"ğŸ“‹ [SAFE_CSV] Columns: {list(df.columns)}")
                    break
                    
                except Exception as sep_error:
                    logger.warning(f"âš ï¸ [SAFE_CSV] Failed with separator '{sep}': {str(sep_error)}")
                    continue
            
        except Exception as e:
            logger.error(f"âŒ [SAFE_CSV] Error in normal CSV processing: {str(e)}")
            df = None
    
    # ìµœì¢… í™•ì¸ ë° ì˜¤ë¥˜ ì²˜ë¦¬
    if df is None:
        # ë§ˆì§€ë§‰ìœ¼ë¡œ ê¸°ë³¸ pandas ì‹œë„
        try:
            logger.info(f"ğŸ”§ [SAFE_CSV] Final attempt with default pandas settings...")
            df = pd.read_csv(csv_path_str, encoding='utf-8')
            logger.info(f"âœ… [SAFE_CSV] Final attempt successful: {df.shape}")
            logger.info(f"ğŸ“‹ [SAFE_CSV] Columns: {list(df.columns)}")
            return df
        except Exception as final_error:
            logger.error(f"âŒ [SAFE_CSV] All methods failed: {str(final_error)}")
            raise RuntimeError("Failed to load CSV with all methods")
    
    return df

# ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
def load_data(file_path, model_type=None, use_cache=True):
    from app.data.preprocessor import process_excel_data_complete
    """
    ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì „ì²˜ë¦¬
    
    Args:
        file_path (str): ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ('lstm', 'varmax', None)
                         - 'lstm': ë‹¨ì¼/ëˆ„ì  ì˜ˆì¸¡ìš©, 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°
                         - 'varmax': ì¥ê¸°ì˜ˆì¸¡ìš©, ëª¨ë“  ë°ì´í„° ìœ ì§€
                         - None: ê¸°ë³¸ ë™ì‘ (ëª¨ë“  ë°ì´í„° ìœ ì§€)
        use_cache (bool): ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (default: True)
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
    cache_key = f"{file_path}|{model_type}|{os.path.getmtime(file_path)}"
    current_time = time.time()
    
    if use_cache and cache_key in _dataframe_cache:
        cached_data, cache_time = _dataframe_cache[cache_key]
        if (current_time - cache_time) < _cache_expiry_seconds:
            logger.info(f"ğŸš€ [CACHE_HIT] Using cached DataFrame for {os.path.basename(file_path)} (saved {current_time - cache_time:.1f}s ago)")
            return cached_data.copy()  # ë³µì‚¬ë³¸ ë°˜í™˜ìœ¼ë¡œ ì›ë³¸ ë³´í˜¸
        else:
            # ë§Œë£Œëœ ìºì‹œ ì œê±°
            del _dataframe_cache[cache_key]
            logger.info(f"ğŸ—‘ï¸ [CACHE_EXPIRED] Removed expired cache for {os.path.basename(file_path)}")
    
    logger.info(f"ğŸ“ [LOAD_DATA] Loading data with model_type: {model_type} from {os.path.basename(file_path)}")

    # ğŸ”’ 1ë‹¨ê³„: ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë¬¼ë¦¬ì  ë³€ê²½ í¬í•¨)
    processed_file_path, actual_ext, is_security_file = process_security_file_in_loader(file_path)
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
    file_path = processed_file_path
    
    # íŒŒì¼ íƒ€ì… ì¬í™•ì¸
    actual_file_type, _ = normalize_security_extension(file_path)
    
    if actual_file_type is None:
        raise ValueError(f"Unsupported or undetectable file format: {file_path}")
    
    logger.info(f"ğŸ“Š [FILE_TYPE] Processed file type: {actual_file_type} (security file: {is_security_file})")
    if is_security_file:
        logger.info(f"ğŸ“ [SECURITY_PROCESSED] File path updated: {os.path.basename(processed_file_path)}")
    
    # 2ë‹¨ê³„: íŒŒì¼ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ë¡œë“œ ë°©ë²• ì‚¬ìš©
    if actual_file_type == 'csv':
        logger.info("Loading CSV file with xlwings fallback support")
        # ì›ë³¸ CSV íŒŒì¼ì€ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬
        try:
            if XLWINGS_AVAILABLE:
                logger.info(f"ğŸ”“ [XLWINGS_CSV] Attempting to load CSV with xlwings: {file_path}")
                df = load_csv_with_xlwings(file_path)
            else:
                df = pd.read_csv(file_path)
        except Exception as e:
            logger.warning(f"âš ï¸ [XLWINGS_CSV] xlwings failed, falling back to pandas: {str(e)}")
            df = pd.read_csv(file_path)
        
        df['Date'] = pd.to_datetime(df['Date'])
        df.set_index('Date', inplace=True)
        
        # ê¸°ë³¸ì ì¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df = df.replace([np.inf, -np.inf], np.nan)
        df = df.ffill().bfill()
        
    elif actual_file_type in ['xlsx', 'xls', 'excel']:
        if is_security_file:
            logger.info(f"Loading Excel file (.{actual_file_type}) from security extension using CSV cache system")
        else:
            logger.info("Loading Excel file using CSV cache system")
        
        # ğŸš€ CSV ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš© (ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•´ ìºì‹œ ìƒì„± ì¤‘ì—ëŠ” ê±´ë„ˆë›°ê¸°)
        if _cache_creation_in_progress:
            logger.info("ğŸ”„ [CSV_CACHE] Cache creation in progress - skipping cache check to prevent infinite loop")
            df = load_excel_as_dataframe(file_path, model_type)
        else:
            cache_valid, cache_paths, extension_info = is_csv_cache_valid(file_path, model_type)
        
            if cache_valid == True:
                # ìºì‹œê°€ ì™„ì „íˆ ìœ íš¨í•˜ë©´ CSV ìºì‹œ ë¡œë”©
                logger.info("âœ… [CSV_CACHE] Using valid cache, loading from CSV...")
                df = load_csv_cache(file_path, model_type)
            elif cache_valid == "extension":
                # ğŸ¯ ë°ì´í„° í™•ì¥ì¸ ê²½ìš°: ê¸°ì¡´ ìºì‹œ + ìƒˆë¡œìš´ ë°ì´í„° ì²˜ë¦¬
                logger.info("ğŸ”„ [CSV_CACHE] Data extension detected, updating cache...")
                logger.info(f"    ğŸ“ˆ Extension: {extension_info.get('new_rows_count', 0)} new rows")
                logger.info(f"    ğŸ“… Date range: {extension_info.get('old_end_date')} â†’ {extension_info.get('new_end_date')}")
                
                try:
                    # ê¸°ì¡´ ìºì‹œ ì •ë³´ ë¡œë“œ
                    logger.info("ğŸ“‹ [CSV_CACHE] Loading existing cache info...")
                    csv_cache_file = cache_paths['csv_file']
                    df_cached = load_csv_safe_with_fallback(str(csv_cache_file))
                    logger.info(f"    ğŸ“Š Existing cache: {df_cached.shape}")
                    
                    # ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì™„ì „í•œ CSV ìºì‹œ ì¬ìƒì„±
                    # (í™•ì¥ëœ ë°ì´í„°ì˜ ê²½ìš° ì „ì²´ ì¬ì²˜ë¦¬ê°€ ë” ì•ˆì „í•¨)
                    logger.info("ğŸ”§ [CSV_CACHE] Regenerating cache with extended data...")
                    df = create_csv_cache_from_excel(file_path, model_type)
                    
                    logger.info(f"âœ… [CSV_CACHE] Successfully updated cache for extended data")
                    logger.info(f"    ğŸ“Š Old cache: {df_cached.shape} â†’ New cache: {df.shape}")
                    
                    # Date ì»¬ëŸ¼ì„ ì¸ë±ìŠ¤ë¡œ ë³µì› (create_csv_cache_from_excel ê²°ê³¼ì— ë§ì¶°)
                    if 'Date' in df.columns:
                        df['Date'] = pd.to_datetime(df['Date'])
                        df.set_index('Date', inplace=True)
                    
                except Exception as ext_error:
                    logger.warning(f"âš ï¸ [CSV_CACHE] Failed to update extended cache: {str(ext_error)}")
                    logger.info("ğŸ”„ [CSV_CACHE] Falling back to complete cache regeneration...")
                    df = create_csv_cache_from_excel(file_path, model_type)
            else:
                # ìºì‹œê°€ ì—†ê±°ë‚˜ ë¬´íš¨í•˜ë©´ Excelì—ì„œ CSV ìºì‹œ ìƒì„±
                logger.info("ğŸ“Š [CSV_CACHE] Cache invalid or missing, creating new cache from Excel...")
                df = create_csv_cache_from_excel(file_path, model_type)
    
    else:
        raise ValueError(f"Unsupported file format after normalization: {actual_file_type}")
    
    logger.info(f"Original data shape: {df.shape} (from {df.index.min()} to {df.index.max()})")
    
    # ğŸ”‘ ëª¨ë¸ íƒ€ì…ë³„ ë°ì´í„° í•„í„°ë§
    if model_type == 'lstm':
        # LSTM ëª¨ë¸ìš©: 2022ë…„ ì´ì „ ë°ì´í„° ì œê±°
        cutoff_date = pd.to_datetime('2022-01-01')
        original_shape = df.shape
        df = df[df.index >= cutoff_date]
        
        logger.info(f"ğŸ“Š LSTM model: Filtered data from 2022-01-01")
        logger.info(f"  Original: {original_shape[0]} records")
        logger.info(f"  Filtered: {df.shape[0]} records (removed {original_shape[0] - df.shape[0]} records)")
        logger.info(f"  Date range: {df.index.min()} to {df.index.max()}")
        
        if df.empty:
            raise ValueError("No data available after 2022-01-01 filter for LSTM model")
            
    elif model_type == 'varmax':
        # VARMAX ëª¨ë¸ìš©: ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        logger.info(f"ğŸ“Š VARMAX model: Using all available data")
        logger.info(f"  Full date range: {df.index.min()} to {df.index.max()}")
        
    else:
        # ê¸°ë³¸ ë™ì‘: ëª¨ë“  ë°ì´í„° ì‚¬ìš©
        logger.info(f"ğŸ“Š Default mode: Using all available data")
        logger.info(f"  Full date range: {df.index.min()} to {df.index.max()}")
    
    # ëª¨ë“  inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
    df = df.replace([np.inf, -np.inf], np.nan)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ - ëª¨ë“  ì»¬ëŸ¼ì— ë™ì¼í•˜ê²Œ ì ìš©
    df = df.ffill().bfill()
    
    # ì²˜ë¦¬ í›„ ë‚¨ì•„ìˆëŠ” infë‚˜ nan í™•ì¸
    # ìˆ«ì ì»¬ëŸ¼ë§Œ ì„ íƒí•´ì„œ isinf ê²€ì‚¬
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    has_nan = df.isnull().any().any()
    has_inf = False
    if len(numeric_cols) > 0:
        has_inf = np.isinf(df[numeric_cols].values).any()
    
    if has_nan or has_inf:
        logger.warning("Dataset still contains NaN or inf values after preprocessing")
        
        # ğŸ“Š ìƒì„¸í•œ ì»¬ëŸ¼ ë¶„ì„ ë° ë¬¸ì œ ì§„ë‹¨
        logger.warning("=" * 60)
        logger.warning("ğŸ“Š DATA QUALITY ANALYSIS")
        logger.warning("=" * 60)
        
        # 1. ë°ì´í„° íƒ€ì… ì •ë³´
        logger.warning(f"ğŸ“‹ Total columns: {len(df.columns)}")
        logger.warning(f"ğŸ”¢ Numeric columns: {len(numeric_cols)} - {list(numeric_cols)}")
        non_numeric_cols = [col for col in df.columns if col not in numeric_cols]
        logger.warning(f"ğŸ”¤ Non-numeric columns: {len(non_numeric_cols)} - {list(non_numeric_cols)}")
        
        # 2. NaN ê°’ ë¶„ì„
        problematic_cols_nan = df.columns[df.isnull().any()]
        if len(problematic_cols_nan) > 0:
            logger.warning(f"âš ï¸ Columns with NaN values: {len(problematic_cols_nan)}")
            for col in problematic_cols_nan:
                nan_count = df[col].isnull().sum()
                total_count = len(df[col])
                percentage = (nan_count / total_count) * 100
                logger.warning(f"   â€¢ {col}: {nan_count}/{total_count} ({percentage:.1f}%) NaN")
        
        # 3. inf ê°’ ë¶„ì„ (ìˆ«ì ì»¬ëŸ¼ë§Œ)
        problematic_cols_inf = []
        if len(numeric_cols) > 0:
            for col in numeric_cols:
                if np.isinf(df[col]).any():
                    problematic_cols_inf.append(col)
                    inf_count = np.isinf(df[col]).sum()
                    total_count = len(df[col])
                    percentage = (inf_count / total_count) * 100
                    logger.warning(f"   â€¢ {col}: {inf_count}/{total_count} ({percentage:.1f}%) inf values")
        
        if len(problematic_cols_inf) > 0:
            logger.warning(f"âš ï¸ Columns with inf values: {len(problematic_cols_inf)} - {problematic_cols_inf}")
        
        # 4. ê° ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì…ê³¼ ìƒ˜í”Œ ê°’
        logger.warning("ğŸ“ Column details:")
        for col in df.columns:
            dtype = str(df[col].dtype)
            non_null_count = df[col].count()
            sample_values = df[col].dropna().head(3).tolist()
            logger.warning(f"   â€¢ {col}: {dtype} ({non_null_count} non-null) - Sample: {sample_values}")
        
        problematic_cols = list(set(list(problematic_cols_nan) + problematic_cols_inf))
        logger.warning("=" * 60)
        logger.warning(f"ğŸ¯ SUMMARY: {len(problematic_cols)} problematic columns found: {problematic_cols}")
        logger.warning("=" * 60)
        
        # ì¶”ê°€ì ì¸ ì „ì²˜ë¦¬: ë‚¨ì€ inf/nan ê°’ì„ í•´ë‹¹ ì»¬ëŸ¼ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´ (ìˆ«ì ì»¬ëŸ¼ë§Œ)
        for col in problematic_cols:
            if col in numeric_cols:
                # ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ inf ì²˜ë¦¬
                col_mean = df[col].replace([np.inf, -np.inf], np.nan).mean()
                df[col] = df[col].replace([np.inf, -np.inf], np.nan).fillna(col_mean)
            else:
                # ë¹„ìˆ«ì ì»¬ëŸ¼ì— ëŒ€í•´ì„œëŠ” NaNë§Œ ì²˜ë¦¬
                df[col] = df[col].ffill().bfill()
    
    logger.info(f"Final shape after preprocessing: {df.shape}")
    
    # ğŸ”§ ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥ (ì„±ê³µì ìœ¼ë¡œ ë¡œë”©ëœ ê²½ìš°)
    if use_cache:
        _dataframe_cache[cache_key] = (df.copy(), current_time)
        logger.info(f"ğŸ’¾ [CACHE_SAVE] Saved DataFrame to cache for {os.path.basename(file_path)} (expires in {_cache_expiry_seconds}s)")
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬: ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬
        expired_keys = []
        for key, (cached_df, cache_time) in _dataframe_cache.items():
            if (current_time - cache_time) >= _cache_expiry_seconds:
                expired_keys.append(key)
        
        for key in expired_keys:
            del _dataframe_cache[key]
        
        if expired_keys:
            logger.info(f"ğŸ—‘ï¸ [CACHE_CLEANUP] Removed {len(expired_keys)} expired cache entries")
    
    return df

# ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜
variable_groups = {
    'crude_oil': ['WTI', 'Brent', 'Dubai'],
    'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
    'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
    'lpg': ['C3_LPG', 'C4_LPG'],
    'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
    'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
    'spread': ['biweekly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
    'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
    'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
    'economics': ['Dow_Jones', 'Euro', 'Gold'],
    'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
    'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
    'Flat Rate_Chiba'],
    'ETF': ['DIG', 'DUG', 'IYE', 'VDE', 'XLE']
}

def load_holidays_from_file(filepath=None):
    """
    CSV ë˜ëŠ” Excel íŒŒì¼ì—ì„œ íœ´ì¼ ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        filepath (str): íœ´ì¼ ëª©ë¡ íŒŒì¼ ê²½ë¡œ, Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    
    Returns:
        set: íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ - holidays í´ë”ë¡œ ë³€ê²½
    if filepath is None:
        holidays_dir = Path('holidays')
        holidays_dir.mkdir(exist_ok=True)
        filepath = str(holidays_dir / 'holidays.csv')
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    _, ext = os.path.splitext(filepath)
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ ëª©ë¡ ìƒì„±
    if not os.path.exists(filepath):
        logger.warning(f"Holiday file {filepath} not found. Creating default holiday file.")
        
        # ê¸°ë³¸ 2025ë…„ ì‹±ê°€í´ ê³µíœ´ì¼
        default_holidays = [
            "2025-01-01", "2025-01-29", "2025-01-30", "2025-03-31", "2025-04-18", 
            "2025-05-01", "2025-05-12", "2025-06-07", "2025-08-09", "2025-10-20", 
            "2025-12-25", "2026-01-01"
        ]
        
        # ê¸°ë³¸ íŒŒì¼ ìƒì„±
        df = pd.DataFrame({'date': default_holidays, 'description': ['Singapore Holiday']*len(default_holidays)})
        
        if ext.lower() == '.xlsx':
            df.to_excel(filepath, index=False)
        else:
            df.to_csv(filepath, index=False)
        
        logger.info(f"Created default holiday file at {filepath}")
        return set(default_holidays)
    
    try:
        # íŒŒì¼ ë¡œë“œ - ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
        if ext.lower() == '.xlsx':
            # Excel íŒŒì¼ì˜ ê²½ìš° xlwings ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ ì‚¬ìš©
            try:
                df = load_data_safe_holidays(filepath)
            except Exception as e:
                logger.warning(f"âš ï¸ [HOLIDAYS] xlwings loading failed, using safe_read_excel: {str(e)}")
                df = safe_read_excel(filepath)
        else:
            # CSV íŒŒì¼ë„ ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•˜ì—¬ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
            try:
                # ì¼ë°˜ pandasë¡œ ë¨¼ì € ì‹œë„
                df = pd.read_csv(filepath)
                logger.info(f"âœ… [HOLIDAYS] CSV loaded with pandas: {len(df)} rows")
            except Exception as pandas_error:
                logger.warning(f"âš ï¸ [HOLIDAYS] pandas CSV loading failed: {str(pandas_error)}")
                try:
                    # xlwingsë¡œ CSV ì½ê¸° ì‹œë„ (ë³´ì•ˆ ìš°íšŒ)
                    if XLWINGS_AVAILABLE:
                        logger.info(f"ğŸ”„ [HOLIDAYS] Trying xlwings for CSV: {os.path.basename(filepath)}")
                        df = load_csv_with_xlwings(filepath)
                        logger.info(f"âœ… [HOLIDAYS] CSV loaded with xlwings: {len(df)} rows")
                    else:
                        # xlwings ì—†ìœ¼ë©´ ë‹¤ì‹œ pandasë¡œ ì‹œë„ (ì˜¤ë¥˜ ì¬ë°œìƒ)
                        raise pandas_error
                except Exception as xlwings_error:
                    logger.error(f"âŒ [HOLIDAYS] Both pandas and xlwings failed for CSV")
                    logger.error(f"   Pandas error: {str(pandas_error)}")
                    logger.error(f"   xlwings error: {str(xlwings_error)}")
                    raise pandas_error  # ì›ë˜ pandas ì˜¤ë¥˜ë¥¼ ì¬ë°œìƒ
        
        # 'date' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'date' not in df.columns:
            logger.error(f"Holiday file {filepath} does not have 'date' column")
            return set()
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format: {date_str}")
        
        logger.info(f"Loaded {len(holidays)} holidays from {filepath}")
        return holidays
        
    except Exception as e:
        logger.error(f"Error loading holiday file: {str(e)}")
        logger.error(traceback.format_exc())
        return set()

# ì „ì—­ ë³€ìˆ˜ë¡œ íœ´ì¼ ì§‘í•© ê´€ë¦¬
holidays = load_holidays_from_file()

def detect_file_type_by_content(file_path):
    """
    íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ íŒŒì¼ íƒ€ì…ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    íšŒì‚¬ ë³´ì•ˆìœ¼ë¡œ ì¸í•´ í™•ì¥ìê°€ ë³€ê²½ëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬
    """
    try:
        # íŒŒì¼ì˜ ì²« ëª‡ ë°”ì´íŠ¸ë¥¼ ì½ì–´ì„œ íŒŒì¼ íƒ€ì… ê°ì§€
        with open(file_path, 'rb') as f:
            header = f.read(8)
        
        # Excel íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        if header[:4] == b'PK\x03\x04':  # ZIP ê¸°ë°˜ íŒŒì¼ (xlsx)
            return 'xlsx'
        elif header[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':  # OLE2 ê¸°ë°˜ íŒŒì¼ (xls)
            return 'xls'
        
        # CSV íŒŒì¼ì¸ì§€ í™•ì¸ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                # CSV íŠ¹ì„± í™•ì¸: ì‰¼í‘œë‚˜ íƒ­ì´ í¬í•¨ë˜ì–´ ìˆê³ , Date ì»¬ëŸ¼ì´ ìˆëŠ”ì§€
                if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                    return 'csv'
        except:
            # UTF-8ë¡œ ì½ê¸° ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    first_line = f.readline()
                    if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                        return 'csv'
            except:
                pass
        
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return None
        
    except Exception as e:
        logger.warning(f"File type detection failed: {str(e)}")
        return None

def normalize_security_extension(file_path):
    """
    íšŒì‚¬ ë³´ì•ˆì •ì±…ìœ¼ë¡œ ë³€ê²½ëœ í™•ì¥ìë¥¼ ì›ë˜ í™•ì¥ìë¡œ ë³µì›
    
    Args:
        file_path (str): íŒŒì¼ ê²½ë¡œ
    
    Returns:
        tuple: (ì‹¤ì œ íŒŒì¼ íƒ€ì…, ë³´ì•ˆ í™•ì¥ìì¸ì§€ ì—¬ë¶€)
    """
    # ë³´ì•ˆ í™•ì¥ì ë§¤í•‘
    security_extensions = {
        '.cs': 'csv',      # csv -> cs
        '.xl': 'xlsx',     # xlsx -> xl  
        '.log': 'xlsx',    # log -> xlsx (ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ Excel íŒŒì¼ì„ logë¡œ ìœ„ì¥)
        '.dat': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
        '.txt': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
    }
    
    filename_lower = file_path.lower()
    original_ext = os.path.splitext(filename_lower)[1]
    
    # ë³´ì•ˆ í™•ì¥ìì¸ì§€ í™•ì¸
    if original_ext in security_extensions:
        logger.info(f"ğŸ”’ [SECURITY] Security extension detected: {original_ext}")
        
        if security_extensions[original_ext]:
            # ì§ì ‘ ë§¤í•‘ì´ ìˆëŠ” ê²½ìš°
            normalized_type = security_extensions[original_ext]
            logger.info(f"ğŸ”„ [SECURITY] Extension normalization: {original_ext} -> {normalized_type}")
            return normalized_type, True
        else:
            # ë‚´ìš© ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
            detected_type = detect_file_type_by_content(file_path)
            if detected_type:
                logger.info(f"ğŸ“Š [CONTENT_DETECTION] Detected file type by content: {detected_type}")
                return detected_type, True
            else:
                logger.warning(f"âš ï¸ [CONTENT_DETECTION] Failed to detect file type for: {original_ext}")
                return None, True
    
    # ì¼ë°˜ í™•ì¥ìì¸ ê²½ìš°
    if original_ext == '.csv':
        return 'csv', False
    elif original_ext in ['.xlsx', '.xls']:
        return 'excel', False
    else:
        return None, False

def get_csv_cache_path(file_path, model_type=None, use_file_cache_dirs=True):
    """
    CSV ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_path (str): ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
        use_file_cache_dirs (bool): ê¸°ì¡´ íŒŒì¼ ìºì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        dict: ìºì‹œ ê²½ë¡œ ì •ë³´
    """
    try:
        if use_file_cache_dirs:
            # ê¸°ì¡´ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ êµ¬ì¡° í™œìš©
            cache_dirs = get_file_cache_dirs(file_path)
            cache_root = cache_dirs['root']
        else:
            # ë‹¨ìˆœ ìºì‹œ êµ¬ì¡°
            file_hash = get_data_content_hash(file_path)
            if not file_hash:
                raise ValueError("Cannot generate file hash for caching")
            
            file_name = Path(file_path).stem
            cache_dir_name = f"{file_hash[:12]}_{file_name}"
            cache_root = Path(CACHE_ROOT_DIR) / cache_dir_name
        
        # CSV ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        csv_cache_dir = Path(cache_root) / 'processed_csv'
        csv_cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ íƒ€ì…ë³„ ìºì‹œ íŒŒì¼ëª… (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        if model_type:
            cache_filename = f"data_{model_type}.cs"
            metadata_filename = f"metadata_{model_type}.json"
        else:
            cache_filename = "data.cs"
            metadata_filename = "metadata.json"
        
        return {
            'csv_cache_dir': csv_cache_dir,
            'csv_file': csv_cache_dir / cache_filename,
            'metadata_file': csv_cache_dir / metadata_filename,
            'cache_root': cache_root
        }
        
    except Exception as e:
        logger.error(f"âŒ Failed to get CSV cache path: {str(e)}")
        raise e

def create_csv_cache_metadata(file_path, model_type, processing_info):
    """
    CSV ìºì‹œ ë©”íƒ€ë°ì´í„° ìƒì„±
    
    Args:
        file_path (str): ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
        processing_info (dict): ì²˜ë¦¬ ì •ë³´
    
    Returns:
        dict: ë©”íƒ€ë°ì´í„°
    """
    import hashlib
    import os
    from datetime import datetime
    
    file_stat = os.stat(file_path)
    file_hash = get_data_content_hash(file_path)
    
    metadata = {
        'original_file': {
            'name': os.path.basename(file_path),
            'path': file_path,
            'size': file_stat.st_size,
            'modified_time': file_stat.st_mtime,
            'content_hash': file_hash
        },
        'processing': {
            'model_type': model_type,
            'pipeline_version': '1.0.0',  # ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë²„ì „
            'created_time': datetime.now().isoformat(),
            'processing_info': processing_info
        },
        'csv_cache': {
            'created_time': datetime.now().isoformat(),
            'format_version': '1.0'
        }
    }
    
    return metadata

def is_csv_cache_valid(file_path, model_type=None):
    """
    CSV ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (ë°ì´í„° í™•ì¥ ê³ ë ¤)
    
    Args:
        file_path (str): ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
    
    Returns:
        tuple: (ìœ íš¨ì„±, ìºì‹œ ê²½ë¡œ ì •ë³´, í™•ì¥ ì •ë³´)
    """
    try:
        from app.data.cache_manager import check_data_extension
        
        cache_paths = get_csv_cache_path(file_path, model_type)
        csv_file = cache_paths['csv_file']
        metadata_file = cache_paths['metadata_file']
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not csv_file.exists() or not metadata_file.exists():
            logger.info(f"ğŸ“‹ [CSV_CACHE] Cache files not found for {os.path.basename(file_path)}")
            return False, cache_paths, None
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # ëª¨ë¸ íƒ€ì… í™•ì¸
        processing_info = metadata.get('processing', {})
        if processing_info.get('model_type') != model_type:
            logger.info(f"ğŸ“‹ [CSV_CACHE] Model type changed ({processing_info.get('model_type')} -> {model_type}), cache invalid")
            return False, cache_paths, None
        
        # ğŸ” ì›ë³¸ íŒŒì¼ ë³€ê²½ í™•ì¸ - ë°ì´í„° í™•ì¥ ê³ ë ¤
        original_info = metadata.get('original_file', {})
        original_hash = original_info.get('content_hash')
        current_hash = get_data_content_hash(file_path)
        
        if original_hash == current_hash:
            # í•´ì‹œê°€ ë™ì¼í•˜ë©´ ìºì‹œ ìœ íš¨
            logger.info(f"âœ… [CSV_CACHE] Identical file hash, cache valid for {os.path.basename(file_path)}")
            return True, cache_paths, None
        
        # ğŸš€ í•´ì‹œê°€ ë‹¤ë¥¸ ê²½ìš°: ë°ì´í„° í™•ì¥ì¸ì§€ í™•ì¸
        logger.info(f"ğŸ” [CSV_CACHE] File hash changed, checking for data extension...")
        
        # ì´ì „ ìºì‹œëœ íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ (ë©”íƒ€ë°ì´í„°ì—ì„œ)
        original_file_path = original_info.get('path')
        if not original_file_path or not os.path.exists(original_file_path):
            logger.info(f"ğŸ“‹ [CSV_CACHE] Original file path not found, treating as new file")
            return False, cache_paths, None
        
        # ë°ì´í„° í™•ì¥ ê²€ì‚¬
        try:
            extension_result = check_data_extension(original_file_path, file_path)
            
            if extension_result.get('is_extension', False):
                # ğŸ¯ ë°ì´í„° í™•ì¥ìœ¼ë¡œ í™•ì¸ë¨ - ìºì‹œ ì¬í™œìš© ê°€ëŠ¥
                logger.info(f"âœ… [CSV_CACHE] Data extension detected!")
                logger.info(f"    ğŸ“ˆ Extension type: {extension_result.get('validation_details', {}).get('extension_type', 'Unknown')}")
                logger.info(f"    â• New rows: {extension_result.get('new_rows_count', 0)}")
                logger.info(f"    ğŸ“… Old range: {extension_result.get('old_start_date')} ~ {extension_result.get('old_end_date')}")
                logger.info(f"    ğŸ“… New range: {extension_result.get('new_start_date')} ~ {extension_result.get('new_end_date')}")
                
                # í™•ì¥ëœ ë°ì´í„°ì˜ ê²½ìš° ìºì‹œë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ìœ íš¨í•œ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
                # ê¸°ì¡´ ìºì‹œ + ìƒˆë¡œìš´ ë°ì´í„° ë¶€ë¶„ë§Œ ì²˜ë¦¬í•˜ë„ë¡ ì •ë³´ ì œê³µ
                return "extension", cache_paths, extension_result
            else:
                # í™•ì¥ì´ ì•„ë‹Œ ì™„ì „íˆ ë‹¤ë¥¸ ë°ì´í„°
                logger.info(f"ğŸ“‹ [CSV_CACHE] File changed but not a data extension, cache invalid")
                logger.info(f"    Reason: {extension_result.get('validation_details', {}).get('reason', 'Unknown')}")
                return False, cache_paths, None
                
        except Exception as ext_error:
            logger.warning(f"âš ï¸ [CSV_CACHE] Extension check failed: {str(ext_error)}")
            logger.info(f"ğŸ“‹ [CSV_CACHE] Treating as file change, cache invalid")
            return False, cache_paths, None
        
    except Exception as e:
        logger.warning(f"âš ï¸ [CSV_CACHE] Error checking cache validity: {str(e)}")
        return False, None, None

def create_csv_cache_from_excel(file_path, model_type=None):
    """
    Excel íŒŒì¼ì„ ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ê±°ì³ CSV ìºì‹œë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    global _cache_creation_in_progress
    
    # ë¬´í•œ ë£¨í”„ ë°©ì§€: ì´ë¯¸ ìºì‹œ ìƒì„± ì¤‘ì´ë©´ ì—ëŸ¬ ë°œìƒ
    if _cache_creation_in_progress:
        logger.error("âŒ [CSV_CACHE] Cache creation already in progress - preventing infinite loop")
        raise RuntimeError("Cache creation already in progress to prevent infinite loop")
    
    logger.info(f"ğŸ“Š [CSV_CACHE] Creating CSV cache from Excel with preprocessor: {os.path.basename(file_path)}")
    
    # ìºì‹œ ìƒì„± ì‹œì‘ í”Œë˜ê·¸ ì„¤ì •
    _cache_creation_in_progress = True
    
    # ğŸ”’ ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë¬¼ë¦¬ì  ë³€ê²½ í¬í•¨)
    processed_file_path, actual_ext, is_security_file = process_security_file_in_loader(file_path)
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
    file_path = processed_file_path
    
    # íŒŒì¼ íƒ€ì… ì¬í™•ì¸
    actual_file_type, _ = normalize_security_extension(file_path)
    
    if is_security_file:
        logger.info(f"ğŸ”’ [CSV_CACHE] Security file processed: {actual_file_type}")
    
    processing_start_time = time.time()
    processing_info = {
        'start_time': processing_start_time,
        'file_type': actual_file_type,
        'is_security_file': is_security_file,
        'errors_encountered': [],
        'processing_method': 'unknown'
    }
    
    try:
        # ğŸš€ ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í†µí•´ DataFrame ë¡œë”©
        logger.info("ğŸ”§ [CSV_CACHE] Using complete preprocessing pipeline...")
        df = load_excel_as_dataframe(file_path, model_type)
        
        if df is None or df.empty:
            raise ValueError("Failed to load Excel file - empty or None DataFrame returned")
        
        processing_info['processing_method'] = 'preprocessor_pipeline'
        processing_info['preprocessing_success'] = True
        
        # ë¡œë”© ì„±ê³µ í›„ ì •ë³´ ìˆ˜ì§‘
        processing_info['end_time'] = time.time()
        processing_info['duration_seconds'] = processing_info['end_time'] - processing_start_time
        processing_info['final_shape'] = list(df.shape)
        processing_info['date_range'] = [df.index.min().strftime('%Y-%m-%d'), df.index.max().strftime('%Y-%m-%d')]
        processing_info['columns_count'] = len(df.columns)
        processing_info['columns_sample'] = list(df.columns)[:10]  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ
        
        logger.info(f"ğŸ“Š [CSV_CACHE] Preprocessor pipeline completed: {df.shape} ({processing_info['duration_seconds']:.2f}s)")
        logger.info(f"ğŸ“… [CSV_CACHE] Date range: {processing_info['date_range'][0]} ~ {processing_info['date_range'][1]}")
        logger.info(f"ğŸ“‹ [CSV_CACHE] Columns: {processing_info['columns_count']} total")
        
        # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        missing_count = df.isnull().sum().sum()
        inf_count = 0
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            inf_count = np.isinf(df[numeric_cols].values).sum()
        
        processing_info['data_quality'] = {
            'missing_values': int(missing_count),
            'infinite_values': int(inf_count),
            'numeric_columns': len(numeric_cols),
            'total_columns': len(df.columns)
        }
        
        if missing_count > 0:
            logger.info(f"ğŸ“Š [CSV_CACHE] Data quality: {missing_count} missing values, {inf_count} infinite values")
        else:
            logger.info(f"âœ… [CSV_CACHE] Data quality: No missing or infinite values")
        
        # CSV ìºì‹œ ì €ì¥
        cache_paths = get_csv_cache_path(file_path, model_type)
        csv_file = cache_paths['csv_file']
        metadata_file = cache_paths['metadata_file']
        
        # Date ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
        df_for_csv = df.reset_index()
        
        logger.info(f"ğŸ’¾ [CSV_CACHE] Saving processed data to CSV: {csv_file}")
        
        # CSV ì €ì¥ (UTF-8 ì¸ì½”ë”©, ì¸ë±ìŠ¤ ì œì™¸)
        df_for_csv.to_csv(csv_file, index=False, encoding='utf-8')
        
        # ì €ì¥ëœ íŒŒì¼ í¬ê¸° í™•ì¸
        csv_file_size = os.path.getsize(csv_file) / (1024 * 1024)  # MB
        processing_info['csv_file_size_mb'] = round(csv_file_size, 2)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = create_csv_cache_metadata(file_path, model_type, processing_info)
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… [CSV_CACHE] Cache created successfully:")
        logger.info(f"  ğŸ“ CSV file: {csv_file} ({processing_info['csv_file_size_mb']} MB)")
        logger.info(f"  ğŸ“‹ Metadata: {metadata_file}")
        logger.info(f"  â±ï¸ Processing time: {processing_info['duration_seconds']:.2f}s")
        
        # ìºì‹œ ìƒì„± ì™„ë£Œ í”Œë˜ê·¸ í•´ì œ
        _cache_creation_in_progress = False
        
        return df
        
    except Exception as e:
        processing_info['end_time'] = time.time()
        processing_info['duration_seconds'] = processing_info['end_time'] - processing_start_time
        processing_info['final_error'] = str(e)
        processing_info['preprocessing_success'] = False
        
        logger.error(f"âŒ [CSV_CACHE] Failed to create cache from Excel:")
        logger.error(f"  ğŸ“ File: {os.path.basename(file_path)}")
        logger.error(f"  ğŸ”´ Error: {str(e)}")
        logger.error(f"  â±ï¸ Duration: {processing_info['duration_seconds']:.2f}s")
        
        # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ë©”íƒ€ë°ì´í„°ëŠ” ì €ì¥ (ë””ë²„ê¹…ìš©)
        try:
            cache_paths = get_csv_cache_path(file_path, model_type)
            metadata_file = cache_paths['metadata_file']
            
            # ì‹¤íŒ¨ ì •ë³´ê°€ í¬í•¨ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
            error_metadata = create_csv_cache_metadata(file_path, model_type, processing_info)
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(error_metadata, f, indent=2, ensure_ascii=False)
                
            logger.info(f"ğŸ“‹ [CSV_CACHE] Error metadata saved for debugging: {metadata_file}")
        except Exception as meta_error:
            logger.warning(f"âš ï¸ [CSV_CACHE] Failed to save error metadata: {str(meta_error)}")
        
        # ìºì‹œ ìƒì„± ì‹¤íŒ¨ í”Œë˜ê·¸ í•´ì œ
        _cache_creation_in_progress = False
        
        raise e

def load_csv_cache(file_path, model_type=None):
    """
    CSV ìºì‹œë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜ (pandas ìš°ì„ , xlwingsëŠ” fallback)
    
    Args:
        file_path (str): ì›ë³¸ íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
    
    Returns:
        pd.DataFrame: ìºì‹œëœ ë°ì´í„°í”„ë ˆì„
    """
    cache_paths = get_csv_cache_path(file_path, model_type)
    csv_file = cache_paths['csv_file']
    
    logger.info(f"ğŸ“– [CSV_CACHE] Loading cached CSV: {csv_file}")
    
    try:
        # pandasë¡œ ë¨¼ì € ì‹œë„ (CSVëŠ” pandasê°€ ë” ì•ˆì •ì )
        logger.info(f"ğŸ“Š [CSV_CACHE] Attempting pandas CSV loading...")
        
        # ì—¬ëŸ¬ êµ¬ë¶„ìë¡œ ì‹œë„
        separators = [',', ';', '\t']
        df = None
        
        for sep in separators:
            try:
                df_test = pd.read_csv(csv_file, sep=sep, encoding='utf-8')
                # ë‹¨ì¼ ì»¬ëŸ¼ì— ì‰¼í‘œê°€ í¬í•¨ëœ ê²½ìš° ì²´í¬
                if len(df_test.columns) == 1 and ',' in df_test.columns[0]:
                    logger.warning(f"âš ï¸ [CSV_CACHE] Single column with commas detected using separator '{sep}', trying next...")
                    continue
                else:
                    df = df_test
                    logger.info(f"âœ… [CSV_CACHE] Successfully loaded with separator '{sep}': {df.shape}")
                    break
            except Exception as sep_error:
                logger.warning(f"âš ï¸ [CSV_CACHE] Failed with separator '{sep}': {str(sep_error)}")
                continue
        
        # pandasë¡œ ì„±ê³µí•˜ì§€ ëª»í•œ ê²½ìš° xlwings ì‹œë„
        if df is None and XLWINGS_AVAILABLE:
            logger.info(f"ğŸ”„ [CSV_CACHE] Pandas failed, trying xlwings...")
            try:
                df = load_csv_with_xlwings(str(csv_file))
                
                # xlwingsë¡œ ì½ì–´ë„ ë‹¨ì¼ ì»¬ëŸ¼ ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸
                if len(df.columns) == 1 and ',' in df.columns[0]:
                    logger.warning(f"âš ï¸ [CSV_CACHE] xlwings also returned single column issue, attempting manual parsing...")
                    # ìˆ˜ë™ìœ¼ë¡œ CSV íŒŒì‹± ì‹œë„
                    with open(csv_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    # ì²« ë²ˆì§¸ ì¤„ì„ íŒŒì‹±í•˜ì—¬ ì»¬ëŸ¼ í™•ì¸
                    if lines:
                        header = lines[0].strip()
                        if ',' in header:
                            # ì‰¼í‘œë¡œ ë¶„ë¦¬ëœ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ê³  ì¬ì‹œë„
                            df = pd.read_csv(csv_file, sep=',', encoding='utf-8')
                            logger.info(f"âœ… [CSV_CACHE] Manual parsing successful: {df.shape}")
                        
            except Exception as xlwings_error:
                logger.warning(f"âš ï¸ [CSV_CACHE] xlwings also failed: {str(xlwings_error)}")
        
        # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•œ ê²½ìš°
        if df is None:
            raise ValueError("Failed to load CSV cache with both pandas and xlwings")
        
        # ì»¬ëŸ¼ í™•ì¸ ë° ë¡œê¹…
        logger.info(f"ğŸ“‹ [CSV_CACHE] Loaded columns: {list(df.columns)}")
        
        # Date ì»¬ëŸ¼ ì²˜ë¦¬
        if 'Date' in df.columns:
            logger.info(f"âœ… [CSV_CACHE] Date column found, converting to datetime...")
            df['Date'] = pd.to_datetime(df['Date'])
            df.set_index('Date', inplace=True)
            logger.info(f"ğŸ“… [CSV_CACHE] Date range: {df.index.min()} ~ {df.index.max()}")
        else:
            logger.error(f"âŒ [DATE_COLUMN] Date column not found. Available columns: {list(df.columns)}")
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸
            first_col = df.columns[0]
            try:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ëª‡ ê°œ ê°’ì„ ë‚ ì§œë¡œ íŒŒì‹±í•´ë³´ê¸°
                test_dates = pd.to_datetime(df[first_col].head(5), errors='coerce')
                if test_dates.notna().sum() >= 3:  # 5ê°œ ì¤‘ 3ê°œ ì´ìƒì´ ìœ íš¨í•œ ë‚ ì§œë©´
                    logger.info(f"ğŸ”„ [DATE_COLUMN] Using first column '{first_col}' as Date")
                    df = df.rename(columns={first_col: 'Date'})
                    df['Date'] = pd.to_datetime(df['Date'])
                    df.set_index('Date', inplace=True)
                else:
                    logger.error(f"âŒ [DATE_COLUMN] First column is not date format: {df[first_col].iloc[0]}")
                    raise ValueError(f"No valid Date column found in cached CSV")
            except Exception as date_error:
                logger.error(f"âŒ [DATE_COLUMN] First column date parsing failed: {str(date_error)}")
                raise ValueError(f"No valid Date column found in cached CSV")
        
        logger.info(f"âœ… [CSV_CACHE] Cache loaded successfully: {df.shape}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [CSV_CACHE] Failed to load cache: {str(e)}")
        raise e

def load_excel_as_dataframe(file_path, model_type=None):
    """
    Excel íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜ (preprocessor ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
    
    Returns:
        pd.DataFrame: ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
    """
    logger.info(f"ğŸ“Š [EXCEL_TO_DF] Loading Excel as DataFrame with preprocessor: {os.path.basename(file_path)}")
    
    # ğŸ”’ ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë¬¼ë¦¬ì  ë³€ê²½ í¬í•¨)
    processed_file_path, actual_ext, is_security_file = process_security_file_in_loader(file_path)
    
    # ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œë¡œ ì—…ë°ì´íŠ¸
    file_path = processed_file_path
    
    # íŒŒì¼ íƒ€ì… ì¬í™•ì¸
    actual_file_type, _ = normalize_security_extension(file_path)
    
    if is_security_file:
        logger.info(f"ğŸ”’ [EXCEL_TO_DF] Security file processed: {actual_file_type}")
    
    df = None
    
    try:
        # ğŸš€ 1ë‹¨ê³„: preprocessorì˜ ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìš°ì„  ì‹œë„
        logger.info("ğŸ”§ [EXCEL_TO_DF] Attempting complete preprocessing pipeline...")
        try:
            # ì ì ˆí•œ ì‹œíŠ¸ ì´ë¦„ ê²°ì •
            sheet_name = '29 Nov 2010 till todate'
            try:
                # ì‹œíŠ¸ ëª©ë¡ í™•ì¸ (ë³´ì•ˆ íŒŒì¼ë„ ì§€ì›í•˜ë„ë¡ ì•ˆì „í•œ ë°©ì‹ ì‚¬ìš©)
                if XLWINGS_AVAILABLE:
                    try:
                        # xlwingsë¡œ ì‹œíŠ¸ ëª©ë¡ í™•ì¸ ì‹œë„
                        import xlwings as xw
                        app = xw.App(visible=False, add_book=False)
                        wb = app.books.open(file_path, read_only=True)
                        available_sheets = [sheet.name for sheet in wb.sheets]
                        wb.close()
                        app.quit()
                        logger.info(f"ğŸ“‹ [EXCEL_TO_DF] Available sheets (xlwings): {available_sheets}")
                    except:
                        # xlwings ì‹¤íŒ¨ ì‹œ pandasë¡œ ì‹œíŠ¸ í™•ì¸
                        excel_file = pd.ExcelFile(file_path, engine='openpyxl')
                        available_sheets = excel_file.sheet_names
                        logger.info(f"ğŸ“‹ [EXCEL_TO_DF] Available sheets (pandas): {available_sheets}")
                else:
                    excel_file = pd.ExcelFile(file_path, engine='openpyxl')
                    available_sheets = excel_file.sheet_names
                    logger.info(f"ğŸ“‹ [EXCEL_TO_DF] Available sheets: {available_sheets}")
                
                if sheet_name not in available_sheets:
                    sheet_name = available_sheets[0]
                    logger.info(f"ğŸ¯ [EXCEL_TO_DF] Using first sheet: {sheet_name}")
                else:
                    logger.info(f"ğŸ¯ [EXCEL_TO_DF] Using target sheet: {sheet_name}")
            except Exception as sheet_error:
                logger.warning(f"âš ï¸ [EXCEL_TO_DF] Sheet detection failed: {str(sheet_error)}")
                sheet_name = 0  # ì¸ë±ìŠ¤ë¡œ ì²« ë²ˆì§¸ ì‹œíŠ¸ ì‚¬ìš©
                logger.info(f"ğŸ¯ [EXCEL_TO_DF] Using first sheet by index: {sheet_name}")
            
            # ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            from app.data.preprocessor import process_excel_data_complete
            df = process_excel_data_complete(file_path, sheet_name, start_date='2013-01-04')
            
            if df is not None and not df.empty:
                logger.info("âœ… [EXCEL_TO_DF] Complete preprocessing pipeline succeeded")
                logger.info(f"ğŸ“Š [EXCEL_TO_DF] Preprocessed data shape: {df.shape}")
                logger.info(f"ğŸ“‹ [EXCEL_TO_DF] Columns: {len(df.columns)} - {list(df.columns)[:5]}...")
            else:
                raise ValueError("Preprocessor returned None or empty DataFrame")
                
        except Exception as preprocessor_error:
            logger.warning(f"âš ï¸ [EXCEL_TO_DF] Complete preprocessing failed: {str(preprocessor_error)}")
            logger.info("ğŸ”„ [EXCEL_TO_DF] Falling back to individual processing steps...")
            
            # 2ë‹¨ê³„: ê°œë³„ ì²˜ë¦¬ ë‹¨ê³„ë¡œ fallback
            df = None
            
            # xlwings ì‹œë„
            if XLWINGS_AVAILABLE and actual_file_type in ['xlsx', 'xls', 'excel']:
                try:
                    logger.info("ğŸ”“ [EXCEL_TO_DF] Attempting Excel load with xlwings...")
                    df = load_data_with_xlwings(file_path, model_type)
                    logger.info("âœ… [EXCEL_TO_DF] Excel loaded successfully with xlwings")
                except Exception as xlwings_error:
                    logger.warning(f"âš ï¸ [EXCEL_TO_DF] xlwings failed: {str(xlwings_error)}")
            
            # pandas fallback
            if df is None:
                logger.info("ğŸ”„ [EXCEL_TO_DF] Attempting pandas Excel loading...")
                try:
                    df = safe_read_excel(file_path, sheet_name=sheet_name)
                    # ë” ê²¬ê³ í•œ ë‚ ì§œ íŒŒì‹± - ì˜ëª»ëœ í˜•ì‹(ì˜ˆ: "1 Mac 2011") ì²˜ë¦¬
                    df['Date'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True, format='mixed')
                    # íŒŒì‹± ì‹¤íŒ¨í•œ ë‚ ì§œ ì œê±°
                    invalid_dates = df['Date'].isna().sum()
                    if invalid_dates > 0:
                        logger.warning(f"âš ï¸ {invalid_dates}ê°œì˜ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ì„ ë°œê²¬í•˜ì—¬ ì œê±°í–ˆìŠµë‹ˆë‹¤.")
                        df = df.dropna(subset=['Date'])
                    logger.info("âœ… [EXCEL_TO_DF] Excel loaded successfully with pandas")
                except Exception as pandas_error:
                    error_msg = f"pandas also failed: {str(pandas_error)}"
                    logger.error(f"âŒ [EXCEL_TO_DF] {error_msg}")
                    raise Exception(f"All loading methods failed. Preprocessor: {preprocessor_error}, Pandas: {pandas_error}")
        
        # Date ì»¬ëŸ¼ì´ ì¸ë±ìŠ¤ê°€ ì•„ë‹Œ ê²½ìš° ì¸ë±ìŠ¤ë¡œ ì„¤ì •
        if 'Date' in df.columns and df.index.name != 'Date':
            df.set_index('Date', inplace=True)
        elif df.index.name != 'Date' and 'Date' not in df.columns:
            # Date ì»¬ëŸ¼ë„ ì¸ë±ìŠ¤ë„ ì•„ë‹Œ ê²½ìš° ì²« ë²ˆì§¸ datetime ì»¬ëŸ¼ ì°¾ê¸°
            for col in df.columns:
                if df[col].dtype == 'datetime64[ns]' or 'date' in col.lower():
                    df = df.rename(columns={col: 'Date'})
                    df.set_index('Date', inplace=True)
                    logger.info(f"ğŸ”„ [EXCEL_TO_DF] Set '{col}' as Date index")
                    break
        
        # ëª¨ë¸ íƒ€ì…ë³„ í•„í„°ë§ ì ìš©
        if model_type == 'lstm':
            cutoff_date = pd.to_datetime('2022-01-01')
            original_shape = df.shape
            df = df[df.index >= cutoff_date]
            
            logger.info(f"ğŸ“Š [EXCEL_TO_DF] LSTM filter applied: {original_shape[0]} -> {df.shape[0]} rows")
            
            if df.empty:
                raise ValueError("No data available after 2022-01-01 filter for LSTM model")
        
        # ìµœì¢… ë°ì´í„° ì •ì œ (preprocessorê°€ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë§Œ)
        if df is not None:
            # ë¬´í•œê°’ ì²˜ë¦¬
            df = df.replace([np.inf, -np.inf], np.nan)
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆì„ ìˆ˜ ìˆìŒ)
            missing_count = df.isnull().sum().sum()
            if missing_count > 0:
                logger.info(f"ğŸ”§ [EXCEL_TO_DF] Handling {missing_count} remaining missing values...")
                df = df.ffill().bfill()
        
        logger.info(f"âœ… [EXCEL_TO_DF] DataFrame created successfully: {df.shape}")
        logger.info(f"ğŸ“… [EXCEL_TO_DF] Date range: {df.index.min()} ~ {df.index.max()}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ [EXCEL_TO_DF] Failed to load Excel as DataFrame: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise e

def convert_excel_to_temp_csv(file_path, model_type=None, temp_dir=None):
    """
    Excel íŒŒì¼ì„ ì™„ì „í•œ ì „ì²˜ë¦¬ë¥¼ ê±°ì³ ì„ì‹œ CSVë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (ë¹„êµìš©)
    
    Args:
        file_path (str): Excel íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
        temp_dir (str): ì„ì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì‹œìŠ¤í…œ ê¸°ë³¸ê°’)
    
    Returns:
        tuple: (ì„ì‹œ CSV íŒŒì¼ ê²½ë¡œ, DataFrame)
    """
    import tempfile
    
    logger.info(f"ğŸ”„ [TEMP_CSV] Converting Excel to temporary CSV with preprocessor: {os.path.basename(file_path)}")
    
    try:
        # ì™„ì „í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í†µí•œ DataFrame ë¡œë”© (ìºì‹œ ìƒì„± ì—†ì´)
        logger.info(f"ğŸ”§ [TEMP_CSV] Using preprocessing pipeline for comparison...")
        df = load_excel_as_dataframe(file_path, model_type)
        
        if df is None or df.empty:
            raise ValueError("Failed to load Excel file for temporary CSV conversion")
        
        logger.info(f"ğŸ“Š [TEMP_CSV] Preprocessed data loaded: {df.shape}")
        
        # ì„ì‹œ CSV íŒŒì¼ ìƒì„±
        if temp_dir is None:
            temp_dir = tempfile.gettempdir()
        
        # íŒŒì¼ëª…ì— íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ëª¨ë¸íƒ€ì… ì¶”ê°€ (êµ¬ë¶„ì„ ìœ„í•´)
        timestamp = int(time.time())
        model_suffix = f"_{model_type}" if model_type else ""
        temp_csv_path = os.path.join(temp_dir, f"temp_{timestamp}{model_suffix}_{os.path.basename(file_path)}.csv")
        
        # Date ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ CSV ì €ì¥
        df_for_csv = df.reset_index()
        df_for_csv.to_csv(temp_csv_path, index=False, encoding='utf-8')
        
        # ì„ì‹œ íŒŒì¼ í¬ê¸° í™•ì¸
        temp_file_size = os.path.getsize(temp_csv_path) / (1024 * 1024)  # MB
        
        logger.info(f"âœ… [TEMP_CSV] Temporary CSV created:")
        logger.info(f"  ğŸ“ File: {temp_csv_path}")
        logger.info(f"  ğŸ“Š Data: {df.shape} ({temp_file_size:.2f} MB)")
        logger.info(f"  ğŸ“… Date range: {df.index.min()} ~ {df.index.max()}")
        
        return temp_csv_path, df
        
    except Exception as e:
        logger.error(f"âŒ [TEMP_CSV] Failed to convert Excel to temporary CSV:")
        logger.error(f"  ğŸ“ File: {os.path.basename(file_path)}")
        logger.error(f"  ğŸ”´ Error: {str(e)}")
        raise e

def compare_csv_files(csv_file1, csv_file2, tolerance=1e-6):
    """
    ë‘ CSV íŒŒì¼ì˜ ë‚´ìš©ì„ ë¹„êµí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        csv_file1 (str): ì²« ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ
        csv_file2 (str): ë‘ ë²ˆì§¸ CSV íŒŒì¼ ê²½ë¡œ
        tolerance (float): ìˆ˜ì¹˜ ë¹„êµ í—ˆìš© ì˜¤ì°¨
    
    Returns:
        dict: ë¹„êµ ê²°ê³¼ ì •ë³´
    """
    try:
        logger.info(f"ğŸ” [CSV_COMPARE] Comparing CSV files:")
        logger.info(f"  ğŸ“„ File 1: {os.path.basename(csv_file1)}")
        logger.info(f"  ğŸ“„ File 2: {os.path.basename(csv_file2)}")
        
        # CSV íŒŒì¼ ë¡œë”© (xlwings ì‚¬ìš©)
        if XLWINGS_AVAILABLE:
            df1 = load_csv_safe_with_fallback(csv_file1)
            df2 = load_csv_safe_with_fallback(csv_file2)
        else:
            df1 = pd.read_csv(csv_file1)
            df2 = pd.read_csv(csv_file2)
        
        # ê¸°ë³¸ í˜•íƒœ ë¹„êµ
        if df1.shape != df2.shape:
            logger.info(f"ğŸ“Š [CSV_COMPARE] Shape mismatch: {df1.shape} vs {df2.shape}")
            return {
                'is_identical': False,
                'is_extension': df2.shape[0] > df1.shape[0] and df2.shape[1] == df1.shape[1],
                'reason': f'Shape difference: {df1.shape} vs {df2.shape}',
                'shape1': df1.shape,
                'shape2': df2.shape
            }
        
        # ì»¬ëŸ¼ ë¹„êµ
        if list(df1.columns) != list(df2.columns):
            logger.info(f"ğŸ“‹ [CSV_COMPARE] Column mismatch")
            return {
                'is_identical': False,
                'is_extension': False,
                'reason': 'Column structure difference',
                'columns1': list(df1.columns),
                'columns2': list(df2.columns)
            }
        
        # Date ì»¬ëŸ¼ ì²˜ë¦¬
        if 'Date' in df1.columns:
            df1['Date'] = pd.to_datetime(df1['Date'])
            df2['Date'] = pd.to_datetime(df2['Date'])
            
            # ë‚ ì§œë¡œ ì •ë ¬
            df1 = df1.sort_values('Date').reset_index(drop=True)
            df2 = df2.sort_values('Date').reset_index(drop=True)
        
        # ë‚´ìš© ë¹„êµ
        differences = []
        
        # ê° ì»¬ëŸ¼ë³„ë¡œ ë¹„êµ
        for col in df1.columns:
            if col == 'Date':
                # ë‚ ì§œ ë¹„êµ
                if not df1[col].equals(df2[col]):
                    date_diffs = df1[col] != df2[col]
                    diff_count = date_diffs.sum()
                    if diff_count > 0:
                        differences.append(f"Date column: {diff_count} differences")
            else:
                # ìˆ˜ì¹˜ ë°ì´í„° ë¹„êµ
                try:
                    # ìˆ˜ì¹˜ ë³€í™˜ ì‹œë„
                    col1_numeric = pd.to_numeric(df1[col], errors='coerce')
                    col2_numeric = pd.to_numeric(df2[col], errors='coerce')
                    
                    # NaN ê°œìˆ˜ ë¹„êµ
                    nan1 = col1_numeric.isna().sum()
                    nan2 = col2_numeric.isna().sum()
                    
                    if nan1 != nan2:
                        differences.append(f"Column {col}: NaN count difference ({nan1} vs {nan2})")
                        continue
                    
                    # ìˆ˜ì¹˜ ë¹„êµ (NaN ì œì™¸)
                    valid_mask = col1_numeric.notna() & col2_numeric.notna()
                    if valid_mask.sum() > 0:
                        valid1 = col1_numeric[valid_mask]
                        valid2 = col2_numeric[valid_mask]
                        
                        if not np.allclose(valid1, valid2, rtol=tolerance, atol=tolerance, equal_nan=True):
                            max_diff = np.abs(valid1 - valid2).max()
                            differences.append(f"Column {col}: numeric differences (max: {max_diff})")
                        
                except:
                    # ìˆ˜ì¹˜ ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ë¹„êµ
                    if not df1[col].equals(df2[col]):
                        diff_count = (df1[col] != df2[col]).sum()
                        differences.append(f"Column {col}: {diff_count} string differences")
        
        # ê²°ê³¼ íŒì •
        is_identical = len(differences) == 0
        
        comparison_result = {
            'is_identical': is_identical,
            'is_extension': False,  # ê°™ì€ shapeì—ì„œëŠ” í™•ì¥ì´ ì•„ë‹˜
            'reason': 'Identical files' if is_identical else f'{len(differences)} differences found',
            'differences': differences,
            'shape1': df1.shape,
            'shape2': df2.shape,
            'tolerance_used': tolerance
        }
        
        if is_identical:
            logger.info(f"âœ… [CSV_COMPARE] Files are identical")
        else:
            logger.info(f"âŒ [CSV_COMPARE] Files differ: {len(differences)} differences")
            for diff in differences[:3]:  # ì²˜ìŒ 3ê°œë§Œ ë¡œê¹…
                logger.info(f"  - {diff}")
        
        return comparison_result
        
    except Exception as e:
        logger.error(f"âŒ [CSV_COMPARE] Error comparing CSV files: {str(e)}")
        return {
            'is_identical': False,
            'is_extension': False,
            'reason': f'Comparison error: {str(e)}',
            'error': str(e)
        }

def check_data_extension_csv_based(existing_excel_path, new_excel_path, model_type=None):
    """
    CSV ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° í™•ì¥ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        existing_excel_path (str): ê¸°ì¡´ Excel íŒŒì¼ ê²½ë¡œ
        new_excel_path (str): ìƒˆ Excel íŒŒì¼ ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì…
    
    Returns:
        dict: í™•ì¥ ì •ë³´
    """
    temp_csv1 = None
    temp_csv2 = None
    
    try:
        logger.info(f"ğŸ” [CSV_EXTENSION] Checking data extension using CSV conversion")
        logger.info(f"  ğŸ“„ Existing: {os.path.basename(existing_excel_path)}")
        logger.info(f"  ğŸ“„ New: {os.path.basename(new_excel_path)}")
        
        # ë‘ Excel íŒŒì¼ì„ ì„ì‹œ CSVë¡œ ë³€í™˜
        temp_csv1, df1 = convert_excel_to_temp_csv(existing_excel_path, model_type)
        temp_csv2, df2 = convert_excel_to_temp_csv(new_excel_path, model_type)
        
        # ê¸°ì¡´ check_data_extension ë¡œì§ì„ DataFrameìœ¼ë¡œ ì ìš©
        result = check_dataframes_extension(df1, df2, existing_excel_path, new_excel_path)
        
        # CSV ê¸°ë°˜ ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
        result['csv_based_comparison'] = True
        result['temp_csv_paths'] = [temp_csv1, temp_csv2]
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ [CSV_EXTENSION] CSV-based extension check failed: {str(e)}")
        return {
            'is_extension': False,
            'new_rows_count': 0,
            'csv_based_comparison': True,
            'validation_details': {'error': str(e)}
        }
        
    finally:
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        for temp_csv in [temp_csv1, temp_csv2]:
            if temp_csv and os.path.exists(temp_csv):
                try:
                    os.remove(temp_csv)
                    logger.info(f"ğŸ—‘ï¸ [CSV_EXTENSION] Cleaned up temporary file: {os.path.basename(temp_csv)}")
                except:
                    logger.warning(f"âš ï¸ [CSV_EXTENSION] Failed to clean up: {temp_csv}")

def check_dataframes_extension(old_df, new_df, old_file_path, new_file_path):
    """
    DataFrame ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° í™•ì¥ì„ í™•ì¸í•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ check_data_extension ë¡œì§ ì¬í™œìš©)
    
    Args:
        old_df (pd.DataFrame): ê¸°ì¡´ ë°ì´í„°í”„ë ˆì„
        new_df (pd.DataFrame): ìƒˆ ë°ì´í„°í”„ë ˆì„
        old_file_path (str): ê¸°ì¡´ íŒŒì¼ ê²½ë¡œ (ë¡œê¹…ìš©)
        new_file_path (str): ìƒˆ íŒŒì¼ ê²½ë¡œ (ë¡œê¹…ìš©)
    
    Returns:
        dict: í™•ì¥ ì •ë³´
    """
    try:
        # Date ì¸ë±ìŠ¤ë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë³€í™˜ (ë¹„êµìš©)
        if old_df.index.name == 'Date':
            old_df = old_df.reset_index()
        if new_df.index.name == 'Date':
            new_df = new_df.reset_index()
        
        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸
        if 'Date' not in old_df.columns or 'Date' not in new_df.columns:
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'validation_details': {'error': 'No Date column found'}
            }
        
        # ë‚ ì§œë¡œ ì •ë ¬
        old_df = old_df.sort_values('Date').reset_index(drop=True)
        new_df = new_df.sort_values('Date').reset_index(drop=True)
        
        # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        old_df['Date'] = pd.to_datetime(old_df['Date'])
        new_df['Date'] = pd.to_datetime(new_df['Date'])
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        old_start_date = old_df['Date'].iloc[0]
        old_end_date = old_df['Date'].iloc[-1]
        new_start_date = new_df['Date'].iloc[0]
        new_end_date = new_df['Date'].iloc[-1]
        
        logger.info(f"ğŸ” [DF_EXTENSION] Old data: {old_start_date.strftime('%Y-%m-%d')} ~ {old_end_date.strftime('%Y-%m-%d')} ({len(old_df)} rows)")
        logger.info(f"ğŸ” [DF_EXTENSION] New data: {new_start_date.strftime('%Y-%m-%d')} ~ {new_end_date.strftime('%Y-%m-%d')} ({len(new_df)} rows)")
        
        # í™•ì¥ ê²€ì¦ (ê¸°ì¡´ ë¡œì§ ì¬ì‚¬ìš©)
        if len(new_df) <= len(old_df):
            logger.info(f"âŒ [DF_EXTENSION] New file is not longer ({len(new_df)} <= {len(old_df)})")
            return {
                'is_extension': False, 
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New file is not longer than old file'}
            }
        
        # ìƒˆ ë°ì´í„°ê°€ ê¸°ì¡´ ë°ì´í„°ë³´ë‹¤ ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
        has_more_data = (new_start_date < old_start_date) or (new_end_date > old_end_date) or (len(new_df) > len(old_df))
        if not has_more_data:
            logger.info(f"âŒ [DF_EXTENSION] New data doesn't provide additional information")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': 'New data does not provide additional information'}
            }
        
        # ê¸°ì¡´ ë°ì´í„°ì˜ ëª¨ë“  ë‚ ì§œê°€ ìƒˆ ë°ì´í„°ì— í¬í•¨ë˜ì–´ì•¼ í•¨
        old_dates = set(old_df['Date'].dt.strftime('%Y-%m-%d'))
        new_dates = set(new_df['Date'].dt.strftime('%Y-%m-%d'))
        
        missing_dates = old_dates - new_dates
        if missing_dates:
            logger.info(f"âŒ [DF_EXTENSION] Some old dates are missing in new data: {len(missing_dates)} dates")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'old_start_date': old_start_date.strftime('%Y-%m-%d'),
                'old_end_date': old_end_date.strftime('%Y-%m-%d'),
                'new_start_date': new_start_date.strftime('%Y-%m-%d'),
                'new_end_date': new_end_date.strftime('%Y-%m-%d'),
                'validation_details': {'reason': f'Missing {len(missing_dates)} dates from old data'}
            }
        
        # ì»¬ëŸ¼ êµ¬ì¡° í™•ì¸
        if list(old_df.columns) != list(new_df.columns):
            logger.info(f"âŒ [DF_EXTENSION] Column structure differs")
            return {
                'is_extension': False,
                'new_rows_count': 0,
                'validation_details': {'reason': 'Column structure differs'}
            }
        
        # í™•ì¥ ìœ í˜• ë¶„ì„
        new_only_dates = new_dates - old_dates
        extension_type = []
        if new_start_date < old_start_date:
            past_dates = len([d for d in new_only_dates if pd.to_datetime(d) < old_start_date])
            extension_type.append(f"ê³¼ê±° ë°ì´í„° {past_dates}ê°œ ì¶”ê°€")
        if new_end_date > old_end_date:
            future_dates = len([d for d in new_only_dates if pd.to_datetime(d) > old_end_date])
            extension_type.append(f"ë¯¸ë˜ ë°ì´í„° {future_dates}ê°œ ì¶”ê°€")
        
        extension_desc = " + ".join(extension_type) if extension_type else "ë°ì´í„° ë³´ì™„"
        
        # ëª¨ë“  ê²€ì¦ í†µê³¼: ë°ì´í„° í™•ì¥ìœ¼ë¡œ ì¸ì •
        new_rows_count = len(new_only_dates)
        
        logger.info(f"âœ… [DF_EXTENSION] Valid data extension: {extension_desc} (+{new_rows_count} new dates)")
        
        return {
            'is_extension': True,
            'new_rows_count': new_rows_count,
            'old_start_date': old_start_date.strftime('%Y-%m-%d'),
            'old_end_date': old_end_date.strftime('%Y-%m-%d'),
            'new_start_date': new_start_date.strftime('%Y-%m-%d'),
            'new_end_date': new_end_date.strftime('%Y-%m-%d'),
            'validation_details': {
                'reason': f'Valid data extension: {extension_desc}',
                'new_dates_added': sorted(list(new_only_dates)),
                'extension_type': extension_type
            }
        }
        
    except Exception as e:
        logger.error(f"DataFrame extension check failed: {str(e)}")
        return {
            'is_extension': False, 
            'new_rows_count': 0,
            'validation_details': {'error': str(e)}
        }

def process_security_file_in_loader(file_path):
    """
    loader.pyìš© ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ - ì‹¤ì œ íŒŒì¼ í™•ì¥ìë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ë³€ê²½
    
    Args:
        file_path (str): ì›ë³¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        tuple: (ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œ, ì‹¤ì œ í™•ì¥ì, ë³´ì•ˆ íŒŒì¼ ì—¬ë¶€)
    """
    # ë³´ì•ˆ í™•ì¥ì í™•ì¸
    actual_file_type, is_security_file = normalize_security_extension(file_path)
    
    if not is_security_file:
        # ë³´ì•ˆ íŒŒì¼ì´ ì•„ë‹ˆë©´ ì›ë³¸ ê²½ë¡œ ë°˜í™˜
        original_ext = os.path.splitext(file_path.lower())[1]
        return file_path, original_ext, False
    
    logger.info(f"ğŸ”’ [LOADER_SECURITY] Processing security file: {os.path.basename(file_path)}")
    
    # íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ ì‹¤ì œ íƒ€ì… ê°ì§€ (í•„ìš”í•œ ê²½ìš°)
    if actual_file_type is None:
        detected_type = detect_file_type_by_content(file_path)
        if detected_type:
            actual_file_type = detected_type
            logger.info(f"ğŸ“Š [LOADER_CONTENT_DETECTION] Detected file type: {detected_type}")
        else:
            logger.error(f"âŒ [LOADER_SECURITY] Cannot determine file type for: {os.path.basename(file_path)}")
            return file_path, None, True
    
    # ìƒˆë¡œìš´ í™•ì¥ì ê²°ì •
    if actual_file_type == 'csv':
        new_ext = '.csv'
    elif actual_file_type in ['xlsx', 'excel']:
        new_ext = '.xlsx'
    elif actual_file_type == 'xls':
        new_ext = '.xls'
    else:
        logger.warning(f"âš ï¸ [LOADER_SECURITY] Unsupported file type: {actual_file_type}")
        return file_path, None, True
    
    # ìƒˆë¡œìš´ íŒŒì¼ ê²½ë¡œ ìƒì„±
    base_path = os.path.splitext(file_path)[0]
    new_file_path = f"{base_path}{new_ext}"
    
    # íŒŒì¼ëª…ì´ ì´ë¯¸ ì˜¬ë°”ë¥¸ í™•ì¥ìì¸ ê²½ìš°
    if new_file_path == file_path:
        logger.info(f"ğŸ“‹ [LOADER_SECURITY] File already has correct extension: {os.path.basename(file_path)}")
        return file_path, new_ext, True
    
    # íŒŒì¼ ì´ë¦„ ë³€ê²½ (í™•ì¥ì ìˆ˜ì •)
    try:
        shutil.move(file_path, new_file_path)
        logger.info(f"ğŸ“ [LOADER_SECURITY] File extension corrected: {os.path.basename(file_path)} -> {os.path.basename(new_file_path)}")
        return new_file_path, new_ext, True
    except Exception as e:
        logger.warning(f"âš ï¸ [LOADER_SECURITY] Failed to rename file: {str(e)}")
        # íŒŒì¼ëª… ë³€ê²½ì— ì‹¤íŒ¨í•´ë„ ì›ë³¸ ê²½ë¡œë¡œ ê³„ì† ì§„í–‰
        return file_path, new_ext, True