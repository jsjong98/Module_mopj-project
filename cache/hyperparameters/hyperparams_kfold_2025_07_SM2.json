{
  "sequence_length": 51,
  "hidden_size": 208,
  "num_layers": 6,
  "dropout": 0.4579309401710595,
  "batch_size": 54,
  "learning_rate": 0.0069782812651260325,
  "num_epochs": 63,
  "patience": 14,
  "loss_alpha": 0.5180909155642153,
  "loss_beta": 0.16506606615265287,
  "warmup_steps": 382,
  "lr_factor": 0.49,
  "lr_patience": 8,
  "min_lr": 1e-07,
  "loss_gamma": 0.07,
  "loss_delta": 0.07
}