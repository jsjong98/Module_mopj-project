import pandas as pd
import numpy as np
import torch
import logging
import traceback
from datetime import timedelta
import pandas as pd
from sklearn.preprocessing import StandardScaler
import os
from pathlib import Path
import json

from app.utils.file_utils import set_seed
from app.data.loader import load_data # generate_predictionsì—ì„œ ì‚¬ìš©
from app.data.preprocessor import variable_groups
from app.utils.date_utils import is_holiday # is_holidayì™€ variable_groupsëŠ” ì—¬ê¸°ì— í•„ìš”
from app.data.cache_manager import get_file_cache_dirs, save_prediction_simple, find_compatible_cache_file
from app.models.lstm_model import ImprovedLSTMPredictor, optimize_hyperparameters_semimonthly_kfold
from app.core.gpu_manager import log_device_usage, check_gpu_availability
from app.utils.date_utils import get_next_n_business_days, get_semimonthly_period, get_next_semimonthly_period, get_semimonthly_date_range, format_date
from app.prediction.metrics import calculate_interval_averages_and_scores, decide_purchase_interval, compute_performance_metrics_improved, calculate_moving_averages_with_history
from app.visualization.attention_viz import visualize_attention_weights
from app.visualization.plotter import plot_prediction_basic, plot_moving_average_analysis
from app.utils.serialization import safe_serialize_value, convert_to_legacy_format
from app.core.state_manager import prediction_state
from app.config import UPLOAD_FOLDER

DEFAULT_DEVICE, CUDA_AVAILABLE = check_gpu_availability()
logger = logging.getLogger(__name__)

def generate_predictions(df, current_date, predict_window=23, features=None, target_col='MOPJ', file_path=None):
    """
    ê°œì„ ëœ ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜ - ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
    ğŸ”‘ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€: current_date ì´í›„ì˜ ì‹¤ì œê°’ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    """
    try:
        from app.data.preprocessor import select_features_from_groups
        from app.utils.date_utils import get_next_semimonthly_dates
        from app.models.lstm_model import train_model
        
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # í˜„ì¬ ë‚ ì§œê°€ ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
        if isinstance(current_date, str):
            current_date = pd.to_datetime(current_date)
        
        # í˜„ì¬ ë‚ ì§œ ê²€ì¦ (ë°ì´í„° ê¸°ì¤€ì¼)
        if current_date not in df.index:
            closest_date = df.index[df.index <= current_date][-1]
            logger.warning(f"Current date {current_date} not found in dataframe. Using closest date: {closest_date}")
            current_date = closest_date
        
        # ì˜ˆì¸¡ ì‹œì‘ì¼ ê³„ì‚°
        prediction_start_date = current_date + pd.Timedelta(days=1)
        while prediction_start_date.weekday() >= 5 or is_holiday(prediction_start_date):
            prediction_start_date += pd.Timedelta(days=1)
        
        # ë°˜ì›” ê¸°ê°„ ê³„ì‚°
        data_semimonthly_period = get_semimonthly_period(current_date)
        prediction_semimonthly_period = get_semimonthly_period(prediction_start_date)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë°˜ì›” ê³„ì‚°
        next_semimonthly_period = get_next_semimonthly_period(prediction_start_date)
        
        logger.info(f"ğŸ¯ Prediction Setup:")
        logger.info(f"  ğŸ“… Data base date: {current_date} (period: {data_semimonthly_period})")
        logger.info(f"  ğŸš€ Prediction start date: {prediction_start_date} (period: {prediction_semimonthly_period})")
        logger.info(f"  ğŸ¯ Purchase interval target period: {next_semimonthly_period}")
        
        # 23ì¼ì¹˜ ì˜ˆì¸¡ì„ ìœ„í•œ ë‚ ì§œ ìƒì„±
        all_business_days = get_next_n_business_days(current_date, df, predict_window)
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ì‹œì‘ì¼ ê¸°ì¤€ìœ¼ë¡œ êµ¬ë§¤ êµ¬ê°„ ê³„ì‚°
        semimonthly_business_days, purchase_target_period = get_next_semimonthly_dates(prediction_start_date, df)
        
        logger.info(f"  ğŸ“Š Total predictions: {len(all_business_days)} days")
        logger.info(f"  ğŸ›’ Purchase target period: {purchase_target_period}")
        logger.info(f"  ğŸ“ˆ Purchase interval business days: {len(semimonthly_business_days)}")
        
        if not all_business_days:
            raise ValueError(f"No future business days found after {current_date}")

        # âœ… í•µì‹¬ ìˆ˜ì •: LSTM ë‹¨ê¸° ì˜ˆì¸¡ì„ ìœ„í•´ 2022ë…„ ì´í›„ ë°ì´í„°ë§Œ ì‚¬ìš©
        cutoff_date_2022 = pd.to_datetime('2022-01-01')
        available_data = df[df.index <= current_date].copy()
        
        # 2022ë…„ ì´í›„ ë°ì´í„°ê°€ ì¶©ë¶„í•œ ê²½ìš° í•´ë‹¹ ê¸°ê°„ë§Œ ì‚¬ìš© (ë‹¨ê¸° ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ)
        recent_data = available_data[available_data.index >= cutoff_date_2022]
        if len(recent_data) >= 50:
            historical_data = recent_data.copy()
            logger.info(f"  ğŸ¯ Using recent data for LSTM: 2022+ ({len(historical_data)} records)")
        else:
            historical_data = available_data.copy()
            logger.info(f"  ğŸ“Š Using full available data: insufficient recent data ({len(recent_data)} < 50)")
        
        logger.info(f"  ğŸ“Š Training data: {len(historical_data)} records up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Training data range: {format_date(historical_data.index.min())} ~ {format_date(historical_data.index.max())}")
        
        # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
        if len(historical_data) < 50:
            raise ValueError(f"Insufficient training data: {len(historical_data)} records (minimum 50 required)")
        
        if features is None:
            selected_features, _ = select_features_from_groups(
                historical_data, 
                variable_groups,
                target_col=target_col,
                vif_threshold=50.0,
                corr_threshold=0.8
            )
        else:
            selected_features = features
            
        if target_col not in selected_features:
            selected_features.append(target_col)
        
        logger.info(f"  ğŸ”§ Selected features ({len(selected_features)}): {selected_features}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ë‚ ì§œë³„ ë‹¤ë¥¸ ìŠ¤ì¼€ì¼ë§ ë³´ì¥
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(historical_data[selected_features])
        target_col_idx = selected_features.index(target_col)
        
        logger.info(f"  âš–ï¸  Scaler fitted on data up to {format_date(current_date)}")
        logger.info(f"  ğŸ“Š Scaled data shape: {scaled_data.shape}")
        
        # âœ… í•µì‹¬: ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì‚¬ìš©
        optimized_params = optimize_hyperparameters_semimonthly_kfold(
            train_data=scaled_data,
            input_size=len(selected_features),
            target_col_idx=target_col_idx,
            device=device,
            current_period=prediction_semimonthly_period,  # âœ… ì˜ˆì¸¡ ì‹œì‘ì¼ì˜ ë°˜ì›” ê¸°ê°„
            file_path=file_path,  # ğŸ”‘ íŒŒì¼ ê²½ë¡œ ì „ë‹¬
            n_trials=30,
            k_folds=10,
            use_cache=True
        )
        
        logger.info(f"âœ… Using hyperparameters for prediction start period: {prediction_semimonthly_period}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ëª¨ë¸ í•™ìŠµ ì‹œ í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í•  ë³´ì¥
        logger.info(f"  ğŸš€ Training model with data up to {format_date(current_date)}")
        model, model_scaler, model_target_col_idx = train_model(
            selected_features,
            target_col,
            current_date,
            historical_data,
            device,
            optimized_params
        )

        model.eval()
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì¼ê´€ì„± í™•ì¸
        if model_target_col_idx != target_col_idx:
            logger.warning(f"Target column index mismatch: {model_target_col_idx} vs {target_col_idx}")
            target_col_idx = model_target_col_idx
        
        logger.info(f"  âœ… Model trained successfully for prediction starting {format_date(prediction_start_date)}")
        
        # âœ… í•µì‹¬ ìˆ˜ì •: ì˜ˆì¸¡ ë°ì´í„° ì¤€ë¹„ ì‹œ ë‚ ì§œë³„ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ë³´ì¥ (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
        seq_len = optimized_params['sequence_length']
        
        # ğŸ”‘ ì¤‘ìš”: current_dateë¥¼ ì˜ˆì¸¡í•˜ë ¤ë©´ current_date ì´ì „ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
        available_dates_before_current = [d for d in df.index if d < current_date]
        
        if len(available_dates_before_current) < seq_len:
            logger.warning(f"âš ï¸  Insufficient historical data before {format_date(current_date)}: {len(available_dates_before_current)} < {seq_len}")
            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì´ì „ ë°ì´í„° ì‚¬ìš©
            sequence_dates = available_dates_before_current
        else:
            # ë§ˆì§€ë§‰ seq_lenê°œì˜ ì´ì „ ë‚ ì§œ ì‚¬ìš©
            sequence_dates = available_dates_before_current[-seq_len:]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ (current_date ì œì™¸!)
        sequence = df.loc[sequence_dates][selected_features].values
        
        logger.info(f"  ğŸ“Š Sequence data: {sequence.shape} from {format_date(sequence_dates[0])} to {format_date(sequence_dates[-1])}")
        logger.info(f"  ğŸš« Excluded current_date: {format_date(current_date)} (preventing data leakage)")
        
        # ëª¨ë¸ì—ì„œ ë°˜í™˜ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš© (ì¼ê´€ì„± ë³´ì¥)
        sequence = model_scaler.transform(sequence)
        prev_value = sequence[-1, target_col_idx]
        
        logger.info(f"  ğŸ“ˆ Previous value (scaled): {prev_value:.4f}")
        logger.info(f"  ğŸ“Š Sequence length used: {len(sequence)} (required: {seq_len})")
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        future_predictions = []  # ë¯¸ë˜ ì˜ˆì¸¡ (ì‹¤ì œê°’ ì—†ìŒ)
        validation_data = []     # ê²€ì¦ ë°ì´í„° (ì‹¤ì œê°’ ìˆìŒ)
        
        with torch.no_grad():
            # 23ì˜ì—…ì¼ ì „ì²´ì— ëŒ€í•´ ì˜ˆì¸¡ ìˆ˜í–‰
            max_pred_days = min(predict_window, len(all_business_days))
            current_sequence = sequence.copy()
            
            # í…ì„œë¡œ ë³€í™˜
            X = torch.FloatTensor(current_sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([prev_value]).to(device)
            
            # ì „ì²´ ì‹œí€€ìŠ¤ ì˜ˆì¸¡
            pred = model(X, prev_tensor).cpu().numpy()[0]
            
            # âœ… í•µì‹¬ ìˆ˜ì •: ê° ë‚ ì§œë³„ ì˜ˆì¸¡ ìƒì„± (ë°ì´í„° ëˆ„ì¶œ ë°©ì§€)
            for j, pred_date in enumerate(all_business_days[:max_pred_days]):
                # âœ… ìŠ¤ì¼€ì¼ ì—­ë³€í™˜ ì‹œ ì¼ê´€ëœ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©
                dummy_matrix = np.zeros((1, len(selected_features)))
                dummy_matrix[0, target_col_idx] = pred[j]
                pred_value = model_scaler.inverse_transform(dummy_matrix)[0, target_col_idx]
                
                # ì˜ˆì¸¡ê°’ ê²€ì¦ ë° ì •ë¦¬
                if np.isnan(pred_value) or np.isinf(pred_value):
                    logger.warning(f"Invalid prediction value for {pred_date}: {pred_value}, skipping")
                    continue
                
                pred_value = float(pred_value)
                
                # âœ… ì‹¤ì œ ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
                last_data_date = df.index.max()
                actual_value = None
                
                # âœ… ì‹¤ì œê°’ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì„¤ì •
                if (pred_date in df.index and 
                    pd.notna(df.loc[pred_date, target_col]) and 
                    pred_date <= last_data_date):
                    
                    actual_value = float(df.loc[pred_date, target_col])
                    
                    if np.isnan(actual_value) or np.isinf(actual_value):
                        actual_value = None
                
                # ê¸°ë³¸ ì˜ˆì¸¡ ì •ë³´ (ì‹¤ì œê°’ í¬í•¨)
                prediction_item = {
                    'date': format_date(pred_date, '%Y-%m-%d'),
                    'prediction': pred_value,
                    'actual': actual_value,  # ğŸ”‘ ì‹¤ì œê°’ í•­ìƒ í¬í•¨
                    'prediction_from': format_date(current_date, '%Y-%m-%d'),
                    'day_offset': j + 1,
                    'is_business_day': pred_date.weekday() < 5 and not is_holiday(pred_date),
                    'is_synthetic': pred_date not in df.index,
                    'semimonthly_period': data_semimonthly_period,
                    'next_semimonthly_period': next_semimonthly_period
                }
                
                # âœ… ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš° ê²€ì¦ ë°ì´í„°ì—ë„ ì¶”ê°€
                if actual_value is not None:
                    validation_item = {
                        **prediction_item,
                        'error': abs(pred_value - actual_value),
                        'error_pct': abs(pred_value - actual_value) / actual_value * 100 if actual_value != 0 else 0.0
                    }
                    validation_data.append(validation_item)
                    
                    # ğŸ“Š ê²€ì¦ íƒ€ì… êµ¬ë¶„ ë¡œê·¸
                    if pred_date <= current_date:
                        logger.debug(f"  âœ… Training validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                    else:
                        logger.debug(f"  ğŸ¯ Test validation: {format_date(pred_date)} - Pred: {pred_value:.2f}, Actual: {actual_value:.2f}")
                elif pred_date > last_data_date:
                    logger.debug(f"  ğŸ”® Future: {format_date(pred_date)} - Pred: {pred_value:.2f} (no actual - beyond data)")
                
                future_predictions.append(prediction_item)
        
        # ğŸ“Š ê²€ì¦ ë°ì´í„° í†µê³„
        training_validation = len([v for v in validation_data if pd.to_datetime(v['date']) <= current_date])
        test_validation = len([v for v in validation_data if pd.to_datetime(v['date']) > current_date])
        
        logger.info(f"ğŸ“Š Prediction Results:")
        logger.info(f"  ğŸ“ˆ Total predictions: {len(future_predictions)}")
        logger.info(f"  âœ… Training validation (â‰¤ {format_date(current_date)}): {training_validation}")
        logger.info(f"  ğŸ¯ Test validation (> {format_date(current_date)}): {test_validation}")
        logger.info(f"  ğŸ“‹ Total validation points: {len(validation_data)}")
        logger.info(f"  ğŸ”® Pure future predictions (> {format_date(df.index.max())}): {len(future_predictions) - len(validation_data)}")
        
        if len(validation_data) == 0:
            logger.info("  â„¹ï¸  Pure future prediction - no validation data available")
        
        # âœ… êµ¬ê°„ í‰ê·  ë° ì ìˆ˜ ê³„ì‚° - ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„ ì‚¬ìš©
        temp_predictions_for_interval = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            if pred_date in semimonthly_business_days:  # ì´ì œ ì˜¬ë°”ë¥¸ ë‹¤ìŒ ë°˜ì›” ë‚ ì§œë“¤
                temp_predictions_for_interval.append({
                    'Date': pred_date,
                    'Prediction': pred['prediction']
                })
        
        logger.info(f"  ğŸ›’ Predictions for interval calculation: {len(temp_predictions_for_interval)} (target period: {purchase_target_period})")
        
        interval_averages, interval_scores, analysis_info = calculate_interval_averages_and_scores(
            temp_predictions_for_interval, 
            semimonthly_business_days
        )

        # ìµœì¢… êµ¬ë§¤ êµ¬ê°„ ê²°ì •
        best_interval = decide_purchase_interval(interval_scores)

        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚° (ê²€ì¦ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        metrics = None
        if validation_data:
            start_day_value = df.loc[current_date, target_col]
            if not (pd.isna(start_day_value) or np.isnan(start_day_value) or np.isinf(start_day_value)):
                try:
                    temp_df_for_metrics = pd.DataFrame([
                        {
                            'Date': pd.to_datetime(item['date']),
                            'Prediction': item['prediction'],
                            'Actual': item['actual']
                        } for item in validation_data
                    ])
                    
                    if not temp_df_for_metrics.empty:
                        metrics = compute_performance_metrics_improved(temp_df_for_metrics, start_day_value)
                        logger.info(f"  ğŸ“Š Computed metrics from {len(validation_data)} validation points")
                    else:
                        logger.info("  âš ï¸  No valid data for metrics computation")
                except Exception as e:
                    logger.error(f"Error computing metrics: {str(e)}")
                    metrics = None
            else:
                logger.warning("Invalid start_day_value for metrics computation")
        else:
            logger.info("  â„¹ï¸  No validation data available - pure future prediction")
        
        # âœ… ì´ë™í‰ê·  ê³„ì‚° ì‹œ ì‹¤ì œê°’ë„ í¬í•¨ (ê²€ì¦ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        temp_predictions_for_ma = []
        for pred in future_predictions:
            pred_date = pd.to_datetime(pred['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_predictions_for_ma.append({
                'Date': pred_date,
                'Prediction': pred['prediction'],
                'Actual': actual_val
            })
        
        logger.info(f"  ğŸ“ˆ Calculating moving averages with historical data up to {format_date(current_date)}")
        ma_results = calculate_moving_averages_with_history(
            temp_predictions_for_ma, 
            historical_data,  # ì´ë¯¸ current_dateê¹Œì§€ë¡œ í•„í„°ë§ë¨
            target_col=target_col
        )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
        attention_data = None
        try:
            sequence_tensor = torch.FloatTensor(sequence).unsqueeze(0).to(device)
            prev_tensor = torch.FloatTensor([float(prev_value)]).to(device)
            
            # ì‹¤ì œ ì‹œí€€ìŠ¤ ë‚ ì§œ ì •ë³´ ì „ë‹¬ (sequence_dates ë³€ìˆ˜ ì‚¬ìš©)
            actual_sequence_end_date = current_date  # current_dateê°€ ì‹¤ì œ ë°ì´í„°ì˜ ë§ˆì§€ë§‰ ë‚ ì§œ
            attention_file, attention_img, feature_importance = visualize_attention_weights(
                model, sequence_tensor, prev_tensor, actual_sequence_end_date, selected_features, sequence_dates
            )
            
            attention_data = {
                'image': attention_img,
                'file_path': attention_file,
                'feature_importance': feature_importance
            }
        except Exception as e:
            logger.error(f"Error in attention analysis: {str(e)}")
        
        # ì‹œê°í™” ìƒì„±
        plots = {
            'basic_plot': {'file': None, 'image': None},
            'ma_plot': {'file': None, 'image': None}
        }

        start_day_value = None
        if current_date in df.index and not pd.isna(df.loc[current_date, target_col]):
            start_day_value = df.loc[current_date, target_col]

        # ğŸ“Š ì‹œê°í™”ìš© ë°ì´í„° ì¤€ë¹„ - ì‹¤ì œê°’ í¬í•¨
        temp_df_for_plot_data = []
        for item in future_predictions:
            pred_date = pd.to_datetime(item['date'])
            actual_val = None
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ë‚ ì§œë©´ ì‹¤ì œê°’ ì„¤ì •
            if (pred_date in df.index and 
                pd.notna(df.loc[pred_date, target_col]) and 
                pred_date <= df.index.max()):
                actual_val = float(df.loc[pred_date, target_col])
            
            temp_df_for_plot_data.append({
                'Date': pred_date,
                'Prediction': item['prediction'],
                'Actual': actual_val
            })
        
        temp_df_for_plot = pd.DataFrame(temp_df_for_plot_data)

        if metrics:
            f1_score = metrics['f1']
            accuracy = metrics['accuracy']
            mape = metrics['mape']
            weighted_score = metrics['weighted_score']
            visualization_type = "with validation data"
        else:
            f1_score = accuracy = mape = weighted_score = 0.0
            visualization_type = "future prediction only"

        if start_day_value is not None and not temp_df_for_plot.empty:
            try:
                basic_plot_file, basic_plot_img = plot_prediction_basic(
                    temp_df_for_plot,
                    prediction_start_date,
                    start_day_value,
                    f1_score,
                    accuracy,
                    mape,
                    weighted_score,
                    current_date=current_date,  # ğŸ”‘ ë°ì´í„° ì»·ì˜¤í”„ ë‚ ì§œ ì „ë‹¬
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
                    title_prefix=f"Prediction Graph ({visualization_type})",
                    file_path=file_path
                )
                
                ma_plot_file, ma_plot_img = plot_moving_average_analysis(
                    ma_results,
                    prediction_start_date,
                    save_prefix=None,  # íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ ì‚¬ìš©
                    title_prefix=f"Moving Average Analysis ({visualization_type})",
                    file_path=file_path
                )
                
                plots['basic_plot'] = {'file': basic_plot_file, 'image': basic_plot_img}
                plots['ma_plot'] = {'file': ma_plot_file, 'image': ma_plot_img}
                
                logger.info(f"  ğŸ“Š Visualizations created ({visualization_type})")
                
            except Exception as e:
                logger.error(f"Error creating visualizations: {str(e)}")
                logger.error(traceback.format_exc())
        else:
            logger.warning("  âš ï¸  No start day value or empty predictions - skipping visualizations")
        
        # ê²°ê³¼ ë°˜í™˜ (ê¸‰ë“±ë½ ëª¨ë“œ ì •ë³´ í¬í•¨)
        return {
            'predictions': future_predictions,
            'predictions_flat': future_predictions,  # í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€
            'validation_data': validation_data,
            'interval_scores': interval_scores,
            'interval_averages': interval_averages,
            'best_interval': best_interval,
            'ma_results': ma_results,
            'metrics': metrics,
            'selected_features': selected_features,
            'attention_data': attention_data,
            'plots': plots,
            'current_date': format_date(current_date, '%Y-%m-%d'),
            'data_end_date': format_date(current_date, '%Y-%m-%d'),
            'semimonthly_period': data_semimonthly_period,
            'next_semimonthly_period': purchase_target_period,  # âœ… ìˆ˜ì •: ì˜¬ë°”ë¥¸ êµ¬ë§¤ ëŒ€ìƒ ê¸°ê°„
            'prediction_semimonthly_period': prediction_semimonthly_period,
            'hyperparameter_period_used': prediction_semimonthly_period,
            'purchase_target_period': purchase_target_period,  # âœ… ì¶”ê°€
            'model_type': 'ImprovedLSTMPredictor',
            'loss_function': 'DirectionalLoss'
        }
        
    except Exception as e:
        logger.error(f"Error in prediction generation: {str(e)}")
        logger.error(traceback.format_exc())
        raise e

def generate_predictions_compatible(df, current_date, predict_window=23, features=None, target_col='MOPJ'):
    """
    ê¸°ì¡´ í”„ë¡ íŠ¸ì—”ë“œì™€ í˜¸í™˜ë˜ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜
    (ìƒˆë¡œìš´ êµ¬ì¡° + ê¸°ì¡´ í˜•íƒœ ë³€í™˜)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ìƒˆë¡œìš´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        new_results = generate_predictions(df, current_date, predict_window, features, target_col)
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(new_results.get('predictions'), dict):
            # ìƒˆë¡œìš´ êµ¬ì¡°ì¸ ê²½ìš°
            future_predictions = new_results['predictions']['future']
            validation_data = new_results['predictions']['validation']
            
            # futureì™€ validationì„ í•©ì³ì„œ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
            all_predictions = future_predictions + validation_data
        else:
            # ê¸°ì¡´ êµ¬ì¡°ì¸ ê²½ìš°
            all_predictions = new_results.get('predictions_flat', new_results.get('predictions', []))
        
        # ê¸°ì¡´ í•„ë“œëª…ìœ¼ë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(all_predictions)
        
        # ê²°ê³¼ì— ê¸°ì¡´ í˜•íƒœ ì¶”ê°€
        new_results['predictions'] = compatible_predictions  # ê¸°ì¡´ í˜¸í™˜ì„±
        new_results['predictions_new'] = new_results.get('predictions')  # ìƒˆë¡œìš´ êµ¬ì¡°ë„ ìœ ì§€
        
        logger.info(f"Generated {len(compatible_predictions)} compatible predictions")
        
        return new_results
        
    except Exception as e:
        logger.error(f"Error in compatible prediction generation: {str(e)}")
        raise e

def generate_predictions_with_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ì´ í¬í•¨ëœ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        logger.info(f"Starting prediction with smart cache save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with smart cache system...")
            
            # ìƒˆë¡œìš´ ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì €ì¥ í•¨ìˆ˜ ì‚¬ìš©
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"âœ… Smart cache save completed successfully")
                logger.info(f"  - Prediction Start Date: {save_result.get('prediction_start_date')}")
                logger.info(f"  - File: {save_result.get('file', 'N/A')}")
                
                # ìºì‹œ ì •ë³´ ì¶”ê°€ (ì•ˆì „í•œ í‚¤ ì ‘ê·¼)
                results['cache_info'] = {
                    'saved': True,
                    'prediction_start_date': save_result.get('prediction_start_date'),
                    'file': save_result.get('file'),
                    'success': save_result.get('success', False)
                }
            else:
                logger.warning(f"âŒ Failed to save prediction with smart cache: {save_result.get('error')}")
                results['cache_info'] = {
                    'saved': False,
                    'error': save_result.get('error')
                }
        else:
            logger.info("Skipping smart cache save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
            results['cache_info'] = {
                'saved': False,
                'reason': 'save_to_csv=False'
            }
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            results['cache_info'] = {'saved': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

def generate_predictions_with_attention_save(df, current_date, predict_window=23, features=None, target_col='MOPJ', save_to_csv=True, file_path=None):
    """
    ì˜ˆì¸¡ ìˆ˜í–‰ ë° attention í¬í•¨ CSV ì €ì¥ í•¨ìˆ˜
    
    Parameters:
    -----------
    df : pandas.DataFrame
        ì „ì²´ ë°ì´í„°
    current_date : str or datetime
        í˜„ì¬ ë‚ ì§œ (ë°ì´í„° ê¸°ì¤€ì¼)
    predict_window : int
        ì˜ˆì¸¡ ê¸°ê°„ (ê¸°ë³¸ 23ì¼)
    features : list, optional
        ì‚¬ìš©í•  íŠ¹ì„± ëª©ë¡
    target_col : str
        íƒ€ê²Ÿ ì»¬ëŸ¼ëª… (ê¸°ë³¸ 'MOPJ')
    save_to_csv : bool
        CSV ì €ì¥ ì—¬ë¶€ (ê¸°ë³¸ True)
    
    Returns:
    --------
    dict : ì˜ˆì¸¡ ê²°ê³¼ (attention ë°ì´í„° í¬í•¨)
    """
    try:
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        logger.info(f"Starting prediction with attention save for {current_date}")
        
        # ê¸°ì¡´ generate_predictions í•¨ìˆ˜ ì‹¤í–‰
        results = generate_predictions(df, current_date, predict_window, features, target_col, file_path)
        
        # attention í¬í•¨ ì €ì¥ ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°
        if save_to_csv:
            logger.info("Saving prediction with attention data...")
            save_result = save_prediction_simple(results, current_date)
            results['save_info'] = save_result
            
            if save_result['success']:
                logger.info(f"SUCCESS: Prediction with attention saved successfully")
                logger.info(f"  - CSV: {save_result['csv_file']}")
                logger.info(f"  - Metadata: {save_result['meta_file']}")
                logger.info(f"  - Attention: {save_result['attention_file'] if save_result.get('attention_file') else 'Not saved'}")
            else:
                logger.warning(f"âŒ Failed to save prediction with attention: {save_result.get('error')}")
        else:
            logger.info("Skipping CSV save (save_to_csv=False)")
            results['save_info'] = {'success': False, 'reason': 'save_to_csv=False'}
        
        return results
        
    except Exception as e:
        logger.error(f"Error in generate_predictions_with_attention_save: {str(e)}")
        logger.error(traceback.format_exc())
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ë°˜í™˜í•˜ë˜, ì €ì¥ ì‹¤íŒ¨ ì •ë³´ í¬í•¨
        if 'results' in locals():
            results['save_info'] = {'success': False, 'error': str(e)}
            return results
        else:
            # ì˜ˆì¸¡ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            raise e

#######################################################################
# ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì²˜ë¦¬
#######################################################################
# ğŸ”§ SyntaxError ìˆ˜ì • - check_existing_prediction í•¨ìˆ˜ (3987ë¼ì¸ ê·¼ì²˜)

def find_compatible_hyperparameters(current_file_path, current_period):
    """
    í˜„ì¬ íŒŒì¼ì´ ê¸°ì¡´ íŒŒì¼ì˜ í™•ì¥ì¸ ê²½ìš°, ê¸°ì¡´ íŒŒì¼ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    
    Parameters:
    -----------
    current_file_path : str
        í˜„ì¬ íŒŒì¼ ê²½ë¡œ
    current_period : str
        í˜„ì¬ ì˜ˆì¸¡ ê¸°ê°„
        
    Returns:
    --------
    dict or None: {
        'hyperparams': dict,
        'source_file': str,
        'extension_info': dict
    } ë˜ëŠ” None (í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ì„ ê²½ìš°)
    """
    try:
        # uploads í´ë”ì˜ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ í™•ì¸ (ğŸ”§ ìˆ˜ì •: xlsx íŒŒì¼ë„ í¬í•¨)
        upload_dir = Path(UPLOAD_FOLDER)
        existing_files = [f for f in upload_dir.glob('*.xlsx') if str(f) != current_file_path]
        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íƒìƒ‰í•  ê¸°ì¡´ íŒŒì¼ ìˆ˜: {len(existing_files)}")
        for i, file in enumerate(existing_files):
            logger.info(f"    {i+1}. {file.name}")
        
        for existing_file in existing_files:
            try:
                from app.data.cache_manager import check_data_extension
                # ğŸ”§ ìˆ˜ì •: í™•ì¥ ê´€ê³„ í™•ì¸ + ë‹¨ìˆœ íŒŒì¼ëª… ìœ ì‚¬ì„± í™•ì¸
                extension_result = check_data_extension(str(existing_file), current_file_path)
                is_extension = extension_result.get('is_extension', False)
                
                # ğŸ“ í™•ì¥ ê´€ê³„ê°€ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ê²½ìš° íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ ëŒ€ì²´ í™•ì¸
                if not is_extension:
                    existing_name = existing_file.stem.lower()
                    current_name = Path(current_file_path).stem.lower()
                    # ê¸°ë³¸ ì´ë¦„ì´ ê°™ê±°ë‚˜ í•˜ë‚˜ê°€ ë‹¤ë¥¸ í•˜ë‚˜ë¥¼ í¬í•¨í•˜ëŠ” ê²½ìš°
                    if (existing_name in current_name or current_name in existing_name or 
                        existing_name.replace('_', '') == current_name.replace('_', '')):
                        is_extension = True
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„±ìœ¼ë¡œ í™•ì¥ ê´€ê³„ ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                
                if is_extension:
                    if extension_result.get('is_extension', False):
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] í™•ì¥ ê´€ê³„ ë°œê²¬: {existing_file.name} -> {Path(current_file_path).name}")
                        logger.info(f"    ğŸ“ˆ Extension type: {extension_result.get('validation_details', {}).get('extension_type', 'Unknown')}")
                        logger.info(f"    â• New rows: {extension_result.get('new_rows_count', 0)}")
                    else:
                        logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] íŒŒì¼ëª… ìœ ì‚¬ì„± ê¸°ë°˜ í˜¸í™˜ì„± ì¸ì •: {existing_file.name} -> {Path(current_file_path).name}")
                    
                    # ê¸°ì¡´ íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ í™•ì¸
                    existing_cache_dirs = get_file_cache_dirs(str(existing_file))
                    existing_models_dir = existing_cache_dirs['models']
                    
                    if os.path.exists(existing_models_dir):
                        # í•´ë‹¹ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ì°¾ê¸°
                        hyperparams_pattern = f"hyperparams_kfold_{current_period.replace('-', '_')}.json"
                        hyperparams_file = os.path.join(existing_models_dir, hyperparams_pattern)
                        
                        if os.path.exists(hyperparams_file):
                            try:
                                with open(hyperparams_file, 'r') as f:
                                    hyperparams = json.load(f)
                                
                                logger.info(f"âœ… [HYPERPARAMS_SEARCH] ê¸°ì¡´ íŒŒì¼ì—ì„œ í˜¸í™˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°œê²¬!")
                                logger.info(f"    ğŸ“ Source file: {existing_file.name}")
                                logger.info(f"    ğŸ“Š Hyperparams file: {hyperparams_pattern}")
                                
                                return {
                                    'hyperparams': hyperparams,
                                    'source_file': str(existing_file),
                                    'extension_info': extension_result,
                                    'period': current_period
                                }
                                
                            except Exception as e:
                                logger.warning(f"ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                        else:
                            # âŒ ì‚­ì œëœ ë¶€ë¶„: ë‹¤ë¥¸ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ì²´ë¡œ ì‚¬ìš©í•˜ëŠ” ë¡œì§ ì œê±°
                            logger.info(f"ğŸ” [HYPERPARAMS_SEARCH] {current_period} ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"íŒŒì¼ í™•ì¥ ê´€ê³„ í™•ì¸ ì‹¤íŒ¨ ({existing_file.name}): {str(e)}")
                continue
        
        logger.info(f"âŒ [HYPERPARAMS_SEARCH] í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± íƒìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None
