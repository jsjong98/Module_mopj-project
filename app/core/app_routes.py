from flask import request, jsonify, send_file, make_response, Blueprint
from flask_cors import cross_origin
import os
import json
import pandas as pd
from datetime import datetime, timedelta
import io
import base64
import tempfile
import csv
import logging                          
import traceback
import shutil
import torch
from werkzeug.utils import secure_filename
import time
from pathlib import Path
import numpy as np
from threading import Thread
from app.config import PREDICTIONS_DIR

# ìµœìƒìœ„ Flask ì•± ê°ì²´ import
from app import app

# ì „ì—­ ìƒíƒœ ë³€ìˆ˜ import
from app.core.state_manager import prediction_state

# ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ì™€ ë³€ìˆ˜ë“¤ import
from app.data.loader import (
    load_data, load_data_safe, load_csv_safe_with_fallback,
    # ğŸš€ ìƒˆë¡œìš´ CSV ìºì‹œ ì‹œìŠ¤í…œ
    normalize_security_extension, detect_file_type_by_content,
    is_csv_cache_valid, create_csv_cache_from_excel, load_csv_cache,
    check_data_extension_csv_based, compare_csv_files
)
from app.data.preprocessor import variable_groups, update_holidays_safe, get_combined_holidays, select_features_from_groups, load_holidays_from_file, holidays
from app.utils.date_utils import is_holiday, get_semimonthly_period, format_date
from app.data.cache_manager import get_file_cache_dirs, get_saved_predictions_list, load_prediction_from_csv, delete_saved_prediction, update_cached_prediction_actual_values, load_accumulated_predictions_from_csv, rebuild_predictions_index_from_existing_files, find_compatible_cache_file, find_existing_cache_range, check_data_extension, check_existing_prediction, get_data_content_hash, calculate_file_hash, save_varmax_prediction, load_varmax_prediction, get_saved_varmax_predictions_list, delete_saved_varmax_prediction
from app.prediction.metrics import calculate_accumulated_purchase_reliability, calculate_prediction_consistency, calculate_moving_averages_with_history # compute_performance_metrics_improvedëŠ” plot_prediction_basicì—ì„œ ì‚¬ìš©ë˜ë¯€ë¡œ visualization.plotterë¥¼ í†µí•´ ì„í¬íŠ¸
from app.prediction.predictor import generate_predictions_compatible # generate_predictions_with_saveëŠ” background_tasksì—ì„œ í˜¸ì¶œë¨
from app.visualization.plotter import plot_prediction_basic, plot_moving_average_analysis, visualize_accumulated_metrics # plot_varmax_prediction_basic, plot_varmax_moving_average_analysisëŠ” varmax_modelì—ì„œ ì‚¬ìš©
from app.visualization.attention_viz import visualize_attention_weights
from app.utils.date_utils import get_semimonthly_period
from app.utils.file_utils import process_security_file, cleanup_excel_processes
# detect_file_type_by_content, normalize_security_extensionì€ ì´ì œ loaderì—ì„œ import
from app.utils.serialization import safe_serialize_value, clean_interval_scores_safe, convert_to_legacy_format, clean_predictions_data # clean_predictions_data, clean_cached_predictionsëŠ” cache_managerì—ì„œ ì‚¬ìš©
from app.core.gpu_manager import compare_gpu_monitoring_methods # get_gpu_infoëŠ” gpu_managerì— ìˆìŒ
from app.models.varmax_model import VARMAXSemiMonthlyForecaster # varmax_decisionì—ì„œ ì‚¬ìš©

logger = logging.getLogger(__name__)

# íŒŒì¼ í•´ì‹œ ìºì‹œ ì¶”ê°€ (ë©”ëª¨ë¦¬ ìºì‹±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”)
_file_hash_cache = {}
_cache_lookup_index = {}  # ë¹ ë¥¸ ìºì‹œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤

# ğŸš€ ì¤‘ë³µ í•¨ìˆ˜ ì œê±°: cache_managerì—ì„œ importí•˜ì—¬ ì‚¬ìš©

# ğŸ”§ DataFrame ë©”ëª¨ë¦¬ ìºì‹œ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
_dataframe_cache = {}
_cache_expiry_seconds = 120  # 2ë¶„ê°„ ìºì‹œ ìœ ì§€

# ğŸš€ ì¤‘ë³µ í•¨ìˆ˜ ì œê±°: cache_managerì—ì„œ importí•˜ì—¬ ì‚¬ìš©

@app.route('/api/health', methods=['GET'])
def health_check():
    """ì„œë²„ ìƒíƒœ í™•ì¸ API"""
    return jsonify({
        'status': 'ok',
        'timestamp': datetime.now().isoformat(),
        'attention_endpoint_available': True
    })

@app.route('/api/test-attention', methods=['GET'])
def test_attention():
    """ì–´í…ì…˜ ë§µ ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ìš©"""
    return jsonify({
        'success': True,
        'message': 'Test attention endpoint is working',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test', methods=['GET'])
def test_api():
    """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
    return jsonify({
        'status': 'ok',
        'message': 'API is working!',
        'timestamp': datetime.now().isoformat()
    })

@app.route('/api/test/cache-dirs', methods=['GET'])
def test_cache_dirs():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    try:
        # í˜„ì¬ ìƒíƒœ í™•ì¸
        current_file = prediction_state.get('current_file', None)
        
        # íŒŒì¼ ê²½ë¡œê°€ ìˆìœ¼ë©´ í•´ë‹¹ íŒŒì¼ë¡œ, ì—†ìœ¼ë©´ ê¸°ë³¸ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        test_file = request.args.get('file_path', current_file)
        
        if test_file and not os.path.exists(test_file):
            return jsonify({
                'error': f'File does not exist: {test_file}',
                'current_file': current_file
            }), 400
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„± í…ŒìŠ¤íŠ¸
        cache_dirs = get_file_cache_dirs(test_file)
        
        # ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        dir_status = {}
        for name, path in cache_dirs.items():
            dir_status[name] = {
                'path': str(path),
                'exists': path.exists(),
                'is_dir': path.is_dir() if path.exists() else False
            }
        
        return jsonify({
            'success': True,
            'test_file': test_file,
            'current_file': current_file,
            'cache_dirs': dir_status,
            'cache_root_exists': Path(CACHE_ROOT_DIR).exists()
        })
        
    except Exception as e:
        logger.error(f"Cache directory test failed: {str(e)}")
        return jsonify({
            'error': str(e),
            'traceback': traceback.format_exc()
        }), 500

def detect_file_type_by_content(file_path):
    """
    íŒŒì¼ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì‹¤ì œ íŒŒì¼ íƒ€ì…ì„ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    íšŒì‚¬ ë³´ì•ˆìœ¼ë¡œ ì¸í•´ í™•ì¥ìê°€ ë³€ê²½ëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬
    """
    try:
        # íŒŒì¼ì˜ ì²« ëª‡ ë°”ì´íŠ¸ë¥¼ ì½ì–´ì„œ íŒŒì¼ íƒ€ì… ê°ì§€
        with open(file_path, 'rb') as f:
            header = f.read(8)
        
        # Excel íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ í™•ì¸
        if header[:4] == b'PK\x03\x04':  # ZIP ê¸°ë°˜ íŒŒì¼ (xlsx)
            return 'xlsx'
        elif header[:8] == b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1':  # OLE2 ê¸°ë°˜ íŒŒì¼ (xls)
            return 'xls'
        
        # CSV íŒŒì¼ì¸ì§€ í™•ì¸ (í…ìŠ¤íŠ¸ ê¸°ë°˜)
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                first_line = f.readline()
                # CSV íŠ¹ì„± í™•ì¸: ì‰¼í‘œë‚˜ íƒ­ì´ í¬í•¨ë˜ì–´ ìˆê³ , Date ì»¬ëŸ¼ì´ ìˆëŠ”ì§€
                if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                    return 'csv'
        except:
            # UTF-8ë¡œ ì½ê¸° ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    first_line = f.readline()
                    if (',' in first_line or '\t' in first_line) and ('date' in first_line.lower() or 'Date' in first_line):
                        return 'csv'
            except:
                pass
        
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return None
        
    except Exception as e:
        logger.warning(f"File type detection failed: {str(e)}")
        return None

def normalize_security_extension(filename):
    """
    íšŒì‚¬ ë³´ì•ˆì •ì±…ìœ¼ë¡œ ë³€ê²½ëœ í™•ì¥ìë¥¼ ì›ë˜ í™•ì¥ìë¡œ ë³µì›
    
    Args:
        filename (str): ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        tuple: (ì •ê·œí™”ëœ íŒŒì¼ëª…, ì›ë³¸ í™•ì¥ì, ë³´ì•ˆ í™•ì¥ìì¸ì§€ ì—¬ë¶€)
    """
    # ë³´ì•ˆ í™•ì¥ì ë§¤í•‘
    security_extensions = {
        '.cs': '.csv',     # csv -> cs
        '.xl': '.xlsx',    # xlsx -> xl  
        '.xls': '.xlsx',   # ê¸°ì¡´ xlsë„ xlsxë¡œ í†µì¼
        '.log': '.xlsx',   # log -> xlsx (ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ Excel íŒŒì¼ì„ logë¡œ ìœ„ì¥)
        '.dat': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
        '.txt': None,      # ë‚´ìš© ë¶„ì„ í•„ìš”
    }
    
    filename_lower = filename.lower()
    original_ext = os.path.splitext(filename_lower)[1]
    
    # ë³´ì•ˆ í™•ì¥ìì¸ì§€ í™•ì¸
    if original_ext in security_extensions:
        if security_extensions[original_ext]:
            # ì§ì ‘ ë§¤í•‘ì´ ìˆëŠ” ê²½ìš°
            normalized_ext = security_extensions[original_ext]
            base_name = os.path.splitext(filename)[0]
            normalized_filename = f"{base_name}{normalized_ext}"
            
            logger.info(f"ğŸ”’ [SECURITY] Extension normalization: {filename} -> {normalized_filename}")
            return normalized_filename, normalized_ext, True
        else:
            # ë‚´ìš© ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
            return filename, original_ext, True
    
    # ì¼ë°˜ í™•ì¥ìì¸ ê²½ìš°
    return filename, original_ext, False

def process_security_file(temp_filepath, original_filename):
    """
    ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ í™•ì¥ìê°€ ë³€ê²½ëœ íŒŒì¼ì„ ì²˜ë¦¬
    
    Args:
        temp_filepath (str): ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        original_filename (str): ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        tuple: (ì²˜ë¦¬ëœ íŒŒì¼ ê²½ë¡œ, ì •ê·œí™”ëœ íŒŒì¼ëª…, ì‹¤ì œ í™•ì¥ì)
    """
    # í™•ì¥ì ì •ê·œí™”
    normalized_filename, detected_ext, is_security_ext = normalize_security_extension(original_filename)
    
    if is_security_ext:
        logger.info(f"ğŸ”’ [SECURITY] Processing security file: {original_filename}")
        
        # íŒŒì¼ ë‚´ìš©ìœ¼ë¡œ ì‹¤ì œ íƒ€ì… ê°ì§€
        if detected_ext is None or detected_ext in ['.dat', '.txt']:
            content_type = detect_file_type_by_content(temp_filepath)
            if content_type:
                detected_ext = f'.{content_type}'
                base_name = os.path.splitext(normalized_filename)[0]
                normalized_filename = f"{base_name}{detected_ext}"
                logger.info(f"ğŸ“Š [CONTENT_DETECTION] Detected file type: {content_type}")
        
        # ìƒˆë¡œìš´ íŒŒì¼ ê²½ë¡œ ìƒì„±
        new_filepath = temp_filepath.replace(os.path.splitext(temp_filepath)[1], detected_ext)
        
        # íŒŒì¼ ì´ë¦„ ë³€ê²½ (í™•ì¥ì ìˆ˜ì •)
        if new_filepath != temp_filepath:
            try:
                shutil.move(temp_filepath, new_filepath)
                logger.info(f"ğŸ“ [SECURITY] File extension corrected: {os.path.basename(temp_filepath)} -> {os.path.basename(new_filepath)}")
                return new_filepath, normalized_filename, detected_ext
            except Exception as e:
                logger.warning(f"âš ï¸ [SECURITY] Failed to rename file: {str(e)}")
                return temp_filepath, normalized_filename, detected_ext
    
    return temp_filepath, normalized_filename, detected_ext

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ê¸°ëŠ¥ì´ ìˆëŠ” ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ API (CSV, Excel ì§€ì›)"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
    
    # ğŸ”’ ë³´ì•ˆ í™•ì¥ì ì •ê·œí™” ì²˜ë¦¬
    normalized_filename, normalized_ext, is_security_file = normalize_security_extension(file.filename)
    
    # ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ í™•ì¸ (ë³´ì•ˆ í™•ì¥ì í¬í•¨)
    allowed_extensions = ['.csv', '.xlsx', '.xls']
    security_extensions = ['.cs', '.xl', '.log', '.dat', '.txt']  # ë³´ì•ˆ í™•ì¥ì ì¶”ê°€
    
    file_ext = os.path.splitext(file.filename.lower())[1]
    
    if file and (file_ext in allowed_extensions or file_ext in security_extensions):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„± (ì›ë³¸ í™•ì¥ì ìœ ì§€)
            original_filename = secure_filename(file.filename)
            temp_filename = secure_filename(f"temp_{int(time.time())}{file_ext}")
            temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            file.save(temp_filepath)
            logger.info(f"ğŸ“¤ [UPLOAD] File saved temporarily: {temp_filename}")
            
            # ğŸ”’ 1ë‹¨ê³„: ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ (í™•ì¥ì ë³µì›) - ìºì‹œ ë¹„êµ ì „ì— ë¨¼ì € ì²˜ë¦¬
            if is_security_file:
                temp_filepath, normalized_filename, actual_ext = process_security_file(temp_filepath, original_filename)
                file_ext = actual_ext  # ì‹¤ì œ í™•ì¥ìë¡œ ì—…ë°ì´íŠ¸
                logger.info(f"ğŸ”’ [SECURITY] File processed: {original_filename} -> {normalized_filename}")
                
                # ì²˜ë¦¬ëœ íŒŒì¼ì´ ì§€ì›ë˜ëŠ” í˜•ì‹ì¸ì§€ ì¬í™•ì¸
                if file_ext not in allowed_extensions:
                    try:
                        os.remove(temp_filepath)
                    except:
                        pass
                    return jsonify({'error': f'ë³´ì•ˆ íŒŒì¼ ì²˜ë¦¬ í›„ ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤: {file_ext}'}), 400
            
            # ğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ë¶„ì„ - ë‚ ì§œ ë²”ìœ„ í™•ì¸ (ë³´ì•ˆ ì²˜ë¦¬ ì™„ë£Œëœ íŒŒì¼ë¡œ)
            # ğŸ”§ ë°ì´í„° ë¡œë”© ìºì‹±ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
            df_analysis = None
            
            try:
                if file_ext == '.csv':
                    df_analysis = pd.read_csv(temp_filepath)
                else:  # Excel íŒŒì¼
                    # Excel íŒŒì¼ì€ load_data í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³ ê¸‰ ì²˜ë¦¬ (ğŸ”§ ìºì‹œ í™œì„±í™”)
                    logger.info(f"ğŸ” [UPLOAD] Starting data analysis for {temp_filename}")
                    df_analysis = load_data(temp_filepath, use_cache=True)
                    # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                    if df_analysis.index.name == 'Date':
                        df_analysis = df_analysis.reset_index()
                if 'Date' in df_analysis.columns:
                    df_analysis['Date'] = pd.to_datetime(df_analysis['Date'])
                    start_date = df_analysis['Date'].min()
                    end_date = df_analysis['Date'].max()
                    total_records = len(df_analysis)
                    
                    # 2022ë…„ ì´í›„ ë°ì´í„° í™•ì¸
                    cutoff_2022 = pd.to_datetime('2022-01-01')
                    recent_data = df_analysis[df_analysis['Date'] >= cutoff_2022]
                    recent_records = len(recent_data)
                    
                    logger.info(f"ğŸ“Š [DATA_ANALYSIS] Full range: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')} ({total_records} records)")
                    logger.info(f"ğŸ“Š [DATA_ANALYSIS] 2022+ range: {recent_records} records")
                    
                    data_info = {
                        'start_date': start_date.strftime('%Y-%m-%d'),
                        'end_date': end_date.strftime('%Y-%m-%d'),
                        'total_records': total_records,
                        'recent_records_2022plus': recent_records,
                        'has_historical_data': start_date < cutoff_2022,
                        'lstm_recommended_cutoff': '2022-01-01'
                    }
                else:
                    # Date ì»¬ëŸ¼ì´ ì—†ëŠ” íŒŒì¼ì˜ ê²½ìš° (ì˜ˆ: holidays.csv)
                    file_type_hint = None
                    if 'holiday' in original_filename.lower():
                        file_type_hint = "íœ´ì¼ íŒŒì¼ë¡œ ë³´ì…ë‹ˆë‹¤. /api/holidays/upload ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©ì„ ê¶Œì¥í•©ë‹ˆë‹¤."
                    data_info = {
                        'warning': 'No Date column found',
                        'file_type_hint': file_type_hint
                    }
            except Exception as e:
                logger.warning(f"Data analysis failed: {str(e)}")
                data_info = {'warning': f'Data analysis failed: {str(e)}'}
            
            # ğŸ”§ Excel íŒŒì¼ ì½ê¸° ì™„ë£Œ í›„ íŒŒì¼ í•¸ë“¤ ê°•ì œ í•´ì œ
            import gc
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ pandasê°€ ì—´ì–´ë‘” íŒŒì¼ í•¸ë“¤ í•´ì œ
            
            # ğŸ” 3ë‹¨ê³„: ìºì‹œ í˜¸í™˜ì„± í™•ì¸ (ë³´ì•ˆ ì²˜ë¦¬ ë° ë°ì´í„° ë¶„ì„ ì™„ë£Œ í›„)
            # ì‚¬ìš©ìì˜ ì˜ë„ëœ ë°ì´í„° ë²”ìœ„ ì¶”ì • (ê¸°ë³¸ê°’: 2022ë…„ë¶€í„° LSTM, ì „ì²´ ë°ì´í„° VARMAX)
            # end_dateê°€ ì •ì˜ë˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ìœ„í•œ ì•ˆì „í•œ fallback
            default_end_date = datetime.now().strftime('%Y-%m-%d')
            intended_range = {
                'start_date': '2022-01-01',  # LSTM ê¶Œì¥ ì‹œì‘ì 
                'cutoff_date': data_info.get('end_date', default_end_date)
            }
            
            logger.info(f"ğŸ” [UPLOAD_CACHE] Starting cache compatibility check:")
            logger.info(f"  ğŸ“ New file: {temp_filename}")
            logger.info(f"  ğŸ“… Data range: {data_info.get('start_date')} ~ {data_info.get('end_date')}")
            logger.info(f"  ğŸ“Š Total records: {data_info.get('total_records')}")
            logger.info(f"  ğŸ¯ Intended range: {intended_range}")
            
            # ğŸ”§ ì´ë¯¸ ë¡œë”©ëœ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¤‘ë³µ ë¡œë”© ë°©ì§€
            cache_result = find_compatible_cache_file(temp_filepath, intended_range, cached_df=df_analysis)
            
            logger.info(f"ğŸ¯ [UPLOAD_CACHE] Cache check result:")
            logger.info(f"  âœ… Found: {cache_result['found']}")
            logger.info(f"  ğŸ·ï¸ Type: {cache_result.get('cache_type')}")
            if cache_result.get('cache_files'):
                logger.info(f"  ğŸ“ Cache files: {[os.path.basename(f) for f in cache_result['cache_files']]}")
            if cache_result.get('compatibility_info'):
                logger.info(f"  â„¹ï¸ Compatibility info: {cache_result['compatibility_info']}")
            
            response_data = {
                'success': True,
                'filepath': temp_filepath,
                'filename': os.path.basename(temp_filepath),
                'original_filename': original_filename,
                'normalized_filename': normalized_filename if is_security_file else original_filename,
                'data_info': data_info,
                'model_recommendations': {
                    'varmax': 'ì „ì²´ ë°ì´í„° ì‚¬ìš© ê¶Œì¥ (ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„)',
                    'lstm': '2022ë…„ ì´í›„ ë°ì´í„° ì‚¬ìš© ê¶Œì¥ (ë‹¨ê¸° ì •í™•ë„ í–¥ìƒ)'
                },
                'security_info': {
                    'is_security_file': is_security_file,
                    'original_extension': os.path.splitext(file.filename.lower())[1] if is_security_file else None,
                    'detected_extension': file_ext if is_security_file else None,
                    'message': f"ë³´ì•ˆ íŒŒì¼ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.splitext(file.filename)[1]} -> {file_ext}" if is_security_file else None
                },
                'cache_info': {
                    'found': cache_result['found'],
                    'cache_type': cache_result.get('cache_type'),
                    'message': None
                }
            }
            
            if cache_result['found']:
                cache_type = cache_result['cache_type']
                cache_files = cache_result.get('cache_files', [])
                compatibility_info = cache_result.get('compatibility_info', {})
                
                if cache_type == 'exact':
                    cache_file = cache_files[0] if cache_files else None
                    response_data['cache_info']['message'] = f"ë™ì¼í•œ ë°ì´í„° ë°œê²¬! ê¸°ì¡´ ìºì‹œë¥¼ í™œìš©í•©ë‹ˆë‹¤. ({os.path.basename(cache_file) if cache_file else 'Unknown'})"
                    response_data['cache_info']['compatible_file'] = cache_file
                    logger.info(f"âœ… [CACHE] Exact match found: {cache_file}")
                    
                elif cache_type == 'extension':
                    cache_file = cache_files[0] if cache_files else None
                    extension_details = compatibility_info.get('extension_details', {})
                    new_rows = extension_details.get('new_rows_count', compatibility_info.get('new_rows_count', 0))
                    extension_type = extension_details.get('validation_details', {}).get('extension_type', ['ë°ì´í„° í™•ì¥'])
                    
                    if isinstance(extension_type, list):
                        extension_desc = ' + '.join(extension_type)
                    else:
                        extension_desc = str(extension_type)
                    
                    response_data['cache_info']['message'] = f"ğŸ“ˆ ë°ì´í„° í™•ì¥ ê°ì§€! {extension_desc} (+{new_rows}ê°œ ìƒˆ í–‰). ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ ìºì‹œë¥¼ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    response_data['cache_info']['compatible_file'] = cache_file
                    response_data['cache_info']['extension_info'] = compatibility_info
                    response_data['cache_info']['hyperparams_reusable'] = True  # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš© ê°€ëŠ¥ í‘œì‹œ
                    logger.info(f"ğŸ“ˆ [CACHE] Extension detected from {cache_file}: {extension_desc} (+{new_rows} rows)")
                    
                elif cache_type in ['partial', 'near_complete', 'multi_cache']:
                    best_coverage = compatibility_info.get('best_coverage', 0)
                    total_caches = compatibility_info.get('total_compatible_caches', len(cache_files))
                    
                    if cache_type == 'near_complete':
                        response_data['cache_info']['message'] = f"ğŸ¯ ê±°ì˜ ì™„ì „í•œ ìºì‹œ ë§¤ì¹˜! ({best_coverage:.1%} ì»¤ë²„ë¦¬ì§€) ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìµœëŒ€í•œ í™œìš©í•©ë‹ˆë‹¤."
                    elif cache_type == 'multi_cache':
                        response_data['cache_info']['message'] = f"ğŸ”— ë‹¤ì¤‘ ìºì‹œ ë°œê²¬! {total_caches}ê°œ ìºì‹œì—ì„œ {best_coverage:.1%} ì»¤ë²„ë¦¬ì§€ë¡œ ì˜ˆì¸¡ì„ ê°€ì†í™”í•©ë‹ˆë‹¤."
                    else:  # partial
                        response_data['cache_info']['message'] = f"ğŸ“Š ë¶€ë¶„ ìºì‹œ ë§¤ì¹˜! ({best_coverage:.1%} ì»¤ë²„ë¦¬ì§€) ì¼ë¶€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¬í™œìš©í•©ë‹ˆë‹¤."
                    
                    response_data['cache_info']['compatible_files'] = cache_files
                    response_data['cache_info']['compatibility_info'] = compatibility_info
                    logger.info(f"ğŸ¯ [ENHANCED_CACHE] {cache_type} cache found: {total_caches} caches, {best_coverage:.1%} coverage")
                
                # ğŸ”§ íŒŒì¼ ì²˜ë¦¬ ë¡œì§ ê°œì„ : ë°ì´í„° í™•ì¥ ì‹œ ìƒˆ íŒŒì¼ ì‚¬ìš©
                if cache_type == 'exact' and cache_files:
                    # ì •í™•íˆ ë™ì¼í•œ íŒŒì¼ì¸ ê²½ìš°ì—ë§Œ ê¸°ì¡´ íŒŒì¼ ì‚¬ìš©
                    cache_file = cache_files[0]
                    response_data['filepath'] = cache_file
                    response_data['filename'] = os.path.basename(cache_file)
                    
                    # ì„ì‹œ íŒŒì¼ ì‚­ì œ (ì™„ì „íˆ ë™ì¼í•œ ê²½ìš°ë§Œ)
                    if temp_filepath != cache_file:
                        try:
                            os.remove(temp_filepath)
                            logger.info(f"ğŸ—‘ï¸ [CLEANUP] Temporary file removed (exact match): {temp_filename}")
                        except Exception as e:
                            logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file {temp_filename}: {str(e)}")
                            # ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            
                elif cache_type == 'extension' and cache_files:
                    # ğŸ”„ ë°ì´í„° í™•ì¥ì˜ ê²½ìš°: ìƒˆ íŒŒì¼ì„ ì‚¬ìš©í•˜ë˜, ìºì‹œ ì •ë³´ëŠ” ìœ ì§€
                    logger.info(f"ğŸ“ˆ [EXTENSION] Data extension detected - using NEW file with cache info")
                    
                    # ìƒˆ íŒŒì¼ì„ ì •ì‹ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (ì›ë³¸ í™•ì¥ì ìœ ì§€)
                    try:
                        content_hash = get_data_content_hash(temp_filepath)
                        final_filename = f"data_{content_hash}{file_ext}" if content_hash else temp_filename
                    except Exception as hash_error:
                        logger.warning(f"âš ï¸ Hash calculation failed for extended file, using timestamp-based filename: {str(hash_error)}")
                        final_filename = temp_filename  # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ëª… ìœ ì§€
                    
                    final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                    
                    if temp_filepath != final_filepath:
                        # ğŸ”§ ê°•í™”ëœ íŒŒì¼ ì´ë™ ë¡œì§ (Excel íŒŒì¼ ë½ í•´ì œ ëŒ€ê¸°)
                        moved_successfully = False
                        for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                            try:
                                # Excel íŒŒì¼ ì½ê¸° í›„ íŒŒì¼ ë½ í•´ì œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ëŒ€ê¸°
                                import gc
                                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ íŒŒì¼ í•¸ë“¤ í•´ì œ
                                time.sleep(0.5 + attempt * 0.5)  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                                
                                shutil.move(temp_filepath, final_filepath)
                                logger.info(f"ğŸ“ [UPLOAD] Extended file renamed: {final_filename} (attempt {attempt + 1})")
                                moved_successfully = True
                                break
                            except OSError as move_error:
                                logger.warning(f"âš ï¸ Extended file move attempt {attempt + 1} failed: {str(move_error)}")
                                if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                    logger.warning(f"âš ï¸ All move attempts failed, keeping original filename: {str(move_error)}")
                                    final_filepath = temp_filepath
                                    final_filename = temp_filename
                        
                        if not moved_successfully:
                            final_filepath = temp_filepath
                            final_filename = temp_filename
                    else:
                        logger.info(f"ğŸ“ [UPLOAD] Extended file already has correct name: {final_filename}")
                        
                    response_data['filepath'] = final_filepath
                    response_data['filename'] = final_filename
                    
                    # í™•ì¥ ì •ë³´ì— ìƒˆ íŒŒì¼ ì •ë³´ ì¶”ê°€
                    response_data['cache_info']['new_file_used'] = True
                    response_data['cache_info']['original_cache_file'] = cache_files[0]
                    
                    # ğŸ”‘ ë°ì´í„° í™•ì¥ í‘œì‹œ - í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ì‚¬ìš© ê°€ëŠ¥
                    response_data['data_extended'] = True
                    response_data['hyperparams_inheritance'] = {
                        'available': True,
                        'source_file': os.path.basename(cache_files[0]),
                        'extension_type': extension_desc if 'extension_desc' in locals() else 'ë°ì´í„° í™•ì¥',
                        'new_rows_added': new_rows if 'new_rows' in locals() else compatibility_info.get('new_rows_count', 0)
                    }
                    
                else:
                    # ìƒˆ íŒŒì¼ì€ ìœ ì§€ (ë¶€ë¶„/ë‹¤ì¤‘ ìºì‹œì˜ ê²½ìš°, ì›ë³¸ í™•ì¥ì ìœ ì§€)
                    try:
                        content_hash = get_data_content_hash(temp_filepath)
                        final_filename = f"data_{content_hash}{file_ext}" if content_hash else temp_filename
                    except Exception as hash_error:
                        logger.warning(f"âš ï¸ Hash calculation failed, using timestamp-based filename: {str(hash_error)}")
                        final_filename = temp_filename  # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ëª… ìœ ì§€
                    
                    final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                    
                    if temp_filepath != final_filepath:
                        # ğŸ”§ ê°•í™”ëœ íŒŒì¼ ì´ë™ ë¡œì§ (Excel íŒŒì¼ ë½ í•´ì œ ëŒ€ê¸°)
                        moved_successfully = False
                        for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                            try:
                                # Excel íŒŒì¼ ì½ê¸° í›„ íŒŒì¼ ë½ í•´ì œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ëŒ€ê¸°
                                import gc
                                gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ íŒŒì¼ í•¸ë“¤ í•´ì œ
                                time.sleep(0.5 + attempt * 0.5)  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                                
                                shutil.move(temp_filepath, final_filepath)
                                logger.info(f"ğŸ“ [UPLOAD] File renamed: {final_filename} (attempt {attempt + 1})")
                                moved_successfully = True
                                break
                            except OSError as move_error:
                                logger.warning(f"âš ï¸ File move attempt {attempt + 1} failed: {str(move_error)}")
                                if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                    logger.warning(f"âš ï¸ All move attempts failed, keeping original filename: {str(move_error)}")
                                    final_filepath = temp_filepath
                                    final_filename = temp_filename
                        
                        if not moved_successfully:
                            final_filepath = temp_filepath
                            final_filename = temp_filename
                    else:
                        logger.info(f"ğŸ“ [UPLOAD] File already has correct name: {final_filename}")
                        
                    response_data['filepath'] = final_filepath
                    response_data['filename'] = final_filename
                    # í™•ì¥ ë°ì´í„°ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ìƒˆë¡œìš´ ë°ì´í„° ë©”ì‹œì§€ ì„¤ì •
                    if cache_type != 'extension':
                        response_data['cache_info']['message'] = "ìƒˆë¡œìš´ ë°ì´í„°ì…ë‹ˆë‹¤. ëª¨ë¸ë³„ë¡œ ì ì ˆí•œ ë°ì´í„° ë²”ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
            
            else:
                # ğŸ†• ìºì‹œê°€ ì—†ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ íŒŒì¼ ì²˜ë¦¬
                logger.info(f"ğŸ†• [NEW_FILE] No cache found - processing as new data file")
                
                # ìƒˆ íŒŒì¼ì„ ì •ì‹ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ (data ì ‘ë‘ì‚¬ ì‚¬ìš©)
                try:
                    content_hash = get_data_content_hash(temp_filepath)
                    final_filename = f"data_{content_hash[:12]}_{os.path.splitext(normalized_filename)[0]}{file_ext}" if content_hash else temp_filename
                except Exception as hash_error:
                    logger.warning(f"âš ï¸ Hash calculation failed for new file, using timestamp-based filename: {str(hash_error)}")
                    final_filename = temp_filename  # í•´ì‹œ ì‹¤íŒ¨ ì‹œ ì„ì‹œ íŒŒì¼ëª… ìœ ì§€
                
                final_filepath = os.path.join(app.config['UPLOAD_FOLDER'], final_filename)
                
                if temp_filepath != final_filepath:
                    # ğŸ”§ ê°•í™”ëœ íŒŒì¼ ì´ë™ ë¡œì§ (Excel íŒŒì¼ ë½ í•´ì œ ëŒ€ê¸°)
                    moved_successfully = False
                    for attempt in range(3):  # ìµœëŒ€ 3ë²ˆ ì‹œë„
                        try:
                            # Excel íŒŒì¼ ì½ê¸° í›„ íŒŒì¼ ë½ í•´ì œë¥¼ ìœ„í•œ ì¶©ë¶„í•œ ëŒ€ê¸°
                            import gc
                            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ìœ¼ë¡œ íŒŒì¼ í•¸ë“¤ í•´ì œ
                            time.sleep(0.5 + attempt * 0.5)  # ì ì§„ì ìœ¼ë¡œ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                            
                            shutil.move(temp_filepath, final_filepath)
                            logger.info(f"ğŸ“ [UPLOAD] New file renamed: {final_filename} (attempt {attempt + 1})")
                            moved_successfully = True
                            break
                        except OSError as move_error:
                            logger.warning(f"âš ï¸ New file move attempt {attempt + 1} failed: {str(move_error)}")
                            if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                logger.warning(f"âš ï¸ All move attempts failed, keeping original filename: {str(move_error)}")
                                final_filepath = temp_filepath
                                final_filename = temp_filename
                    
                    if not moved_successfully:
                        final_filepath = temp_filepath
                        final_filename = temp_filename
                else:
                    logger.info(f"ğŸ“ [UPLOAD] New file already has correct name: {final_filename}")
                    
                response_data['filepath'] = final_filepath
                response_data['filename'] = final_filename
                response_data['cache_info']['message'] = "ìƒˆë¡œìš´ ë°ì´í„°ì…ë‹ˆë‹¤. ëª¨ë¸ë³„ë¡œ ì ì ˆí•œ ë°ì´í„° ë²”ìœ„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
            
            # ğŸ”‘ ì—…ë¡œë“œëœ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥
            prediction_state['current_file'] = response_data['filepath']
            logger.info(f"ğŸ“ Set current_file in prediction_state: {response_data['filepath']}")
            
            # ğŸ”§ ì„±ê³µ ì‹œ temp íŒŒì¼ ì •ë¦¬ (final_filepathì™€ ë‹¤ë¥¸ ê²½ìš°ì—ë§Œ)
            if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                final_filepath = response_data.get('filepath')
                if final_filepath and temp_filepath != final_filepath:
                    try:
                        os.remove(temp_filepath)
                        logger.info(f"ğŸ—‘ï¸ [CLEANUP] Success - temp file removed: {os.path.basename(temp_filepath)}")
                    except Exception as cleanup_error:
                        logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file after success: {str(cleanup_error)}")
                else:
                    logger.info(f"ğŸ“ [CLEANUP] Temp file kept as final file: {os.path.basename(temp_filepath)}")
            
            return jsonify(response_data)
            
        except Exception as e:
            logger.error(f"Error during file upload: {str(e)}")
            # ğŸ”§ ê°•í™”ëœ temp íŒŒì¼ ì •ë¦¬
            try:
                if 'temp_filepath' in locals() and os.path.exists(temp_filepath):
                    os.remove(temp_filepath)
                    logger.info(f"ğŸ—‘ï¸ [CLEANUP] Temp file removed on error: {os.path.basename(temp_filepath)}")
            except Exception as cleanup_error:
                logger.warning(f"âš ï¸ [CLEANUP] Failed to remove temp file on error: {str(cleanup_error)}")
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({'error': 'Invalid file type. Only CSV and Excel files (.csv, .xlsx, .xls) are allowed'}), 400

@app.route('/api/holidays', methods=['GET'])
def get_holidays():
    """íœ´ì¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        # íœ´ì¼ì„ ë‚ ì§œì™€ ì„¤ëª…ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        holidays_list = []
        file_holidays = load_holidays_from_file()  # íŒŒì¼ì—ì„œ ë¡œë“œ
        
        # í˜„ì¬ ì „ì—­ íœ´ì¼ì—ì„œ íŒŒì¼ íœ´ì¼ê³¼ ìë™ ê°ì§€ íœ´ì¼ êµ¬ë¶„
        auto_detected = holidays - file_holidays
        
        for holiday_date in file_holidays:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (from file)',
                'source': 'file'
            })
        
        for holiday_date in auto_detected:
            holidays_list.append({
                'date': holiday_date,
                'description': 'Holiday (detected from missing data)',
                'source': 'auto_detected'
            })
        
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        holidays_list.sort(key=lambda x: x['date'])
        
        return jsonify({
            'success': True,
            'holidays': holidays_list,
            'count': len(holidays_list),
            'file_holidays': len(file_holidays),
            'auto_detected_holidays': len(auto_detected)
        })
    except Exception as e:
        logger.error(f"Error getting holidays: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e),
            'holidays': [],
            'count': 0
        }), 500

@app.route('/api/holidays/upload', methods=['POST'])
def upload_holidays():
    """íœ´ì¼ ëª©ë¡ íŒŒì¼ ì—…ë¡œë“œ API"""
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400
        
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No selected file'}), 400
        
    if file and (file.filename.endswith('.csv') or file.filename.endswith('.xlsx')):
        try:
            # ì„ì‹œ íŒŒì¼ëª… ìƒì„±
            filename = secure_filename(f"holidays_{int(time.time())}{os.path.splitext(file.filename)[1]}")
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            
            # íŒŒì¼ ì €ì¥
            file.save(filepath)
            
            # íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ - ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ ì‚¬ìš©
            logger.info(f"ğŸ–ï¸ [HOLIDAY_UPLOAD] Processing uploaded holiday file: {filename}")
            new_holidays = update_holidays_safe(filepath)
            
            # ì›ë³¸ íŒŒì¼ì„ holidays ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
            holidays_dir = 'holidays'
            if not os.path.exists(holidays_dir):
                os.makedirs(holidays_dir)
                logger.info(f"ğŸ“ Created holidays directory: {holidays_dir}")
            
            permanent_path = os.path.join(holidays_dir, 'holidays' + os.path.splitext(file.filename)[1])
            shutil.copy2(filepath, permanent_path)
            logger.info(f"ğŸ“ Holiday file copied to: {permanent_path}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                os.remove(filepath)
                logger.info(f"ğŸ—‘ï¸ Temporary file removed: {filepath}")
            except:
                pass
            
            return jsonify({
                'success': True,
                'message': f'Successfully uploaded and loaded {len(new_holidays)} holidays',
                'filepath': permanent_path,
                'filename': os.path.basename(permanent_path),
                'holidays': list(new_holidays)
            })
        except Exception as e:
            logger.error(f"Error during holiday file upload: {str(e)}")
            logger.error(traceback.format_exc())
            return jsonify({'error': f'Error processing file: {str(e)}'}), 500
    
    return jsonify({
        'error': 'Invalid file type. Only CSV and Excel files are allowed',
        'supported_extensions': {
            'standard': ['.csv', '.xlsx', '.xls'],
            'security': ['.cs (csv)', '.xl (xlsx)', '.log (xlsx)', '.dat (auto-detect)', '.txt (auto-detect)']
        }
    }), 400

# xlwings ëŒ€ì•ˆ ë¡œë” (ë³´ì•ˆí”„ë¡œê·¸ë¨ì´ íŒŒì¼ì„ ì ê·¸ëŠ” ê²½ìš° ì‚¬ìš©)
try:
    import xlwings as xw
    XLWINGS_AVAILABLE = True
    logger.info("âœ… xlwings library available - Excel security bypass enabled")
except ImportError:
    XLWINGS_AVAILABLE = False
    logger.warning("âš ï¸ xlwings not available - falling back to pandas only")

@app.route('/api/holidays/reload', methods=['POST'])
def reload_holidays():
    """íœ´ì¼ ëª©ë¡ ì¬ë¡œë“œ API - ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨"""
    try:
        filepath = request.json.get('filepath') if request.json else None
        
        logger.info(f"ğŸ”„ [HOLIDAY_RELOAD] Reloading holidays from: {filepath or 'default file'}")
        
        # ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ì„ í¬í•¨í•œ ì•ˆì „í•œ ì¬ë¡œë“œ
        new_holidays = update_holidays_safe(filepath)
        
        return jsonify({
            'success': True,
            'message': f'Successfully reloaded {len(new_holidays)} holidays',
            'holidays': list(new_holidays),
            'security_bypass_used': XLWINGS_AVAILABLE
        })
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAY_RELOAD] Error reloading holidays: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to reload holidays: {str(e)}'
        }), 500

@app.route('/api/file/metadata', methods=['GET'])
def get_file_metadata():
    """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ API"""
    filepath = request.args.get('filepath')
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì½ê¸° ë°©ì‹ ê²°ì •
        file_ext = os.path.splitext(filepath.lower())[1]
        
        if file_ext == '.csv':
            # CSV íŒŒì¼ ì²˜ë¦¬
            df = pd.read_csv(filepath, nrows=5)  # ì²˜ìŒ 5í–‰ë§Œ ì½ê¸°
            columns = df.columns.tolist()
            latest_date = None
            
            if 'Date' in df.columns:
                # ë‚ ì§œ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì½ì–´ì„œ ìµœì‹  ë‚ ì§œ í™•ì¸
                dates_df = pd.read_csv(filepath, usecols=['Date'])
                dates_df['Date'] = pd.to_datetime(dates_df['Date'])
                latest_date = dates_df['Date'].max().strftime('%Y-%m-%d')
        else:
            # Excel íŒŒì¼ ì²˜ë¦¬ (ê³ ê¸‰ ì²˜ë¦¬ ì‚¬ìš©) - ğŸ”§ ì¤‘ë³µ ë¡œë”© ë°©ì§€
            logger.info(f"ğŸ” [METADATA] Loading Excel data for metadata extraction...")
            df = load_data(filepath)
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                full_df = df.copy()  # ğŸ”§ ì „ì²´ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
                df = df.reset_index()
            else:
                full_df = df.copy()  # ğŸ”§ ì „ì²´ ë°ì´í„° ì €ì¥ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
            
            # ì²˜ìŒ 5í–‰ë§Œ ì„ íƒ
            df_sample = df.head(5)
            columns = df.columns.tolist()
            latest_date = None
            
            if 'Date' in df.columns:
                # ğŸ”§ ì´ë¯¸ ë¡œë”©ëœ ë°ì´í„°ì—ì„œ ìµœì‹  ë‚ ì§œ í™•ì¸ (ì¤‘ë³µ ë¡œë”© ë°©ì§€)
                if full_df.index.name == 'Date':
                    latest_date = pd.to_datetime(full_df.index).max().strftime('%Y-%m-%d')
                else:
                    latest_date = pd.to_datetime(full_df['Date']).max().strftime('%Y-%m-%d')
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            df = df_sample
        
        return jsonify({
            'success': True,
            'rows': len(df),
            'columns': columns,
            'latest_date': latest_date,
            'sample': df.head().to_dict(orient='records')
        })
    except Exception as e:
        logger.error(f"Error reading file metadata: {str(e)}")
        return jsonify({'error': f'Error reading file: {str(e)}'}), 500
    
@app.route('/api/data/dates', methods=['GET'])
def get_available_dates():
    filepath = request.args.get('filepath')
    days_limit = int(request.args.get('limit', 999999))  # ê¸°ë³¸ê°’ì„ ë§¤ìš° í° ìˆ˜ë¡œ ì„¤ì • (ëª¨ë“  ë‚ ì§œ)
    force_refresh = request.args.get('force_refresh', 'false').lower() == 'true'  # ê°•ì œ ìƒˆë¡œê³ ì¹¨ ì˜µì…˜
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'File not found'}), 404
    
    try:
        # ğŸ”„ íŒŒì¼ì˜ ìµœì‹  í•´ì‹œì™€ ìˆ˜ì • ì‹œê°„ í™•ì¸í•˜ì—¬ ë³€ê²½ ê°ì§€
        current_file_hash = get_data_content_hash(filepath)
        current_file_mtime = os.path.getmtime(filepath)
        
        logger.info(f"ğŸ” [DATE_REFRESH] Checking file status:")
        logger.info(f"  ğŸ“ File: {os.path.basename(filepath)}")
        logger.info(f"  ğŸ”‘ Current hash: {current_file_hash[:12] if current_file_hash else 'None'}...")
        logger.info(f"  â° Modified time: {datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"  ğŸ”„ Force refresh: {force_refresh}")
        
        # íŒŒì¼ ë°ì´í„° ë¡œë“œ ë° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ, í•­ìƒ ìµœì‹  íŒŒì¼ ë‚´ìš© í™•ì¸)
        # ğŸ”‘ ë‹¨ì¼ ë‚ ì§œ ì˜ˆì¸¡ìš©: LSTM ëª¨ë¸ íƒ€ì… ì§€ì •í•˜ì—¬ 2022ë…„ ì´í›„ ë°ì´í„°ë§Œ ë¡œë“œ
        file_ext = os.path.splitext(filepath.lower())[1]
        if file_ext == '.csv':
            # CSV íŒŒì¼ì˜ ê²½ìš° ë‹¤ì–‘í•œ êµ¬ë¶„ìë¡œ ì‹œë„
            try:
                df = pd.read_csv(filepath)
                # ë§Œì•½ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì—ì„œ ì‰¼í‘œê°€ ë°œê²¬ë˜ë©´ êµ¬ë¶„ìê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                if len(df.columns) == 1 and ',' in str(df.columns[0]):
                    logger.info("ğŸ”§ [CSV_PARSE] Trying different separators for CSV...")
                    # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ì‹œë„
                    try:
                        df = pd.read_csv(filepath, sep=';')
                        logger.info(f"âœ… [CSV_PARSE] Successfully parsed with semicolon: {df.shape}")
                    except:
                        # íƒ­ìœ¼ë¡œ ì‹œë„
                        try:
                            df = pd.read_csv(filepath, sep='\t')
                            logger.info(f"âœ… [CSV_PARSE] Successfully parsed with tab: {df.shape}")
                        except:
                            logger.warning("âš ï¸ [CSV_PARSE] Could not parse with alternative separators")
            except Exception as e:
                logger.error(f"âŒ [CSV_PARSE] Error reading CSV: {str(e)}")
                raise
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš© (LSTM ëª¨ë¸ íƒ€ì… ì§€ì •)
            df = load_data(filepath, model_type='lstm')
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                df = df.reset_index()

        # Date ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì²˜ë¦¬
        if 'Date' not in df.columns:
            logger.error(f"âŒ [DATE_COLUMN] Date column not found. Available columns: {list(df.columns)}")
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ í˜•íƒœì¸ì§€ í™•ì¸
            first_col = df.columns[0]
            try:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ì²« ëª‡ ê°œ ê°’ì´ ë‚ ì§œì¸ì§€ í™•ì¸
                sample_values = df[first_col].head(5).dropna()
                pd.to_datetime(sample_values)
                logger.info(f"âœ… [DATE_COLUMN] Using first column as Date: {first_col}")
                df['Date'] = pd.to_datetime(df[first_col])
            except:
                logger.error(f"âŒ [DATE_COLUMN] First column is not date format: {first_col}")
                raise ValueError(f"Date column not found and first column '{first_col}' is not a valid date format")
        else:
            df['Date'] = pd.to_datetime(df['Date'])
        
        df = df.sort_values('Date')
        
        # ğŸ–ï¸ ë°ì´í„°ë¥¼ ë¡œë“œí•œ í›„ íœ´ì¼ ì •ë³´ ìë™ ì—…ë°ì´íŠ¸ (ë¹ˆ í‰ì¼ ê°ì§€) - ì„ì‹œ ë¹„í™œì„±í™”
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Auto-detection temporarily disabled to show more dates...")
        # updated_holidays = update_holidays(df=df)
        updated_holidays = load_holidays_from_file()  # íŒŒì¼ íœ´ì¼ë§Œ ì‚¬ìš©
        logger.info(f"ğŸ–ï¸ [HOLIDAYS] Total holidays (file only): {len(updated_holidays)}")
        
        # ğŸ“Š ì‹¤ì œ íŒŒì¼ ë°ì´í„° ë²”ìœ„ í™•ì¸ (ìºì‹œ ë¬´ì‹œ)
        total_rows = len(df)
        data_start_date = df.iloc[0]['Date']
        data_end_date = df.iloc[-1]['Date']
        
        logger.info(f"ğŸ“Š [ACTUAL_DATA] File analysis results:")
        logger.info(f"  ğŸ“ˆ Total data rows: {total_rows}")
        logger.info(f"  ğŸ“… Actual date range: {data_start_date.strftime('%Y-%m-%d')} ~ {data_end_date.strftime('%Y-%m-%d')}")
        
        # ğŸ” ê¸°ì¡´ ìºì‹œì™€ ë¹„êµ (ìˆëŠ” ê²½ìš°)
        existing_cache_range = find_existing_cache_range(filepath)
        if existing_cache_range and not force_refresh:
            # numpy.ndarrayì™€ Timestamp ë¹„êµ ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ Timestampë¡œ ì •ê·œí™”
            try:
                cache_start = pd.to_datetime(existing_cache_range['start_date'])
                cache_cutoff = pd.to_datetime(existing_cache_range['cutoff_date'])
                
                # ì‹¤ì œ ë°ì´í„° ë‚ ì§œë“¤ë„ Timestampë¡œ ë³€í™˜í•˜ì—¬ ì•ˆì „í•œ ë¹„êµ
                actual_start = pd.to_datetime(data_start_date)
                actual_end = pd.to_datetime(data_end_date)
            except Exception as e:
                logger.warning(f"âš ï¸ [CACHE_COMPARE] Error converting dates for comparison: {str(e)}")
                cache_start = None
                cache_cutoff = None
            
            # cache_startì™€ cache_cutoffê°€ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ ë¹„êµ ìˆ˜í–‰
            if cache_start is not None and cache_cutoff is not None:
                # ë‹¨ì¼ ê°’ìœ¼ë¡œ ë³€í™˜ (arrayì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©)
                if hasattr(cache_start, '__len__') and len(cache_start) > 0:
                    cache_start = cache_start[0] if hasattr(cache_start[0], 'strftime') else pd.to_datetime(cache_start[0])
                if hasattr(cache_cutoff, '__len__') and len(cache_cutoff) > 0:
                    cache_cutoff = cache_cutoff[0] if hasattr(cache_cutoff[0], 'strftime') else pd.to_datetime(cache_cutoff[0])
                
                # í™•ì‹¤íˆ Timestamp íƒ€ì…ìœ¼ë¡œ ë³€í™˜
                cache_start = pd.Timestamp(cache_start)
                cache_cutoff = pd.Timestamp(cache_cutoff)
                data_start_date = pd.Timestamp(data_start_date)
                data_end_date = pd.Timestamp(data_end_date)
                
                logger.info(f"ğŸ’¾ [CACHE_COMPARISON] Found existing cache range:")
                logger.info(f"  ğŸ“… Cached range: {cache_start.strftime('%Y-%m-%d')} ~ {cache_cutoff.strftime('%Y-%m-%d')}")
                
                # ì‹¤ì œ ë°ì´í„°ê°€ ìºì‹œëœ ë²”ìœ„ë³´ë‹¤ í™•ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸
                data_extended = (
                    data_start_date < cache_start or 
                    data_end_date > cache_cutoff
                )
            else:
                # ìºì‹œ ë‚ ì§œ ë¹„êµì— ì‹¤íŒ¨í•œ ê²½ìš° í™•ì¥ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
                logger.warning("âš ï¸ [CACHE_COMPARE] Cache date comparison failed, treating as data extended")
                data_extended = True
            
            if data_extended:
                logger.info(f"ğŸ“ˆ [DATA_EXTENSION] Data has been extended!")
                logger.info(f"  â¬…ï¸ Start extension: {data_start_date.strftime('%Y-%m-%d')} vs cached {cache_start.strftime('%Y-%m-%d')}")
                logger.info(f"  â¡ï¸ End extension: {data_end_date.strftime('%Y-%m-%d')} vs cached {cache_cutoff.strftime('%Y-%m-%d')}")
                logger.info(f"  ğŸ”„ Using extended data range for date calculation")
            else:
                logger.info(f"âœ… [NO_EXTENSION] Data range matches cached range, proceeding with current data")
        else:
            if force_refresh:
                logger.info(f"ğŸ”„ [FORCE_REFRESH] Ignoring cache due to force refresh")
            else:
                logger.info(f"ğŸ“­ [NO_CACHE] No existing cache found, using full data range")
        
        # ë°ì´í„° ë§ˆì§€ë§‰ ë‚ ì§œì˜ ë‹¤ìŒ ì˜ì—…ì¼ì„ ê³„ì‚°í•˜ì—¬ ì˜ˆì¸¡ ì‹œì‘ì  ì„¤ì • (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)
        # ìµœì†Œ 100ê°œ í–‰ ì´ìƒì˜ íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥
        min_history_rows = 100
        prediction_start_index = max(min_history_rows, total_rows // 4)  # 25% ì§€ì  ë˜ëŠ” ìµœì†Œ 100í–‰ ì¤‘ í° ê°’
        
        # ì‹¤ì œ ì˜ˆì¸¡ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ë‚ ì§œ (ì¶©ë¶„í•œ íˆìŠ¤í† ë¦¬ê°€ ìˆëŠ” ë‚ ì§œë¶€í„°)
        predictable_dates = df.iloc[prediction_start_index:]['Date']
        
        # ì˜ˆì¸¡ ì‹œì‘ ì„ê³„ê°’ ê³„ì‚° (ì°¸ê³ ìš©)
        if prediction_start_index < total_rows:
            prediction_threshold_date = df.iloc[prediction_start_index]['Date']
        else:
            prediction_threshold_date = data_end_date
        
        logger.info(f"ğŸ¯ [PREDICTION_CALC] Prediction calculation:")
        logger.info(f"  ğŸ“Š Min history rows: {min_history_rows}")
        logger.info(f"  ğŸ“ Start index: {prediction_start_index} (date: {prediction_threshold_date.strftime('%Y-%m-%d')})")
        logger.info(f"  ğŸ“… Predictable dates: {len(predictable_dates)} dates available")
        
        # ì˜ˆì¸¡ ê°€ëŠ¥í•œ ëª¨ë“  ë‚ ì§œë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë°˜í™˜ (ìµœì‹  ë‚ ì§œë¶€í„°)
        # days_limitë³´ë‹¤ ì‘ì€ ê²½ìš°ì—ë§Œ ì œí•œ ì ìš©
        if len(predictable_dates) <= days_limit:
            dates = predictable_dates.sort_values(ascending=False).dt.strftime('%Y-%m-%d').tolist()
        else:
            dates = predictable_dates.sort_values(ascending=False).head(days_limit).dt.strftime('%Y-%m-%d').tolist()
        
        logger.info(f"ğŸ”¢ [FINAL_RESULT] Final date calculation:")
        logger.info(f"  ğŸ“Š Available predictable dates: {len(predictable_dates)}")
        logger.info(f"  ğŸ“‹ Returned dates: {len(dates)}")
        logger.info(f"  ğŸ“… Latest available date: {dates[0] if dates else 'None'}")
        
        response_data = {
            'success': True,
            'dates': dates,
            'latest_date': dates[0] if dates else None,  # ì²« ë²ˆì§¸ ìš”ì†Œê°€ ìµœì‹  ë‚ ì§œ (ë‚´ë¦¼ì°¨ìˆœ)
            'data_start_date': data_start_date.strftime('%Y-%m-%d'),
            'data_end_date': data_end_date.strftime('%Y-%m-%d'),
            'prediction_threshold': prediction_threshold_date.strftime('%Y-%m-%d'),
            'min_history_rows': min_history_rows,
            'total_rows': total_rows,
            'file_hash': current_file_hash[:12] if current_file_hash else None,  # ì¶”ê°€: íŒŒì¼ í•´ì‹œ ì •ë³´
            'file_modified': datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S')  # ì¶”ê°€: íŒŒì¼ ìˆ˜ì • ì‹œê°„
        }
        
        logger.info(f"ğŸ“¡ [API_RESPONSE] Sending enhanced dates response:")
        logger.info(f"  ğŸ“… Data range: {response_data['data_start_date']} ~ {response_data['data_end_date']}")
        logger.info(f"  ğŸ¯ Prediction threshold: {response_data['prediction_threshold']}")
        logger.info(f"  ğŸ“… Available date range: {dates[-1] if dates else 'None'} ~ {dates[0] if dates else 'None'} (ìµœì‹ ë¶€í„°)")
        logger.info(f"  ğŸ”‘ File signature: {response_data['file_hash']} @ {response_data['file_modified']}")
        
        return jsonify(response_data)
    except Exception as e:
        logger.error(f"Error reading dates: {str(e)}")
        return jsonify({'error': f'Error reading dates: {str(e)}'}), 500

@app.route('/api/data/refresh', methods=['POST'])
def refresh_file_data():
    """íŒŒì¼ ë°ì´í„° ìƒˆë¡œê³ ì¹¨ ë° ìºì‹œ ê°±ì‹  API"""
    try:
        filepath = request.json.get('filepath') if request.json else request.args.get('filepath')
        if not filepath or not os.path.exists(filepath):
            return jsonify({'error': 'File not found'}), 404
        
        # íŒŒì¼ í•´ì‹œì™€ ìˆ˜ì • ì‹œê°„ í™•ì¸
        current_file_hash = get_data_content_hash(filepath)
        current_file_mtime = os.path.getmtime(filepath)
        
        logger.info(f"ğŸ”„ [FILE_REFRESH] Starting file data refresh:")
        logger.info(f"  ğŸ“ File: {os.path.basename(filepath)}")
        logger.info(f"  ğŸ”‘ Hash: {current_file_hash[:12] if current_file_hash else 'None'}...")
        
        # ê¸°ì¡´ ìºì‹œ í™•ì¸
        existing_cache_range = find_existing_cache_range(filepath)
        refresh_needed = False
        refresh_reason = []
        
        if existing_cache_range:
            # ìºì‹œëœ ë©”íƒ€ë°ì´í„°ì™€ ë¹„êµ
            meta_file = existing_cache_range.get('meta_file')
            if meta_file and os.path.exists(meta_file):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    cached_hash = meta_data.get('file_hash')
                    cached_mtime = meta_data.get('file_modified_time')
                    
                    if cached_hash != current_file_hash:
                        refresh_needed = True
                        refresh_reason.append("File content changed")
                        
                    if cached_mtime and cached_mtime != current_file_mtime:
                        refresh_needed = True
                        refresh_reason.append("File modification time changed")
                        
                except Exception as e:
                    logger.warning(f"Error reading cache metadata: {str(e)}")
                    refresh_needed = True
                    refresh_reason.append("Cache metadata error")
            else:
                refresh_needed = True
                refresh_reason.append("No cache metadata found")
        else:
            refresh_needed = True
            refresh_reason.append("No existing cache")
        
        # íŒŒì¼ ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
        file_ext = os.path.splitext(filepath.lower())[1]
        if file_ext == '.csv':
            # CSV íŒŒì¼ì˜ ê²½ìš° ë‹¤ì–‘í•œ êµ¬ë¶„ìë¡œ ì‹œë„
            try:
                df = pd.read_csv(filepath)
                # ë§Œì•½ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì—ì„œ ì‰¼í‘œê°€ ë°œê²¬ë˜ë©´ êµ¬ë¶„ìê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
                if len(df.columns) == 1 and ',' in str(df.columns[0]):
                    logger.info("ğŸ”§ [CSV_PARSE] Trying different separators for CSV...")
                    # ì„¸ë¯¸ì½œë¡ ìœ¼ë¡œ ì‹œë„
                    try:
                        df = pd.read_csv(filepath, sep=';')
                        logger.info(f"âœ… [CSV_PARSE] Successfully parsed with semicolon: {df.shape}")
                    except:
                        # íƒ­ìœ¼ë¡œ ì‹œë„
                        try:
                            df = pd.read_csv(filepath, sep='\t')
                            logger.info(f"âœ… [CSV_PARSE] Successfully parsed with tab: {df.shape}")
                        except:
                            logger.warning("âš ï¸ [CSV_PARSE] Could not parse with alternative separators")
            except Exception as e:
                logger.error(f"âŒ [CSV_PARSE] Error reading CSV: {str(e)}")
                raise
        else:
            # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
            df = load_data(filepath)
            # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
            if df.index.name == 'Date':
                df = df.reset_index()

        # Date ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ì²˜ë¦¬
        if 'Date' not in df.columns:
            logger.error(f"âŒ [DATE_COLUMN] Date column not found. Available columns: {list(df.columns)}")
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ ë‚ ì§œ í˜•íƒœì¸ì§€ í™•ì¸
            first_col = df.columns[0]
            try:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì˜ ì²« ëª‡ ê°œ ê°’ì´ ë‚ ì§œì¸ì§€ í™•ì¸
                sample_values = df[first_col].head(5).dropna()
                pd.to_datetime(sample_values)
                logger.info(f"âœ… [DATE_COLUMN] Using first column as Date: {first_col}")
                df['Date'] = pd.to_datetime(df[first_col])
            except:
                logger.error(f"âŒ [DATE_COLUMN] First column is not date format: {first_col}")
                raise ValueError(f"Date column not found and first column '{first_col}' is not a valid date format")
        else:
            df['Date'] = pd.to_datetime(df['Date'])
        
        df = df.sort_values('Date')
        
        current_data_range = {
            'start_date': df.iloc[0]['Date'],
            'end_date': df.iloc[-1]['Date'],
            'total_rows': len(df)
        }
        
        # ìºì‹œì™€ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë¹„êµ
        if existing_cache_range and not refresh_needed:
            cache_start = pd.to_datetime(existing_cache_range['start_date'])
            cache_cutoff = pd.to_datetime(existing_cache_range['cutoff_date'])
            
            if (current_data_range['start_date'] < cache_start or 
                current_data_range['end_date'] > cache_cutoff):
                refresh_needed = True
                refresh_reason.append("Data range extended")
        
        response_data = {
            'success': True,
            'refresh_needed': refresh_needed,
            'refresh_reasons': refresh_reason,
            'file_info': {
                'hash': current_file_hash[:12] if current_file_hash else None,
                'modified_time': datetime.fromtimestamp(current_file_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                'total_rows': current_data_range['total_rows'],
                'date_range': {
                    'start': current_data_range['start_date'].strftime('%Y-%m-%d'),
                    'end': current_data_range['end_date'].strftime('%Y-%m-%d')
                }
            }
        }
        
        if existing_cache_range:
            response_data['cache_info'] = {
                'date_range': {
                    'start': existing_cache_range['start_date'],
                    'end': existing_cache_range['cutoff_date']
                },
                'meta_file': existing_cache_range.get('meta_file')
            }
        
        logger.info(f"ğŸ“Š [REFRESH_ANALYSIS] File refresh analysis:")
        logger.info(f"  ğŸ”„ Refresh needed: {refresh_needed}")
        logger.info(f"  ğŸ“ Reasons: {', '.join(refresh_reason) if refresh_reason else 'None'}")
        logger.info(f"  ğŸ“… Current range: {response_data['file_info']['date_range']['start']} ~ {response_data['file_info']['date_range']['end']}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in file refresh check: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/compare-files', methods=['POST'])
def debug_compare_files():
    """ë‘ íŒŒì¼ì„ ì§ì ‘ ë¹„êµí•˜ì—¬ ì°¨ì´ì ì„ ë¶„ì„í•˜ëŠ” ë””ë²„ê¹… API"""
    try:
        data = request.json
        file1_path = data.get('file1_path')
        file2_path = data.get('file2_path')
        
        if not file1_path or not file2_path:
            return jsonify({'error': 'Both file paths are required'}), 400
            
        if not os.path.exists(file1_path) or not os.path.exists(file2_path):
            return jsonify({'error': 'One or both files do not exist'}), 404
        
        logger.info(f"ğŸ” [DEBUG_COMPARE] Comparing files:")
        logger.info(f"  ğŸ“ File 1: {file1_path}")
        logger.info(f"  ğŸ“ File 2: {file2_path}")
        
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´
        file1_hash = get_data_content_hash(file1_path)
        file2_hash = get_data_content_hash(file2_path)
        file1_size = os.path.getsize(file1_path)
        file2_size = os.path.getsize(file2_path)
        file1_mtime = os.path.getmtime(file1_path)
        file2_mtime = os.path.getmtime(file2_path)
        
        # ë°ì´í„° ë¶„ì„ (íŒŒì¼ í˜•ì‹ì— ë§ê²Œ)
        def load_file_safely(filepath):
            file_ext = os.path.splitext(filepath.lower())[1]
            if file_ext == '.csv':
                return pd.read_csv(filepath)
            else:
                # Excel íŒŒì¼ì¸ ê²½ìš° load_data í•¨ìˆ˜ ì‚¬ìš©
                df = load_data(filepath)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if df.index.name == 'Date':
                    df = df.reset_index()
                return df
        
        df1 = load_file_safely(file1_path)
        df2 = load_file_safely(file2_path)
        
        if 'Date' in df1.columns and 'Date' in df2.columns:
            df1['Date'] = pd.to_datetime(df1['Date'])
            df2['Date'] = pd.to_datetime(df2['Date'])
            df1 = df1.sort_values('Date')
            df2 = df2.sort_values('Date')
            
            file1_dates = {
                'start': df1['Date'].min(),
                'end': df1['Date'].max(),
                'count': len(df1)
            }
            
            file2_dates = {
                'start': df2['Date'].min(),
                'end': df2['Date'].max(),
                'count': len(df2)
            }
        else:
            file1_dates = {'error': 'No Date column'}
            file2_dates = {'error': 'No Date column'}
        
        # í™•ì¥ ì²´í¬
        extension_result = check_data_extension(file1_path, file2_path)
        
        # ìºì‹œ í˜¸í™˜ì„± ì²´í¬
        cache_result = find_compatible_cache_file(file2_path)
        
        response_data = {
            'success': True,
            'comparison': {
                'file1': {
                    'path': file1_path,
                    'hash': file1_hash[:12] if file1_hash else None,
                    'size': file1_size,
                    'modified': datetime.fromtimestamp(file1_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'dates': {
                        'start': file1_dates['start'].strftime('%Y-%m-%d') if isinstance(file1_dates.get('start'), pd.Timestamp) else str(file1_dates.get('start')),
                        'end': file1_dates['end'].strftime('%Y-%m-%d') if isinstance(file1_dates.get('end'), pd.Timestamp) else str(file1_dates.get('end')),
                        'count': file1_dates.get('count')
                    } if 'error' not in file1_dates else file1_dates
                },
                'file2': {
                    'path': file2_path,
                    'hash': file2_hash[:12] if file2_hash else None,
                    'size': file2_size,
                    'modified': datetime.fromtimestamp(file2_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                    'dates': {
                        'start': file2_dates['start'].strftime('%Y-%m-%d') if isinstance(file2_dates.get('start'), pd.Timestamp) else str(file2_dates.get('start')),
                        'end': file2_dates['end'].strftime('%Y-%m-%d') if isinstance(file2_dates.get('end'), pd.Timestamp) else str(file2_dates.get('end')),
                        'count': file2_dates.get('count')
                    } if 'error' not in file2_dates else file2_dates
                },
                'identical_hash': file1_hash == file2_hash,
                'size_difference': file2_size - file1_size,
                'extension_analysis': extension_result,
                'cache_analysis': cache_result
            }
        }
        
        logger.info(f"ğŸ“Š [DEBUG_COMPARE] Comparison results:")
        logger.info(f"  ğŸ”‘ Identical hash: {file1_hash == file2_hash}")
        logger.info(f"  ğŸ“ Size difference: {file2_size - file1_size} bytes")
        logger.info(f"  ğŸ“ˆ Is extension: {extension_result.get('is_extension', False)}")
        logger.info(f"  ğŸ’¾ Cache found: {cache_result.get('found', False)}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error in file comparison: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved', methods=['GET'])
def get_saved_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ëª©ë¡ ì¡°íšŒ API"""
    try:
        limit = int(request.args.get('limit', 100))
        predictions_list = get_saved_predictions_list(limit)
        
        return jsonify({
            'success': True,
            'predictions': predictions_list,
            'count': len(predictions_list)
        })
    except Exception as e:
        logger.error(f"Error getting saved predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['GET'])
def get_saved_prediction_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    try:
        result = load_prediction_from_csv(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 404
    except Exception as e:
        logger.error(f"Error loading saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>', methods=['DELETE'])
def delete_saved_prediction_api(date):
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ ì‚­ì œ API"""
    try:
        result = delete_saved_prediction(date)
        
        if result['success']:
            return jsonify(result)
        else:
            return jsonify({'error': result['error']}), 500
    except Exception as e:
        logger.error(f"Error deleting saved prediction: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/saved/<date>/update-actual', methods=['POST'])
def update_prediction_actual_values_api(date):
    """ìºì‹œëœ ì˜ˆì¸¡ì˜ ì‹¤ì œê°’ë§Œ ì—…ë°ì´íŠ¸í•˜ëŠ” API - ì„±ëŠ¥ ìµœì í™”"""
    try:
        # ìš”ì²­ íŒŒë¼ë¯¸í„°
        data = request.json or {}
        update_latest_only = data.get('update_latest_only', True)
        
        logger.info(f"ğŸ”„ [API] Updating actual values for prediction {date}")
        logger.info(f"  ğŸ“Š Update latest only: {update_latest_only}")
        
        # ì‹¤ì œê°’ ì—…ë°ì´íŠ¸ ì‹¤í–‰
        result = update_cached_prediction_actual_values(date, update_latest_only)
        
        if result['success']:
            logger.info(f"âœ… [API] Successfully updated {result.get('updated_count', 0)} actual values")
            return jsonify({
                'success': True,
                'updated_count': result.get('updated_count', 0),
                'message': f'Updated {result.get("updated_count", 0)} actual values',
                'predictions': result['predictions']
            })
        else:
            logger.error(f"âŒ [API] Failed to update actual values: {result.get('error')}")
            return jsonify({'error': result['error']}), 500
            
    except Exception as e:
        logger.error(f"âŒ [API] Error updating actual values: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/predictions/export', methods=['GET'])
def export_predictions():
    """ì €ì¥ëœ ì˜ˆì¸¡ ê²°ê³¼ë“¤ì„ í•˜ë‚˜ì˜ CSV íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸° API"""
    try:
        start_date = request.args.get('start_date')
        end_date = request.args.get('end_date')
        
        # ë‚ ì§œ ë²”ìœ„ì— ë”°ë¥¸ ì˜ˆì¸¡ ë¡œë“œ
        if start_date:
            predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        else:
            # ëª¨ë“  ì €ì¥ëœ ì˜ˆì¸¡ ë¡œë“œ
            predictions_list = get_saved_predictions_list(limit=1000)
            predictions = []
            for pred_info in predictions_list:
                loaded = load_prediction_from_csv(pred_info['prediction_date'])
                if loaded['success']:
                    predictions.extend(loaded['predictions'])
        
        if not predictions:
            return jsonify({'error': 'No predictions found for export'}), 404
        
        # DataFrameìœ¼ë¡œ ë³€í™˜
        if isinstance(predictions[0], dict) and 'predictions' in predictions[0]:
            # ëˆ„ì  ì˜ˆì¸¡ í˜•ì‹ì¸ ê²½ìš°
            all_predictions = []
            for pred_group in predictions:
                all_predictions.extend(pred_group['predictions'])
            export_df = pd.DataFrame(all_predictions)
        else:
            # ë‹¨ìˆœ ì˜ˆì¸¡ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
            export_df = pd.DataFrame(predictions)
        
        # ì„ì‹œ íŒŒì¼ ìƒì„± (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.cs', delete=False)
        export_df.to_csv(temp_file.name, index=False)
        temp_file.close()
        
        # íŒŒì¼ ì „ì†¡
        return send_file(
            temp_file.name,
            as_attachment=True,
            download_name=f'predictions_export_{datetime.now().strftime("%Y%m%d_%H%M%S")}.cs',
            mimetype='application/octet-stream'
        )
        
    except Exception as e:
        logger.error(f"Error exporting predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

# 7. API ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì • - ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì‚¬ìš©
@app.route('/api/predict', methods=['POST'])
def start_prediction_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ì‹œì‘ API - ìºì‹œ ìš°ì„  ì‚¬ìš© (ë¡œê·¸ ê°•í™”)"""
    from app.prediction.background_tasks import background_prediction_simple_compatible # âœ… í•¨ìˆ˜ ë‚´ë¶€ì— ì¶”ê°€
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    current_date = data.get('date')
    save_to_csv = data.get('save_to_csv', True)
    use_cache = data.get('use_cache', True)  # ê¸°ë³¸ê°’ True
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    # ğŸ”‘ íŒŒì¼ ê²½ë¡œë¥¼ ì „ì—­ ìƒíƒœì— ì €ì¥ (ìºì‹œ ì—°ë™ìš©)
    prediction_state['current_file'] = filepath
    
    # âœ… ë¡œê·¸ ê°•í™”
    logger.info(f"ğŸš€ Prediction API called:")
    logger.info(f"  ğŸ“… Target date: {current_date}")
    logger.info(f"  ğŸ“ Data file: {filepath}")
    logger.info(f"  ğŸ’¾ Save to CSV: {save_to_csv}")
    logger.info(f"  ğŸ”„ Use cache: {use_cache}")
    
    # í˜¸í™˜ì„± ìœ ì§€ ë°±ê·¸ë¼ìš´ë“œ í•¨ìˆ˜ ì‹¤í–‰ (ìºì‹œ ìš°ì„  ì‚¬ìš©, ë‹¨ì¼ ì˜ˆì¸¡ë§Œ)
    thread = Thread(target=background_prediction_simple_compatible, 
                   args=(filepath, current_date, save_to_csv, use_cache))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Compatible prediction started (cache-first)',
        'use_cache': use_cache,
        'cache_priority': 'high',
        'features': ['Cache-first loading', 'Unified file naming', 'Enhanced logging', 'Past/Future visualization split']
    })

@app.route('/api/predict/status', methods=['GET'])
def prediction_status():
    """ì˜ˆì¸¡ ìƒíƒœ í™•ì¸ API (ë‚¨ì€ ì‹œê°„ ì¶”ê°€)"""
    from app.prediction.background_tasks import calculate_estimated_time_remaining # âœ… í•¨ìˆ˜ ë‚´ë¶€ì— ì¶”ê°€
    global prediction_state
    
    status = {
        'is_predicting': prediction_state['is_predicting'],
        'progress': prediction_state['prediction_progress'],
        'error': prediction_state['error']
    }
    
    # ì˜ˆì¸¡ ì¤‘ì¸ ê²½ìš° ë‚¨ì€ ì‹œê°„ ê³„ì‚°
    if prediction_state['is_predicting'] and prediction_state['prediction_start_time']:
        time_info = calculate_estimated_time_remaining(
            prediction_state['prediction_start_time'], 
            prediction_state['prediction_progress']
        )
        status.update(time_info)
    
    # ì˜ˆì¸¡ì´ ì™„ë£Œëœ ê²½ìš° ë‚ ì§œ ì •ë³´ë„ ë°˜í™˜
    if not prediction_state['is_predicting'] and prediction_state['current_date']:
        status['current_date'] = prediction_state['current_date']
    
    return jsonify(status)

@app.route('/api/results', methods=['GET'])
def get_prediction_results_compatible():
    """í˜¸í™˜ì„±ì„ ìœ ì§€í•˜ëŠ” ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API (ì˜¤ë¥˜ ìˆ˜ì •)"""
    global prediction_state
    
    logger.info(f"=== API /results called (compatible version) ===")
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    if prediction_state['latest_predictions'] is None:
        return jsonify({'error': 'No prediction results available'}), 404

    try:
        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        if isinstance(prediction_state['latest_predictions'], list):
            raw_predictions = prediction_state['latest_predictions']
        else:
            raw_predictions = prediction_state['latest_predictions']
        
        # ê¸°ì¡´ í˜•íƒœë¡œ ë³€í™˜
        compatible_predictions = convert_to_legacy_format(raw_predictions)
        
        logger.info(f"Converted {len(raw_predictions)} predictions to legacy format")
        logger.info(f"Sample converted prediction: {compatible_predictions[0] if compatible_predictions else 'None'}")
        
        # ë©”íŠ¸ë¦­ ì •ë¦¬
        metrics = prediction_state['latest_metrics']
        cleaned_metrics = {}
        if metrics:
            for key, value in metrics.items():
                cleaned_metrics[key] = safe_serialize_value(value)
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „ ì •ë¦¬ - ì˜¤ë¥˜ ë°©ì§€ ê°•í™”
        interval_scores = prediction_state['latest_interval_scores'] or []
        
        # interval_scores ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ì•ˆì „ ì²˜ë¦¬
        if interval_scores is None:
            interval_scores = []
        elif not isinstance(interval_scores, (list, dict)):
            logger.warning(f"âš ï¸ Unexpected interval_scores type: {type(interval_scores)}, converting to empty list")
            interval_scores = []
        elif isinstance(interval_scores, dict) and not interval_scores:
            interval_scores = []
        
        try:
            cleaned_interval_scores = clean_interval_scores_safe(interval_scores)
        except Exception as interval_error:
            logger.error(f"âŒ Error cleaning interval_scores: {str(interval_error)}")
            cleaned_interval_scores = []
        
        # MA ê²°ê³¼ ì •ë¦¬ ë° í•„ìš”ì‹œ ì¬ê³„ì‚°
        ma_results = prediction_state['latest_ma_results'] or {}
        cleaned_ma_results = {}
        
        # ì´ë™í‰ê·  ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆë‹¤ë©´ ì¬ê³„ì‚° ì‹œë„
        if not ma_results or len(ma_results) == 0:
            logger.info("ğŸ”„ MA results missing, attempting to recalculate...")
            try:
                # í˜„ì¬ ë°ì´í„°ì™€ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë™í‰ê·  ì¬ê³„ì‚°
                current_date = prediction_state.get('current_date')
                if current_date and prediction_state.get('latest_file_path'):
                    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
                    df = load_data(prediction_state['latest_file_path'])
                    if df is not None and not df.empty:
                        # í˜„ì¬ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
                        if isinstance(current_date, str):
                            current_date_dt = pd.to_datetime(current_date)
                        else:
                            current_date_dt = current_date
                        
                        # ê³¼ê±° ë°ì´í„° ì¶”ì¶œ
                        historical_data = df[df.index <= current_date_dt].copy()
                        
                        # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ì´ë™í‰ê·  ê³„ì‚°ìš©ìœ¼ë¡œ ë³€í™˜
                        ma_input_data = []
                        for pred in raw_predictions:
                            try:
                                ma_item = {
                                    'Date': pd.to_datetime(pred.get('Date') or pred.get('date')),
                                    'Prediction': safe_serialize_value(pred.get('Prediction') or pred.get('prediction')),
                                    'Actual': safe_serialize_value(pred.get('Actual') or pred.get('actual'))
                                }
                                ma_input_data.append(ma_item)
                            except Exception as e:
                                logger.warning(f"âš ï¸ Error processing MA data item: {str(e)}")
                                continue
                        
                        # ì´ë™í‰ê·  ê³„ì‚°
                        if ma_input_data:
                            ma_results = calculate_moving_averages_with_history(
                                ma_input_data, historical_data, target_col='MOPJ'
                            )
                            if ma_results:
                                logger.info(f"âœ… MA recalculated successfully with {len(ma_results)} windows")
                                prediction_state['latest_ma_results'] = ma_results
                            else:
                                logger.warning("âš ï¸ MA recalculation returned empty results")
                        else:
                            logger.warning("âš ï¸ No valid input data for MA calculation")
                    else:
                        logger.warning("âš ï¸ Unable to load original data for MA calculation")
                else:
                    logger.warning("âš ï¸ Missing current_date or file_path for MA calculation")
            except Exception as e:
                logger.error(f"âŒ Error recalculating MA: {str(e)}")
        
        # MA ê²°ê³¼ ì •ë¦¬
        for key, value in ma_results.items():
            if isinstance(value, list):
                cleaned_ma_results[key] = []
                for item in value:
                    if isinstance(item, dict):
                        cleaned_item = {}
                        for k, v in item.items():
                            cleaned_item[k] = safe_serialize_value(v)
                        cleaned_ma_results[key].append(cleaned_item)
                    else:
                        cleaned_ma_results[key].append(safe_serialize_value(item))
            else:
                cleaned_ma_results[key] = safe_serialize_value(value)
        
        # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬
        attention_data = prediction_state['latest_attention_data']
        cleaned_attention = None
        
        logger.info(f"ğŸ“Š [ATTENTION] Processing attention data: available={bool(attention_data)}")
        if attention_data:
            logger.info(f"ğŸ“Š [ATTENTION] Original keys: {list(attention_data.keys())}")
            
            cleaned_attention = {}
            for key, value in attention_data.items():
                if key == 'image' and value:
                    cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                    logger.info(f"ğŸ“Š [ATTENTION] Image data preserved (length: {len(value) if isinstance(value, str) else 'N/A'})")
                elif isinstance(value, dict):
                    cleaned_attention[key] = {}
                    for k, v in value.items():
                        cleaned_attention[key][k] = safe_serialize_value(v)
                    logger.info(f"ğŸ“Š [ATTENTION] Dict processed for key '{key}': {len(cleaned_attention[key])} items")
                else:
                    cleaned_attention[key] = safe_serialize_value(value)
                    logger.info(f"ğŸ“Š [ATTENTION] Value processed for key '{key}': {type(value)}")
            
            logger.info(f"ğŸ“Š [ATTENTION] Final cleaned keys: {list(cleaned_attention.keys())}")
        else:
            logger.warning(f"ğŸ“Š [ATTENTION] No attention data available in prediction_state")
        
        # í”Œë¡¯ ë°ì´í„° ì •ë¦¬
        plots = prediction_state['latest_plots'] or {}
        cleaned_plots = {}
        for key, value in plots.items():
            if isinstance(value, dict):
                cleaned_plots[key] = {}
                for k, v in value.items():
                    if k == 'image' and v:
                        cleaned_plots[key][k] = v  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                    else:
                        cleaned_plots[key][k] = safe_serialize_value(v)
            else:
                cleaned_plots[key] = safe_serialize_value(value)
        
        response_data = {
            'success': True,
            'current_date': safe_serialize_value(prediction_state['current_date']),
            'predictions': compatible_predictions,  # í˜¸í™˜ì„± ìœ ì§€ëœ í˜•íƒœ
            'interval_scores': cleaned_interval_scores,
            'ma_results': cleaned_ma_results,
            'attention_data': cleaned_attention,
            'plots': cleaned_plots,
            'metrics': cleaned_metrics if cleaned_metrics else None,
            'selected_features': prediction_state['selected_features'] or [],
            'feature_importance': safe_serialize_value(prediction_state.get('feature_importance')),
            'semimonthly_period': safe_serialize_value(prediction_state['semimonthly_period']),
            'next_semimonthly_period': safe_serialize_value(prediction_state['next_semimonthly_period'])
        }
        
        # ğŸ”§ ê°•í™”ëœ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        try:
            test_json = json.dumps(response_data)
            # ì§ë ¬í™”ëœ JSONì— NaNì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¶”ê°€ í™•ì¸
            if 'NaN' in test_json or 'Infinity' in test_json:
                logger.error(f"JSON contains NaN/Infinity values")
                # NaN ê°’ë“¤ì„ ëª¨ë‘ nullë¡œ êµì²´
                test_json_cleaned = test_json.replace('NaN', 'null').replace('Infinity', 'null').replace('-Infinity', 'null')
                response_data = json.loads(test_json_cleaned)
            logger.info(f"JSON serialization test: SUCCESS (length: {len(test_json)})")
        except Exception as json_error:
            logger.error(f"JSON serialization test: FAILED - {str(json_error)}")
            logger.error(f"Error details: {traceback.format_exc()}")
            
            # ğŸ”§ ê°•í™”ëœ ì‘ê¸‰ ì²˜ì¹˜: ì¬ê·€ì  NaN ì œê±°
            try:
                logger.info("ğŸ”§ Attempting emergency data cleaning...")
                
                def deep_clean_nan(obj):
                    """ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  NaN ê°’ì„ ì œê±°"""
                    if obj is None:
                        return None
                    elif isinstance(obj, dict):
                        cleaned = {}
                        for k, v in obj.items():
                            cleaned[k] = deep_clean_nan(v)
                        return cleaned
                    elif isinstance(obj, list):
                        return [deep_clean_nan(item) for item in obj]
                    elif isinstance(obj, (int, float, np.number)):
                        try:
                            if pd.isna(obj) or np.isnan(obj) or np.isinf(obj):
                                return None
                            return float(obj) if isinstance(obj, (float, np.floating)) else int(obj)
                        except:
                            return None
                    elif isinstance(obj, str):
                        if obj.lower() in ['nan', 'inf', '-inf', 'infinity', '-infinity']:
                            return None
                        return obj
                    else:
                        return safe_serialize_value(obj)
                
                # ì „ì²´ ì‘ë‹µ ë°ì´í„° ì •ë¦¬
                response_data = deep_clean_nan(response_data)
                
                # ì¬ì‹œë„
                test_json = json.dumps(response_data)
                logger.info("âœ… Emergency cleaning successful")
                
            except Exception as emergency_error:
                logger.error(f"âŒ Emergency cleaning failed: {str(emergency_error)}")
                logger.error(f"âŒ Original error: {str(json_error)}")
                return jsonify({
                    'success': False,
                    'error': f'Data serialization error: {str(json_error)}'
                }), 500
        
        logger.info(f"=== Compatible Response Summary ===")
        logger.info(f"Total predictions: {len(compatible_predictions)}")
        logger.info(f"Has metrics: {cleaned_metrics is not None}")
        logger.info(f"Sample prediction fields: {list(compatible_predictions[0].keys()) if compatible_predictions else 'None'}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"Error creating compatible response: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'Error creating response: {str(e)}'}), 500

@app.route('/api/results/attention-map', methods=['GET'])
def get_attention_map():
    """ì–´í…ì…˜ ë§µ ë°ì´í„° ì¡°íšŒ API"""
    global prediction_state
    
    logger.info("ğŸ” [ATTENTION_MAP] API call received - FINAL UPDATE")
    
    # ì–´í…ì…˜ ë°ì´í„° í™•ì¸
    attention_data = prediction_state.get('latest_attention_data')
    
    # í…ŒìŠ¤íŠ¸ìš©: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ìƒì„±
    test_mode = request.args.get('test', '').lower() == 'true'
    
    if not attention_data:
        if test_mode:
            logger.info("ğŸ§ª [ATTENTION_MAP] Creating test data")
            attention_data = {
                'image': 'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg==',
                'feature_importance': {
                    'Feature_1': 0.35,
                    'Feature_2': 0.25,
                    'Feature_3': 0.20,
                    'Feature_4': 0.15,
                    'Feature_5': 0.05
                },
                'temporal_importance': {
                    '2024-01-01': 0.1,
                    '2024-01-02': 0.2,
                    '2024-01-03': 0.3,
                    '2024-01-04': 0.4
                }
            }
        else:
            logger.warning("âš ï¸ [ATTENTION_MAP] No attention data available")
            return jsonify({
                'error': 'No attention map data available',
                'message': 'ì˜ˆì¸¡ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”. ì˜ˆì¸¡ ì™„ë£Œ í›„ ì–´í…ì…˜ ë§µ ë°ì´í„°ê°€ ìƒì„±ë©ë‹ˆë‹¤.',
                'suggestion': 'CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì˜ˆì¸¡ì„ ì‹¤í–‰í•œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
                'test_url': '/api/results/attention-map?test=true'
            }), 404
    
    logger.info(f"ğŸ“Š [ATTENTION_MAP] Available keys: {list(attention_data.keys())}")
    
    # ì–´í…ì…˜ ë°ì´í„° ì •ë¦¬ ë° ì§ë ¬í™”
    cleaned_attention = {}
    
    try:
        for key, value in attention_data.items():
            if key == 'image' and value:
                cleaned_attention[key] = value  # base64 ì´ë¯¸ì§€ëŠ” ê·¸ëŒ€ë¡œ
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Image data preserved (length: {len(value) if isinstance(value, str) else 'N/A'})")
            elif isinstance(value, dict):
                cleaned_attention[key] = {}
                for k, v in value.items():
                    cleaned_attention[key][k] = safe_serialize_value(v)
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Dict processed for key '{key}': {len(cleaned_attention[key])} items")
            else:
                cleaned_attention[key] = safe_serialize_value(value)
                logger.info(f"ğŸ“Š [ATTENTION_MAP] Value processed for key '{key}': {type(value)}")
        
        response_data = {
            'success': True,
            'attention_data': cleaned_attention,
            'current_date': safe_serialize_value(prediction_state.get('current_date')),
            'feature_importance': safe_serialize_value(prediction_state.get('feature_importance'))
        }
        
        # JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸
        json.dumps(response_data)
        
        logger.info(f"âœ… [ATTENTION_MAP] Response ready with keys: {list(cleaned_attention.keys())}")
        
        return jsonify(response_data)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [ATTENTION_MAP] Error processing attention data: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'error': f'Error processing attention map: {str(e)}'}), 500

@app.route('/api/features', methods=['GET'])
def get_features():
    """ì„ íƒëœ íŠ¹ì„± ì¡°íšŒ API"""
    global prediction_state
    
    if prediction_state['selected_features'] is None:
        return jsonify({'error': 'No feature information available'}), 404
    
    return jsonify({
        'success': True,
        'current_date': prediction_state['current_date'],
        'selected_features': prediction_state['selected_features'],
        'feature_importance': prediction_state['feature_importance']
    })

# ì •ì  íŒŒì¼ ì œê³µ
@app.route('/static/<path:path>')
def serve_static(path):
    return send_file(os.path.join('static', path))

# ê¸°ë³¸ ë¼ìš°íŠ¸
@app.route('/')
def index():
    return jsonify({
        'app': 'MOPJ Prediction API',
        'version': '1.0.0',
        'status': 'running',
        'endpoints': [
            '/api/health',
            '/api/upload',
            '/api/holidays',
            '/api/holidays/upload',
            '/api/holidays/reload',
            '/api/file/metadata',
            '/api/data/dates',
            '/api/predict',
            '/api/predict/accumulated',
            '/api/predict/status',
            '/api/results',
            '/api/results/predictions',
            '/api/results/interval-scores',
            '/api/results/moving-averages',
            '/api/results/attention-map',
            '/api/results/accumulated',
            '/api/results/accumulated/interval-scores',
            '/api/results/accumulated/<date>',
            '/api/results/accumulated/report',
            '/api/results/accumulated/visualization',
            '/api/results/reliability',  # ìƒˆë¡œ ì¶”ê°€ëœ ì‹ ë¢°ë„ API
            '/api/features'
        ],
        'new_features': [
            'Prediction consistency scoring (ì˜ˆì¸¡ ì‹ ë¢°ë„)',
            'Purchase reliability percentage (êµ¬ë§¤ ì‹ ë¢°ë„)',
            'Holiday management system',
            'Accumulated predictions analysis'
        ]
    })

# 4. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘
@app.route('/api/predict/accumulated', methods=['POST'])
def start_accumulated_prediction():
    """ì—¬ëŸ¬ ë‚ ì§œì— ëŒ€í•œ ëˆ„ì  ì˜ˆì¸¡ ì‹œì‘ API (ì €ì¥/ë¡œë“œ ê¸°ëŠ¥ í¬í•¨)"""
    from app.prediction.background_tasks import run_accumulated_predictions_with_save # âœ… í•¨ìˆ˜ ë‚´ë¶€ì— ì¶”ê°€
    global prediction_state
    
    if prediction_state['is_predicting']:
        return jsonify({
            'success': False,
            'error': 'Prediction already in progress',
            'progress': prediction_state['prediction_progress']
        }), 409
    
    data = request.json
    filepath = data.get('filepath')
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    save_to_csv = data.get('save_to_csv', True)
    use_saved_data = data.get('use_saved_data', True)  # ì €ì¥ëœ ë°ì´í„° í™œìš© ì—¬ë¶€
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    
    if not start_date:
        return jsonify({'error': 'Start date is required'}), 400
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëˆ„ì  ì˜ˆì¸¡ ì‹¤í–‰
    thread = Thread(target=run_accumulated_predictions_with_save, 
                   args=(filepath, start_date, end_date, save_to_csv, use_saved_data))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Accumulated prediction started',
        'save_to_csv': save_to_csv,
        'use_saved_data': use_saved_data,
        'status_url': '/api/predict/status'
    })

# 5. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ
@app.route('/api/results/accumulated', methods=['GET'])
def get_accumulated_results():
    global prediction_state
    
    logger.info("ğŸ” [ACCUMULATED] API call received")
    
    if prediction_state['is_predicting']:
        logger.warning("âš ï¸ [ACCUMULATED] Prediction still in progress")
        return jsonify({
            'success': False,
            'error': 'Prediction in progress',
            'progress': prediction_state['prediction_progress']
        }), 409

    if not prediction_state['accumulated_predictions']:
        logger.error("âŒ [ACCUMULATED] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404

    logger.info("âœ… [ACCUMULATED] Processing accumulated predictions...")
    
    # ëˆ„ì  êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° - ì˜¬ë°”ë¥¸ ë°©ì‹ ì‚¬ìš©
    accumulated_purchase_reliability, _ = calculate_accumulated_purchase_reliability(
        prediction_state['accumulated_predictions']
    )
    
    logger.info(f"ğŸ’° [ACCUMULATED] Purchase reliability calculated: {accumulated_purchase_reliability}")
    
    # âœ… ìƒì„¸ ë””ë²„ê¹… ë¡œê¹… ì¶”ê°€
    logger.info(f"ğŸ” [ACCUMULATED] Purchase reliability debugging:")
    logger.info(f"   - Type: {type(accumulated_purchase_reliability)}")
    logger.info(f"   - Value: {accumulated_purchase_reliability}")
    logger.info(f"   - Repr: {repr(accumulated_purchase_reliability)}")
    if accumulated_purchase_reliability == 100.0:
        logger.warning(f"âš ï¸ [ACCUMULATED] 100% reliability detected! Detailed analysis:")
        logger.warning(f"   - Total predictions: {len(prediction_state['accumulated_predictions'])}")
        for i, pred in enumerate(prediction_state['accumulated_predictions'][:3]):  # ì²˜ìŒ 3ê°œë§Œ
            logger.warning(f"   - Prediction {i+1}: date={pred.get('date')}, interval_scores_keys={list(pred.get('interval_scores', {}).keys())}")
    
    # ë°ì´í„° ì•ˆì „ì„± ê²€ì‚¬
    safe_interval_scores = []
    if prediction_state.get('accumulated_interval_scores'):
        safe_interval_scores = [
            item for item in prediction_state['accumulated_interval_scores'] 
            if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
        ]
        logger.info(f"ğŸ“Š [ACCUMULATED] Safe interval scores count: {len(safe_interval_scores)}")
    else:
        logger.warning("âš ï¸ [ACCUMULATED] No accumulated_interval_scores found")
    
    consistency_scores = prediction_state.get('accumulated_consistency_scores', {})
    logger.info(f"ğŸ¯ [ACCUMULATED] Consistency scores keys: {list(consistency_scores.keys())}")
    
    # âœ… ìºì‹œ í†µê³„ ì •ë³´ ì¶”ê°€
    cache_stats = prediction_state.get('cache_statistics', {
        'total_dates': 0,
        'cached_dates': 0,
        'new_predictions': 0,
        'cache_hit_rate': 0.0
    })
    
    # ğŸ”§ NaN ê°’ ì²˜ë¦¬ ê°•í™” - ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬
    safe_predictions = clean_predictions_data(prediction_state['accumulated_predictions'])
    
    # ğŸ”§ ëˆ„ì  ë©”íŠ¸ë¦­ìŠ¤ì—ì„œ NaN ê°’ ì œê±°
    safe_accumulated_metrics = {}
    for key, value in prediction_state.get('accumulated_metrics', {}).items():
        safe_accumulated_metrics[key] = safe_serialize_value(value)
    
    # ğŸ”§ ì¼ê´€ì„± ì ìˆ˜ì—ì„œ NaN ê°’ ì œê±°
    safe_consistency_scores = {}
    for key, value in consistency_scores.items():
        safe_consistency_scores[key] = safe_serialize_value(value)
    
    # ğŸ”§ êµ¬ë§¤ ì‹ ë¢°ë„ NaN ê°’ ì²˜ë¦¬
    safe_purchase_reliability = safe_serialize_value(accumulated_purchase_reliability)
    if safe_purchase_reliability is None:
        safe_purchase_reliability = 0.0
    
    # ğŸ”§ ìºì‹œ í†µê³„ NaN ê°’ ì²˜ë¦¬
    safe_cache_stats = {}
    for key, value in cache_stats.items():
        safe_cache_stats[key] = safe_serialize_value(value)
    
    response_data = {
        'success': True,
        'prediction_dates': prediction_state.get('prediction_dates', []),
        'accumulated_metrics': safe_accumulated_metrics,
        'predictions': safe_predictions,
        'accumulated_interval_scores': safe_interval_scores,
        'accumulated_consistency_scores': safe_consistency_scores,
        'accumulated_purchase_reliability': safe_purchase_reliability,
        'cache_statistics': safe_cache_stats
    }
    
    # âœ… ìµœì¢… ì‘ë‹µ ë°ì´í„° ê²€ì¦ ë¡œê¹…
    logger.info(f"ğŸ“¤ [ACCUMULATED] Final response validation:")
    logger.info(f"   - accumulated_purchase_reliability in response: {response_data['accumulated_purchase_reliability']}")
    logger.info(f"   - Type in response: {type(response_data['accumulated_purchase_reliability'])}")
    
    logger.info(f"ğŸ“¤ [ACCUMULATED] Response summary: predictions={len(response_data['predictions'])}, metrics_keys={list(response_data['accumulated_metrics'].keys())}, reliability={response_data['accumulated_purchase_reliability']}")
    
    # ğŸ”§ JSON ì§ë ¬í™” í…ŒìŠ¤íŠ¸ ë° NaN ê°’ ê°•ì œ ì œê±°
    try:
        test_json = json.dumps(response_data)
        # ì§ë ¬í™”ëœ JSONì— NaNì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ì¶”ê°€ í™•ì¸
        if 'NaN' in test_json or 'Infinity' in test_json:
            logger.error(f"ğŸš¨ [ACCUMULATED] JSON contains NaN/Infinity values")
            logger.error(f"   - JSON snippet: {test_json[:500]}...")
            test_json_cleaned = test_json.replace('NaN', 'null').replace('Infinity', 'null').replace('-Infinity', 'null')
            response_data = json.loads(test_json_cleaned)
            logger.info(f"âœ… [ACCUMULATED] JSON NaN values cleaned successfully")
    except Exception as e:
        logger.error(f"âŒ [ACCUMULATED] JSON serialization failed: {e}")
        logger.error(f"   - Error type: {type(e)}")
        logger.error(f"   - Error details: {str(e)}")
        # ì¶”ê°€ ì •ë¦¬ ì‹œë„
        for key, value in response_data.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, float) and (np.isnan(sub_value) or np.isinf(sub_value)):
                        response_data[key][sub_key] = None
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        for sub_key, sub_value in item.items():
                            if isinstance(sub_value, float) and (np.isnan(sub_value) or np.isinf(sub_value)):
                                response_data[key][i][sub_key] = None
    
    # ğŸ”§ Excel í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (API ì‘ë‹µ ì „ì— ì‹¤í–‰)
    cleanup_excel_processes()
    
    return jsonify(response_data)

@app.route('/api/results/accumulated/interval-scores', methods=['GET'])
def get_accumulated_interval_scores():
    global prediction_state
    scores = prediction_state.get('accumulated_interval_scores', [])
    
    # 'days' ì†ì„±ì´ ì—†ëŠ” í•­ëª© í•„í„°ë§
    safe_scores = [
        item for item in scores 
        if item is not None and isinstance(item, dict) and 'days' in item and item['days'] is not None
    ]
    
    return jsonify(safe_scores)

# 7. ëˆ„ì  ë³´ê³ ì„œ API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/report', methods=['GET'])
def get_accumulated_report():
    from app.visualization.plotter import generate_accumulated_report
    """ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„± ë° ë‹¤ìš´ë¡œë“œ API"""
    global prediction_state
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    report_file = generate_accumulated_report()
    if not report_file:
        return jsonify({'error': 'Failed to generate report'}), 500
    
    return send_file(report_file, as_attachment=True)

def return_prediction_result(pred, date, match_type):
    """
    ì˜ˆì¸¡ ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Parameters:
    -----------
    pred : dict
        ì˜ˆì¸¡ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    date : str
        ìš”ì²­ëœ ë‚ ì§œ
    match_type : str
        ë§¤ì¹­ ë°©ì‹ ì„¤ëª…
    
    Returns:
    --------
    JSON response
    """
    try:
        logger.info(f"ğŸ”„ [API] Returning prediction result for date={date}, match_type={match_type}")
        
        # ì˜ˆì¸¡ ë°ì´í„° ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        predictions = pred.get('predictions', [])
        if not isinstance(predictions, list):
            logger.warning(f"âš ï¸ [API] predictions is not a list: {type(predictions)}")
            predictions = []
        
        # êµ¬ê°„ ì ìˆ˜ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ ë° ë³€í™˜
        interval_scores = pred.get('interval_scores', {})
        if isinstance(interval_scores, dict):
            # ë”•ì…”ë„ˆë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            interval_scores_list = []
            for key, interval in interval_scores.items():
                if interval and isinstance(interval, dict) and 'days' in interval:
                    interval_scores_list.append(interval)
            interval_scores = interval_scores_list
        elif not isinstance(interval_scores, list):
            logger.warning(f"âš ï¸ [API] interval_scores is neither dict nor list: {type(interval_scores)}")
            interval_scores = []
        
        # ë©”íŠ¸ë¦­ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ
        metrics = pred.get('metrics', {})
        if not isinstance(metrics, dict):
            logger.warning(f"âš ï¸ [API] metrics is not a dict: {type(metrics)}")
            metrics = {}
        
        # ğŸ”„ ì´ë™í‰ê·  ë°ì´í„° ì¶”ì¶œ (ìºì‹œëœ ë°ì´í„° ë˜ëŠ” íŒŒì¼ì—ì„œ ë¡œë“œ)
        ma_results = pred.get('ma_results', {})
        if not ma_results:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ MA íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    ma_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_ma.json"
                
                if ma_file_path.exists():
                    with open(ma_file_path, 'r', encoding='utf-8') as f:
                        ma_results = json.load(f)
                    logger.info(f"ğŸ“Š [API] MA results loaded from file for {date}: {len(ma_results)} windows")
                else:
                    logger.info(f"âš ï¸ [API] No MA file found for {date}: {ma_file_path}")
                    
                    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì˜ˆì¸¡ ë°ì´í„°ì—ì„œ ì¬ê³„ì‚° (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ì—†ì´ ì œí•œì ìœ¼ë¡œ)
                    if predictions:
                        ma_results = calculate_moving_averages_with_history(
                            predictions, None, target_col='MOPJ', windows=[5, 10, 23]
                        )
                        logger.info(f"ğŸ“Š [API] MA results recalculated for {date}: {len(ma_results)} windows")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading/calculating MA for {date}: {str(e)}")
                ma_results = {}
        
        # ğŸ¯ Attention ë°ì´í„° ì¶”ì¶œ
        attention_data = pred.get('attention_data', {})
        if not attention_data:
            # íŒŒì¼ë³„ ìºì‹œì—ì„œ Attention íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                current_file = prediction_state.get('current_file')
                if current_file:
                    cache_dirs = get_file_cache_dirs(current_file)
                    predictions_dir = cache_dirs['predictions']
                    
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = predictions_dir / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                else:
                    # ë°±ì—…: ê¸€ë¡œë²Œ ìºì‹œ ì‚¬ìš©
                    pred_start_date = pred.get('prediction_start_date') or date
                    attention_file_path = Path(PREDICTIONS_DIR) / f"prediction_start_{pd.to_datetime(pred_start_date).strftime('%Y%m%d')}_attention.json"
                
                if attention_file_path.exists():
                    with open(attention_file_path, 'r', encoding='utf-8') as f:
                        attention_data = json.load(f)
                    logger.info(f"ğŸ“Š [API] Attention data loaded from file for {date}")
                else:
                    logger.info(f"âš ï¸ [API] No attention file found for {date}: {attention_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error loading attention data for {date}: {str(e)}")
        
        # ê¸°ë³¸ ì‘ë‹µ ë°ì´í„° êµ¬ì„±
        response_data = {
            'success': True,
            'date': date,
            'predictions': predictions,
            'interval_scores': interval_scores,
            'metrics': metrics,
            'ma_results': ma_results,
            'attention_data': attention_data,
            'next_semimonthly_period': pred.get('next_semimonthly_period'),
            'actual_business_days': pred.get('actual_business_days'),
            'match_type': match_type,
            'data_end_date': pred.get('date'),
            'prediction_start_date': pred.get('prediction_start_date')
        }
        
        # ê° í•„ë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ì•ˆì „í•˜ê²Œ ì§ë ¬í™”
        safe_response = {}
        for key, value in response_data.items():
            safe_value = safe_serialize_value(value)
            if safe_value is not None:  # Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì¶”ê°€
                safe_response[key] = safe_value
        
        # successì™€ dateëŠ” í•­ìƒ í¬í•¨
        safe_response['success'] = True
        safe_response['date'] = date
        
        logger.info(f"âœ… [API] Successfully prepared response for {date}: predictions={len(safe_response.get('predictions', []))}, interval_scores={len(safe_response.get('interval_scores', []))}, ma_windows={len(safe_response.get('ma_results', {}))}, attention_data={bool(safe_response.get('attention_data'))}")
        
        return jsonify(safe_response)
        
    except Exception as e:
        logger.error(f"ğŸ’¥ [API] Error in return_prediction_result for {date}: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': f'Error processing prediction result: {str(e)}',
            'date': date
        }), 500

# 8. API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€ - íŠ¹ì • ë‚ ì§œ ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ

@app.route('/api/results/accumulated/<date>', methods=['GET'])
def get_accumulated_result_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    global prediction_state
    
    logger.info(f"ğŸ” [API] Searching for accumulated result by date: {date}")
    
    if not prediction_state['accumulated_predictions']:
        logger.warning("âŒ [API] No accumulated prediction results available")
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    logger.info(f"ğŸ“Š [API] Available prediction dates (data_end_date): {[p['date'] for p in prediction_state['accumulated_predictions']]}")
    
    # âœ… 1ë‹¨ê³„: ì •í™•í•œ ë°ì´í„° ê¸°ì¤€ì¼ ë§¤ì¹­ ìš°ì„  í™•ì¸
    logger.info(f"ğŸ” [API] Step 1: Looking for EXACT data_end_date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}")
        
        if data_end_date == date:
            logger.info(f"âœ… [API] Found prediction by EXACT DATA END DATE match: {date}")
            logger.info(f"ğŸ“Š [API] Prediction data preview: predictions={len(pred.get('predictions', []))}, interval_scores={len(pred.get('interval_scores', {}))}")
            return return_prediction_result(pred, date, "exact data end date")
    
    # âœ… 2ë‹¨ê³„: ì •í™•í•œ ë§¤ì¹­ì´ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ì˜ˆì¸¡ ì‹œì‘ì¼ë¡œ ë§¤ì¹­
    logger.info(f"ğŸ” [API] Step 2: No exact match found. Looking for calculated prediction start date match for {date}")
    for i, pred in enumerate(prediction_state['accumulated_predictions']):
        data_end_date = pred.get('date')  # ë°ì´í„° ê¸°ì¤€ì¼
        prediction_start_date = pred.get('prediction_start_date')  # ì˜ˆì¸¡ ì‹œì‘ì¼
        
        logger.info(f"ğŸ” [API] Checking prediction {i+1}: data_end_date={data_end_date}, prediction_start_date={prediction_start_date}")
        
        if data_end_date:
            try:
                data_end_dt = pd.to_datetime(data_end_date)
                calculated_start_date = data_end_dt + pd.Timedelta(days=1)
                
                # ì£¼ë§ê³¼ íœ´ì¼ ê±´ë„ˆë›°ê¸°
                while calculated_start_date.weekday() >= 5 or is_holiday(calculated_start_date):
                    calculated_start_date += pd.Timedelta(days=1)
                
                calculated_start_str = calculated_start_date.strftime('%Y-%m-%d')
                
                if calculated_start_str == date:
                    logger.info(f"âœ… [API] Found prediction by CALCULATED PREDICTION START DATE: {date} (from data end date: {data_end_date})")
                    return return_prediction_result(pred, date, "calculated prediction start date from data end date")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ [API] Error calculating prediction start date for {data_end_date}: {str(e)}")
                continue
    
    logger.error(f"âŒ [API] No prediction results found for date {date}")
    return jsonify({'error': f'No prediction results for date {date}'}), 404

# 10. ëˆ„ì  ì§€í‘œ ì‹œê°í™” API ì—”ë“œí¬ì¸íŠ¸
@app.route('/api/results/accumulated/visualization', methods=['GET'])
def get_accumulated_visualization():
    """ëˆ„ì  ì˜ˆì¸¡ ì§€í‘œ ì‹œê°í™” API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    filename, img_str = visualize_accumulated_metrics()
    if not filename:
        return jsonify({'error': 'Failed to generate visualization'}), 500
    
    return jsonify({
        'success': True,
        'file_path': filename,
        'image': img_str
    })

# ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.route('/api/results/reliability', methods=['GET'])
def get_reliability_scores():
    """ì‹ ë¢°ë„ ì ìˆ˜ ì¡°íšŒ API"""
    global prediction_state
    
    # ë‹¨ì¼ ì˜ˆì¸¡ ì‹ ë¢°ë„
    single_reliability = {}
    if prediction_state.get('latest_interval_scores') and prediction_state.get('latest_predictions'):
        try:
            # ì‹¤ì œ ì˜ì—…ì¼ ìˆ˜ ê³„ì‚°
            actual_business_days = len([p for p in prediction_state['latest_predictions'] 
                                       if p.get('Date') and not p.get('is_synthetic', False)])
            
            single_reliability = {
                'period': prediction_state['next_semimonthly_period']
            }
        except Exception as e:
            logger.error(f"Error calculating single prediction reliability: {str(e)}")
            single_reliability = {'error': 'Unable to calculate single prediction reliability'}
    
    # ëˆ„ì  ì˜ˆì¸¡ ì‹ ë¢°ë„ (ì•ˆì „í•œ ì ‘ê·¼)
    accumulated_reliability = prediction_state.get('accumulated_consistency_scores', {})
    
    return jsonify({
        'success': True,
        'single_prediction_reliability': single_reliability,
        'accumulated_prediction_reliability': accumulated_reliability
    })

@app.route('/api/cache/clear/accumulated', methods=['POST'])
def clear_accumulated_cache():
    """ëˆ„ì  ì˜ˆì¸¡ ìºì‹œ í´ë¦¬ì–´"""
    global prediction_state
    
    try:
        # ëˆ„ì  ì˜ˆì¸¡ ê´€ë ¨ ìƒíƒœ í´ë¦¬ì–´
        prediction_state['accumulated_predictions'] = []
        prediction_state['accumulated_metrics'] = {}
        prediction_state['accumulated_interval_scores'] = []
        prediction_state['accumulated_consistency_scores'] = {}
        prediction_state['accumulated_purchase_reliability'] = 0
        prediction_state['prediction_dates'] = []
        
        logger.info("ğŸ§¹ [CACHE] Accumulated prediction cache cleared")
        
        return jsonify({
            'success': True,
            'message': 'Accumulated prediction cache cleared successfully'
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE] Error clearing accumulated cache: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/debug/reliability', methods=['GET'])
def debug_reliability_calculation():
    """êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚° ë””ë²„ê¹… API"""
    global prediction_state
    
    if not prediction_state['accumulated_predictions']:
        return jsonify({'error': 'No accumulated prediction results available'}), 404
    
    predictions = prediction_state['accumulated_predictions']
    print(f"ğŸ” [DEBUG] Total predictions: {len(predictions)}")
    
    debug_data = {
        'prediction_count': len(predictions),
        'predictions_details': []
    }
    
    total_score = 0
    
    for i, pred in enumerate(predictions):
        pred_date = pred.get('date')
        interval_scores = pred.get('interval_scores', {})
        
        print(f"ğŸ“Š [DEBUG] Prediction {i+1} ({pred_date}):")
        print(f"   - interval_scores type: {type(interval_scores)}")
        print(f"   - interval_scores keys: {list(interval_scores.keys()) if isinstance(interval_scores, dict) else 'N/A'}")
        
        pred_detail = {
            'date': pred_date,
            'interval_scores_type': str(type(interval_scores)),
            'interval_scores_keys': list(interval_scores.keys()) if isinstance(interval_scores, dict) else [],
            'individual_scores': [],
            'best_score': 0
        }
        
        if isinstance(interval_scores, dict):
            for key, score_data in interval_scores.items():
                print(f"   - {key}: {score_data}")
                if isinstance(score_data, dict) and 'score' in score_data:
                    score_value = score_data.get('score', 0)
                    pred_detail['individual_scores'].append({
                        'key': key,
                        'score': score_value,
                        'full_data': score_data
                    })
                    print(f"     -> score: {score_value}")
        
        if pred_detail['individual_scores']:
            best_score = max([s['score'] for s in pred_detail['individual_scores']])
            # ì ìˆ˜ë¥¼ 3ì ìœ¼ë¡œ ì œí•œ
            capped_score = min(best_score, 3.0)
            pred_detail['best_score'] = best_score
            pred_detail['capped_score'] = capped_score
            total_score += capped_score
            print(f"   - Best score: {best_score:.1f}, Capped score: {capped_score:.1f}")
        
        debug_data['predictions_details'].append(pred_detail)
    
    max_possible_score = len(predictions) * 3
    reliability = (total_score / max_possible_score) * 100 if max_possible_score > 0 else 0
    
    debug_data.update({
        'total_score': total_score,
        'max_possible_score': max_possible_score,
        'reliability_percentage': reliability
    })
    
    print(f"ğŸ¯ [DEBUG] CALCULATION SUMMARY:")
    print(f"   - Total predictions: {len(predictions)}")
    print(f"   - Total score: {total_score}")
    print(f"   - Max possible score: {max_possible_score}")
    print(f"   - Reliability: {reliability:.1f}%")
    
    return jsonify(debug_data)

@app.route('/api/cache/check', methods=['POST'])
def check_cached_predictions():
    """ëˆ„ì  ì˜ˆì¸¡ ë²”ìœ„ì—ì„œ ìºì‹œëœ ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ í™•ì¸"""
    data = request.get_json()
    start_date = data.get('start_date')
    end_date = data.get('end_date')
    
    if not start_date or not end_date:
        return jsonify({'error': 'start_date and end_date are required'}), 400
    
    try:
        logger.info(f"ğŸ” [CACHE_CHECK] Checking cache availability for {start_date} to {end_date}")
        
        # ì €ì¥ëœ ì˜ˆì¸¡ í™•ì¸
        cached_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        # ì „ì²´ ë²”ìœ„ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼ ê¸°ì¤€)
        start_dt = pd.to_datetime(start_date)
        end_dt = pd.to_datetime(end_date)
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ê³„ì‚° (ë°ì´í„° ê¸°ì¤€ì¼)
        available_dates = []
        current_dt = start_dt
        while current_dt <= end_dt:
            # ì˜ì—…ì¼ë§Œ í¬í•¨ (ì£¼ë§ê³¼ íœ´ì¼ ì œì™¸)
            if current_dt.weekday() < 5 and not is_holiday(current_dt):
                available_dates.append(current_dt.strftime('%Y-%m-%d'))
            current_dt += pd.Timedelta(days=1)
        
        # ìºì‹œëœ ë‚ ì§œ ëª©ë¡
        cached_dates = [pred['date'] for pred in cached_predictions]
        missing_dates = [date for date in available_dates if date not in cached_dates]
        
        cache_percentage = round(len(cached_predictions) / max(len(available_dates), 1) * 100, 1)
        
        logger.info(f"ğŸ“Š [CACHE_CHECK] Cache status: {len(cached_predictions)}/{len(available_dates)} ({cache_percentage}%)")
        
        return jsonify({
            'success': True,
            'total_dates_in_range': len(available_dates),
            'cached_predictions': len(cached_predictions),
            'cached_dates': cached_dates,
            'missing_dates': missing_dates,
            'cache_percentage': cache_percentage,
            'will_use_cache': len(cached_predictions) > 0,
            'estimated_time_savings': f"ì•½ {len(cached_predictions) * 3}ë¶„ ì ˆì•½ ì˜ˆìƒ" if len(cached_predictions) > 0 else "ì—†ìŒ"
        })
        
    except Exception as e:
        logger.error(f"âŒ [CACHE_CHECK] Error checking cached predictions: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/results/accumulated/recent', methods=['GET'])
def get_recent_accumulated_results():
    """
    í˜ì´ì§€ ë¡œë“œ ì‹œ ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í•˜ëŠ” API
    """
    try:
        # ì €ì¥ëœ ì˜ˆì¸¡ ëª©ë¡ ì¡°íšŒ (ìµœê·¼ ê²ƒë¶€í„°)
        predictions_list = get_saved_predictions_list(limit=50)
        
        if not predictions_list:
            return jsonify({
                'success': False, 
                'message': 'No saved predictions found',
                'has_recent_results': False
            })
        
        # ë‚ ì§œë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì—°ì†ëœ ë²”ìœ„ ì°¾ê¸°
        dates_by_groups = {}
        for pred in predictions_list:
            data_end_date = pred.get('data_end_date', pred.get('prediction_date'))
            if data_end_date:
                date_obj = pd.to_datetime(data_end_date)
                # ì£¼ì°¨ë³„ë¡œ ê·¸ë£¹í™” (ê°™ì€ ì£¼ì˜ ì˜ˆì¸¡ë“¤ì„ í•˜ë‚˜ì˜ ë²”ìœ„ë¡œ ê°„ì£¼)
                week_key = date_obj.strftime('%Y-W%U')
                if week_key not in dates_by_groups:
                    dates_by_groups[week_key] = []
                dates_by_groups[week_key].append({
                    'date': data_end_date,
                    'date_obj': date_obj,
                    'pred_info': pred
                })
        
        # ê°€ì¥ ìµœê·¼ ê·¸ë£¹ ì„ íƒ
        if not dates_by_groups:
            return jsonify({
                'success': False, 
                'message': 'No valid date groups found',
                'has_recent_results': False
            })
        
        # ìµœê·¼ ì£¼ì˜ ì˜ˆì¸¡ë“¤ ê°€ì ¸ì˜¤ê¸°
        latest_week = max(dates_by_groups.keys())
        latest_group = dates_by_groups[latest_week]
        latest_group.sort(key=lambda x: x['date_obj'])
        
        # ì—°ì†ëœ ë‚ ì§œ ë²”ìœ„ ì°¾ê¸°
        start_date = latest_group[0]['date_obj']
        end_date = latest_group[-1]['date_obj']
        
        logger.info(f"ğŸ”„ [AUTO_RESTORE] Found recent accumulated predictions: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
        # ê¸°ì¡´ ìºì‹œì—ì„œ ëˆ„ì  ê²°ê³¼ ë¡œë“œ
        loaded_predictions = load_accumulated_predictions_from_csv(start_date, end_date)
        
        if not loaded_predictions:
            return jsonify({
                'success': False, 
                'message': 'Failed to load cached predictions',
                'has_recent_results': False
            })
        
        # ëˆ„ì  ë©”íŠ¸ë¦­ ê³„ì‚°
        accumulated_metrics = {
            'f1': 0.0,
            'accuracy': 0.0,
            'mape': 0.0,
            'weighted_score': 0.0,
            'total_predictions': 0
        }
        
        for pred in loaded_predictions:
            metrics = pred.get('metrics', {})
            if isinstance(metrics, dict):
                accumulated_metrics['f1'] += metrics.get('f1', 0.0)
                accumulated_metrics['accuracy'] += metrics.get('accuracy', 0.0)
                accumulated_metrics['mape'] += metrics.get('mape', 0.0)
                accumulated_metrics['weighted_score'] += metrics.get('weighted_score', 0.0)
                accumulated_metrics['total_predictions'] += 1
        
        if accumulated_metrics['total_predictions'] > 0:
            count = accumulated_metrics['total_predictions']
            accumulated_metrics['f1'] /= count
            accumulated_metrics['accuracy'] /= count
            accumulated_metrics['mape'] /= count
            accumulated_metrics['weighted_score'] /= count
            
            # ğŸ”§ NaN ê°’ ì²˜ë¦¬ ê°•í™”
            for metric_key in ['f1', 'accuracy', 'mape', 'weighted_score']:
                if pd.isna(accumulated_metrics[metric_key]) or np.isnan(accumulated_metrics[metric_key]) or np.isinf(accumulated_metrics[metric_key]):
                    logger.warning(f"âš ï¸ [CACHED_METRICS] NaN/Inf detected in {metric_key}, setting to 0.0")
                    accumulated_metrics[metric_key] = 0.0
        
        # êµ¬ê°„ ì ìˆ˜ ê³„ì‚°
        accumulated_interval_scores = {}
        for pred in loaded_predictions:
            interval_scores = pred.get('interval_scores', {})
            if isinstance(interval_scores, dict):
                for interval in interval_scores.values():
                    if not interval or not isinstance(interval, dict) or 'days' not in interval or interval['days'] is None:
                        continue
                    interval_key = f"{format_date(interval['start_date'])}-{format_date(interval['end_date'])}"
                    if interval_key in accumulated_interval_scores:
                        accumulated_interval_scores[interval_key]['score'] += interval['score']
                        accumulated_interval_scores[interval_key]['count'] += 1
                    else:
                        accumulated_interval_scores[interval_key] = interval.copy()
                        accumulated_interval_scores[interval_key]['count'] = 1
        
        # ì •ë ¬ëœ êµ¬ê°„ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        accumulated_scores_list = [
            v for v in accumulated_interval_scores.values()
            if v is not None and isinstance(v, dict) and 'days' in v and v['days'] is not None
        ]
        accumulated_scores_list.sort(key=lambda x: x['score'], reverse=True)
        
        # êµ¬ë§¤ ì‹ ë¢°ë„ ê³„ì‚°
        accumulated_purchase_reliability, _ = calculate_accumulated_purchase_reliability(loaded_predictions)
        
        # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        unique_periods = set()
        for pred in loaded_predictions:
            if 'next_semimonthly_period' in pred and pred['next_semimonthly_period']:
                unique_periods.add(pred['next_semimonthly_period'])
        
        accumulated_consistency_scores = {}
        for period in unique_periods:
            try:
                consistency_data = calculate_prediction_consistency(loaded_predictions, period)
                # ğŸ”§ NaN ê°’ ì²˜ë¦¬ ê°•í™”
                if consistency_data and 'consistency_score' in consistency_data:
                    consistency_score = consistency_data['consistency_score']
                    if pd.isna(consistency_score) or np.isnan(consistency_score) or np.isinf(consistency_score):
                        logger.warning(f"âš ï¸ [CACHED_CONSISTENCY] NaN/Inf detected for period {period}, setting to 0.0")
                        consistency_data['consistency_score'] = 0.0
                accumulated_consistency_scores[period] = consistency_data
            except Exception as e:
                logger.error(f"Error calculating consistency for period {period}: {str(e)}")
        
        # ìºì‹œ í†µê³„
        cache_statistics = {
            'total_dates': len(loaded_predictions),
            'cached_dates': len(loaded_predictions),
            'new_predictions': 0,
            'cache_hit_rate': 100.0
        }
        
        # ğŸ”§ NaN ê°’ ì²˜ë¦¬ ê°•í™”
        if pd.isna(cache_statistics['cache_hit_rate']) or np.isnan(cache_statistics['cache_hit_rate']) or np.isinf(cache_statistics['cache_hit_rate']):
            logger.warning(f"âš ï¸ [CACHED_CACHE_STATS] NaN/Inf detected in cache_hit_rate, setting to 0.0")
            cache_statistics['cache_hit_rate'] = 0.0
        
        # ì „ì—­ ìƒíƒœ ì—…ë°ì´íŠ¸ (ì„ íƒì )
        global prediction_state
        prediction_state['accumulated_predictions'] = loaded_predictions
        prediction_state['accumulated_metrics'] = accumulated_metrics
        prediction_state['prediction_dates'] = [p['date'] for p in loaded_predictions]
        prediction_state['accumulated_interval_scores'] = accumulated_scores_list
        prediction_state['accumulated_consistency_scores'] = accumulated_consistency_scores
        prediction_state['accumulated_purchase_reliability'] = accumulated_purchase_reliability
        prediction_state['cache_statistics'] = cache_statistics
        
        logger.info(f"âœ… [AUTO_RESTORE] Successfully restored {len(loaded_predictions)} accumulated predictions")
        
        return jsonify({
            'success': True,
            'has_recent_results': True,
            'restored_range': {
                'start_date': start_date.strftime('%Y-%m-%d'),
                'end_date': end_date.strftime('%Y-%m-%d'),
                'prediction_count': len(loaded_predictions)
            },
            'prediction_dates': [p['date'] for p in loaded_predictions],
            'accumulated_metrics': accumulated_metrics,
            'predictions': loaded_predictions,
            'accumulated_interval_scores': accumulated_scores_list,
            'accumulated_consistency_scores': accumulated_consistency_scores,
            'accumulated_purchase_reliability': accumulated_purchase_reliability,
            'cache_statistics': cache_statistics,
            'message': f"ìµœê·¼ ëˆ„ì  ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìë™ìœ¼ë¡œ ë³µì›í–ˆìŠµë‹ˆë‹¤ ({start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')})"
        })
        
    except Exception as e:
        logger.error(f"âŒ [AUTO_RESTORE] Error restoring recent accumulated results: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False, 
            'error': str(e),
            'has_recent_results': False
        }), 500

@app.route('/api/cache/rebuild-index', methods=['POST'])
def rebuild_predictions_index_api():
    """ì˜ˆì¸¡ ì¸ë±ìŠ¤ ì¬ìƒì„± API (rebuild_index.py ê¸°ëŠ¥ì„ í†µí•©)"""
    try:
        # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        current_file = prediction_state.get('current_file')
        if not current_file:
            return jsonify({'success': False, 'error': 'í˜„ì¬ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.'})
        
        # ğŸ”§ ìƒˆë¡œìš´ rebuild í•¨ìˆ˜ ì‚¬ìš©
        success = rebuild_predictions_index_from_existing_files()
        
        if success:
            cache_dirs = get_file_cache_dirs(current_file)
            index_file = cache_dirs['predictions'] / 'predictions_index.cs'
            
            # ê²°ê³¼ ë°ì´í„° ì½ê¸°
            index_data = []
            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    index_data = list(reader)
            
            return jsonify({
                'success': True,
                'message': f'ì¸ë±ìŠ¤ íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ì¬ìƒì„±í–ˆìŠµë‹ˆë‹¤. ({len(index_data)}ê°œ í•­ëª©)',
                'file_location': str(index_file),
                'entries_count': len(index_data),
                'rebuilt_entries': [{'date': row.get('prediction_start_date', ''), 'data_end': row.get('data_end_date', '')} for row in index_data]
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì¸ë±ìŠ¤ ì¬ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.'
            })
        
    except Exception as e:
        logger.error(f"âŒ Error rebuilding predictions index: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({'success': False, 'error': f'ì¸ë±ìŠ¤ ì¬ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'})

@app.route('/api/cache/clear/semimonthly', methods=['POST'])
def clear_semimonthly_cache():
    """íŠ¹ì • ë°˜ì›” ê¸°ê°„ì˜ ìºì‹œë§Œ ì‚­ì œí•˜ëŠ” API"""
    try:
        data = request.json
        target_date = data.get('date')
        
        if not target_date:
            return jsonify({'error': 'Date parameter is required'}), 400
        
        target_date = pd.to_datetime(target_date)
        target_semimonthly = get_semimonthly_period(target_date)
        
        logger.info(f"ğŸ—‘ï¸ [API] Clearing cache for semimonthly period: {target_semimonthly}")
        
        # í˜„ì¬ íŒŒì¼ì˜ ìºì‹œ ë””ë ‰í† ë¦¬ì—ì„œ í•´ë‹¹ ë°˜ì›” ìºì‹œ ì‚­ì œ
        cache_dirs = get_file_cache_dirs()
        predictions_dir = cache_dirs['predictions']
        
        deleted_files = []
        
        if predictions_dir.exists():
            # ë©”íƒ€ íŒŒì¼ í™•ì¸í•˜ì—¬ ë°˜ì›” ê¸°ê°„ì´ ì¼ì¹˜í•˜ëŠ” ìºì‹œ ì‚­ì œ
            for meta_file in predictions_dir.glob("*_meta.json"):
                try:
                    with open(meta_file, 'r', encoding='utf-8') as f:
                        meta_data = json.load(f)
                    
                    cached_data_end_date = meta_data.get('data_end_date')
                    if cached_data_end_date:
                        cached_data_end_date = pd.to_datetime(cached_data_end_date)
                        cached_semimonthly = get_semimonthly_period(cached_data_end_date)
                        
                        if cached_semimonthly == target_semimonthly:
                            # ê´€ë ¨ íŒŒì¼ë“¤ ì‚­ì œ (ë³´ì•ˆì„ ìœ„í•´ .cs í™•ì¥ì ì‚¬ìš©)
                            base_name = meta_file.stem.replace('_meta', '')
                            files_to_delete = [
                                meta_file,
                                meta_file.parent / f"{base_name}.cs",
                                meta_file.parent / f"{base_name}_attention.json",
                                meta_file.parent / f"{base_name}_ma.json"
                            ]
                            
                            for file_path in files_to_delete:
                                if file_path.exists():
                                    file_path.unlink()
                                    deleted_files.append(str(file_path.name))
                                    logger.info(f"  ğŸ—‘ï¸ Deleted: {file_path.name}")
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Error processing meta file {meta_file}: {str(e)}")
                    continue
        
        return jsonify({
            'success': True,
            'message': f'Cache cleared for semimonthly period: {target_semimonthly}',
            'target_semimonthly': target_semimonthly,
            'target_date': target_date.strftime('%Y-%m-%d'),
            'deleted_files': deleted_files,
            'deleted_count': len(deleted_files)
        })
        
    except Exception as e:
        logger.error(f"âŒ [API] Error clearing semimonthly cache: {str(e)}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

#######################################################################
# VARMAX ì˜ˆì¸¡ ì €ì¥/ë¡œë“œ ì‹œìŠ¤í…œ
#######################################################################

def save_varmax_prediction(prediction_results: dict, prediction_date):
    """
    VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction save")
            return False
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        # ì €ì¥í•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        # JSONìœ¼ë¡œ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        clean_results = {}
        for key, value in prediction_results.items():
            try:
                clean_results[key] = safe_serialize_value(value)
            except Exception as e:
                logger.warning(f"Failed to serialize {key}: {e}")
                continue
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        clean_results['metadata'] = {
            'prediction_date': prediction_date,
            'created_at': datetime.now().isoformat(),
            'file_path': file_path,
            'model_type': 'VARMAX'
        }
        
        # íŒŒì¼ì— ì €ì¥
        with open(prediction_file, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, ensure_ascii=False, indent=2)
        
        # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_varmax_predictions_index({
            'prediction_date': prediction_date,
            'file_path': str(prediction_file),
            'created_at': datetime.now().isoformat(),
            'original_file': file_path
        })
        
        logger.info(f"âœ… VARMAX prediction saved: {prediction_file}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to save VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return False

def load_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX prediction load")
            return None
            
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ë¡œë“œí•  íŒŒì¼ ê²½ë¡œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        
        if not prediction_file.exists():
            logger.info(f"VARMAX prediction file not found: {prediction_file}")
            return None
            
        # íŒŒì¼ì—ì„œ ë¡œë“œ
        with open(prediction_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
        
        # ğŸ” ë¡œë“œëœ ë°ì´í„° íƒ€ì… ë° êµ¬ì¡° í™•ì¸
        logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data type: {type(results)}")
        if isinstance(results, dict):
            logger.info(f"ğŸ” [VARMAX_LOAD] Loaded data keys: {list(results.keys())}")
            
            # ğŸ”§ ma_results í•„ë“œ íƒ€ì… í™•ì¸ ë° ìˆ˜ì •
            if 'ma_results' in results:
                ma_results = results['ma_results']
                logger.info(f"ğŸ” [VARMAX_LOAD] MA results type: {type(ma_results)}")
                
                if isinstance(ma_results, str):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results is string, attempting to parse as JSON...")
                    try:
                        results['ma_results'] = json.loads(ma_results)
                        logger.info(f"ğŸ”§ [VARMAX_LOAD] Successfully parsed ma_results from string to dict")
                    except Exception as e:
                        logger.error(f"âŒ [VARMAX_LOAD] Failed to parse ma_results string as JSON: {e}")
                        results['ma_results'] = {}
                elif not isinstance(ma_results, dict):
                    logger.warning(f"âš ï¸ [VARMAX_LOAD] MA results has unexpected type: {type(ma_results)}, setting empty dict")
                    results['ma_results'] = {}
                    
        elif isinstance(results, str):
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Loaded data is string, not dict: {results[:100]}...")
            # ë¬¸ìì—´ì¸ ê²½ìš° ë‹¤ì‹œ JSON íŒŒì‹± ì‹œë„
            try:
                results = json.loads(results)
                logger.info(f"ğŸ”§ [VARMAX_LOAD] Re-parsed string as JSON: {type(results)}")
            except:
                logger.error(f"âŒ [VARMAX_LOAD] Failed to re-parse string as JSON")
                return None
        else:
            logger.warning(f"âš ï¸ [VARMAX_LOAD] Unexpected data type: {type(results)}")
        
        logger.info(f"âœ… VARMAX prediction loaded: {prediction_file}")
        return results
        
    except Exception as e:
        logger.error(f"âŒ Failed to load VARMAX prediction: {e}")
        logger.error(traceback.format_exc())
        return None

def update_varmax_predictions_index(metadata):
    """
    VARMAX ì˜ˆì¸¡ ì¸ë±ìŠ¤ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        varmax_dir.mkdir(exist_ok=True)
        
        index_file = varmax_dir / 'varmax_index.json'
        
        # ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
        else:
            index = {'predictions': []}
        
        # ìƒˆ ì˜ˆì¸¡ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        prediction_date = metadata['prediction_date']
        index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
        index['predictions'].append(metadata)
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        index['predictions'].sort(key=lambda x: x['prediction_date'], reverse=True)
        
        # ìµœëŒ€ 100ê°œ ìœ ì§€
        index['predictions'] = index['predictions'][:100]
        
        # ì¸ë±ìŠ¤ ì €ì¥
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to update VARMAX predictions index: {e}")
        return False

def get_saved_varmax_predictions_list(limit=100):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            logger.warning("No current file path for VARMAX predictions list")
            return []
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        index_file = varmax_dir / 'varmax_index.json'
        
        if not index_file.exists():
            return []
            
        with open(index_file, 'r', encoding='utf-8') as f:
            index = json.load(f)
        
        predictions = index.get('predictions', [])[:limit]
        
        logger.info(f"âœ… Found {len(predictions)} saved VARMAX predictions")
        return predictions
        
    except Exception as e:
        logger.error(f"âŒ Failed to get saved VARMAX predictions list: {e}")
        return []

def delete_saved_varmax_prediction(prediction_date):
    """
    ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_path = prediction_state.get('current_file', None)
        if not file_path:
            return False
            
        cache_dirs = get_file_cache_dirs(file_path)
        varmax_dir = cache_dirs['root'] / 'varmax'
        
        # ì˜ˆì¸¡ íŒŒì¼ ì‚­ì œ
        prediction_file = varmax_dir / f"varmax_prediction_{prediction_date}.json"
        if prediction_file.exists():
            prediction_file.unlink()
        
        # ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        index_file = varmax_dir / 'varmax_index.json'
        if index_file.exists():
            with open(index_file, 'r', encoding='utf-8') as f:
                index = json.load(f)
            
            index['predictions'] = [p for p in index['predictions'] if p['prediction_date'] != prediction_date]
            
            with open(index_file, 'w', encoding='utf-8') as f:
                json.dump(index, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… VARMAX prediction deleted: {prediction_date}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Failed to delete VARMAX prediction: {e}")
        return False

#######################################################################
# VARMAX ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#######################################################################

def varmax_decision(file_path):
    """Varmax ì˜ì‚¬ê²°ì • ê´€ë ¨"""
    fp = pd.read_csv(file_path)
    df = pd.DataFrame(fp, columns=fp.columns)
    col = df.columns
    # 1) ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    vars_pct = ['max_pct2', 'min_pct2', 'mean_pct2', 'max_pct3', 'min_pct3', 'mean_pct3']
    logger.info(f'ë°ì´í„°í”„ë ˆì„{df}')
    rename_dict = {
    'max_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct2': '[í˜„ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]',
    'max_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct3': '[ì´ì „ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]'
    }
    rename_col = list(rename_dict.values())
    df = df.rename(columns=rename_dict)
    logger.info(f'ì—´{col}')
    # 2) Case ì •ì˜
    case1 = df['saving_rate'] < 0
    abs_thresh = df['saving_rate'].abs().quantile(0.9)
    case2 = df['saving_rate'].abs() >= abs_thresh

    # 3) ìµœì  ì¡°ê±´ íƒìƒ‰ í•¨ìˆ˜
    def find_best_condition(df, case_mask, var):
        best = None
        for direction in ['greater', 'less']:
            for p in np.linspace(0.1, 0.9, 9):
                th = df[var].quantile(p)
                if direction == 'greater':
                    mask = df[var] > th
                else:
                    mask = df[var] < th
                # ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ì€ ê²½ìš° ì œì™¸
                if mask.sum() < 5:
                    continue
                prop = case_mask[mask].mean()
                if best is None or prop > best[4]:
                    best = (direction, p, th, mask.sum(), prop)
        return best

    # 5) ê° ë³€ìˆ˜ë³„ ìµœì  ì¡°ê±´ ì°¾ê¸°
    results_case1 = {var: find_best_condition(df, case1, var) for var in rename_col}
    results_case2 = {var: find_best_condition(df, case2, var) for var in rename_col}

    from itertools import combinations
    # 6) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ saving_rate < 0 ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ (ìƒ˜í”Œ ìˆ˜ â‰¥ 30)
    combi_results_case1 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case1[mask].mean()
                        combi_results_case1.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìŒìˆ˜ ë¹„ìœ¨ [%]": round(rate*100, 3)
                        })
    column_order1 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìŒìˆ˜ ë¹„ìœ¨ [%]"]
    combi_df_case1 = pd.DataFrame(combi_results_case1).sort_values(by="ìŒìˆ˜ ë¹„ìœ¨ [%]", ascending=False)
    combi_df_case1 = combi_df_case1.reindex(columns=column_order1)

    # 7) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ì ˆëŒ“ê°’ ìƒìœ„ 10% ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€
    combi_results_case2 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case2[mask].mean()
                        combi_results_case2.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]": round(rate*100, 3)
                        })
    column_order2 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]"]
    combi_df_case2 = pd.DataFrame(combi_results_case2).sort_values(by="ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]", ascending=False)
    combi_df_case2 = combi_df_case2.reindex(columns=column_order2)
    return {
        'case_1': combi_df_case1.to_dict(orient='records'),
        'case_2': combi_df_case2.to_dict(orient='records')
    }

def background_varmax_prediction(file_path, current_date, pred_days, use_cache=True):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ VARMAX ì˜ˆì¸¡ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜"""
    global prediction_state
    
    try:
        from app.utils.file_utils import set_seed
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        # í˜„ì¬ íŒŒì¼ ìƒíƒœ ì—…ë°ì´íŠ¸
        prediction_state['current_file'] = file_path
        
        # ğŸ” ê¸°ì¡´ ì €ì¥ëœ ì˜ˆì¸¡ í™•ì¸ (use_cache=Trueì¸ ê²½ìš°)
        if use_cache:
            logger.info(f"ğŸ” [VARMAX_CACHE] Checking for existing prediction for date: {current_date}")
            existing_prediction = load_varmax_prediction(current_date)
            
            if existing_prediction:
                logger.info(f"âœ… [VARMAX_CACHE] Found existing VARMAX prediction for {current_date}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Cached data keys: {list(existing_prediction.keys())}")
                logger.info(f"ğŸ” [VARMAX_CACHE] MA results available: {bool(existing_prediction.get('ma_results'))}")
                ma_results = existing_prediction.get('ma_results')
                if ma_results:
                    logger.info(f"ğŸ” [VARMAX_CACHE] MA results type: {type(ma_results)}")
                    if isinstance(ma_results, dict):
                        logger.info(f"ğŸ” [VARMAX_CACHE] MA results keys: {list(ma_results.keys())}")
                    else:
                        logger.warning(f"âš ï¸ [VARMAX_CACHE] MA results is not a dict: {type(ma_results)}")
                
                # ğŸ”‘ ìƒíƒœ ë³µì› (ìˆœì°¨ì ìœ¼ë¡œ)
                logger.info(f"ğŸ”„ [VARMAX_CACHE] Restoring state from cached prediction...")
                
                # ê¸°ì¡´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒíƒœì— ë¡œë“œ (ì•ˆì „í•œ íƒ€ì… ê²€ì‚¬)
                prediction_state['varmax_predictions'] = existing_prediction.get('predictions', [])
                prediction_state['varmax_half_month_averages'] = existing_prediction.get('half_month_averages', [])
                prediction_state['varmax_metrics'] = existing_prediction.get('metrics', {})
                
                # MA results ì•ˆì „í•œ ë¡œë“œ
                ma_results = existing_prediction.get('ma_results', {})
                if isinstance(ma_results, dict):
                    prediction_state['varmax_ma_results'] = ma_results
                else:
                    logger.warning(f"âš ï¸ [VARMAX_CACHE] Invalid ma_results type: {type(ma_results)}, setting empty dict")
                    prediction_state['varmax_ma_results'] = {}
                
                prediction_state['varmax_selected_features'] = existing_prediction.get('selected_features', [])
                prediction_state['varmax_current_date'] = existing_prediction.get('current_date', current_date)
                prediction_state['varmax_model_info'] = existing_prediction.get('model_info', {})
                prediction_state['varmax_plots'] = existing_prediction.get('plots', {})
                
                # ì¦‰ì‹œ ì™„ë£Œ ìƒíƒœë¡œ ì„¤ì •
                prediction_state['varmax_is_predicting'] = False
                prediction_state['varmax_prediction_progress'] = 100
                prediction_state['varmax_error'] = None
                
                logger.info(f"âœ… [VARMAX_CACHE] State restoration completed")
                
                logger.info(f"âœ… [VARMAX_CACHE] Successfully loaded existing prediction for {current_date}")
                logger.info(f"ğŸ” [VARMAX_CACHE] State restored - predictions: {len(prediction_state['varmax_predictions'])}, MA results: {len(prediction_state['varmax_ma_results'])}")
                
                # ğŸ” ìµœì¢… ê²€ì¦
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - is_predicting: {prediction_state.get('varmax_is_predicting')}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - predictions count: {len(prediction_state.get('varmax_predictions', []))}")
                logger.info(f"ğŸ” [VARMAX_CACHE] Final verification - ma_results count: {len(prediction_state.get('varmax_ma_results', {}))}")
                
                # ğŸ›¡ï¸ ìƒíƒœ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
                time.sleep(1.0)
                
                logger.info(f"ğŸ¯ [VARMAX_CACHE] Cache loading process completed for {current_date}")
                return
        
        # ğŸš€ ìƒˆë¡œìš´ ì˜ˆì¸¡ ìˆ˜í–‰
        logger.info(f"ğŸš€ [VARMAX_NEW] Starting new VARMAX prediction for {current_date}")
        forecaster = VARMAXSemiMonthlyForecaster(file_path, pred_days=pred_days)
        prediction_state['varmax_is_predicting'] = True
        prediction_state['varmax_prediction_progress'] = 10
        prediction_state['varmax_prediction_start_time'] = time.time()  # VARMAX ì‹œì‘ ì‹œê°„ ê¸°ë¡
        prediction_state['varmax_error'] = None
        
        # VARMAX ì˜ˆì¸¡ ìˆ˜í–‰
        prediction_state['varmax_prediction_progress'] = 30
        logger.info(f"ğŸ”„ [VARMAX_NEW] Starting prediction generation (30% complete)")
        
        try:
            min_index = 1 # ì„ì‹œ ì¸ë±ìŠ¤
            logger.info(f"ğŸ”„ [VARMAX_NEW] Calling generate_predictions_varmax with current_date={current_date}, var_num={min_index+2}")
            
            # ì˜ˆì¸¡ ì§„í–‰ë¥ ì„ 30%ë¡œ ì„¤ì • (ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ)
            prediction_state['varmax_prediction_progress'] = 30

            mape_list=[]
            valid_indices = []
            for var_num in range(2,8):
                mape_value = forecaster.generate_variables_varmax(current_date, var_num)
                mape_list.append(mape_value)
                if mape_value is not None:
                    valid_indices.append(var_num - 2)  # ì¸ë±ìŠ¤ ì¡°ì •
                logger.info(f"Var {var_num} model MAPE: {mape_value}")
            
            # None ê°’ í•„í„°ë§
            valid_mape_values = [mape for mape in mape_list if mape is not None]
            
            if not valid_mape_values:
                raise Exception("All VARMAX variable models failed to generate valid MAPE values")
            
            # ìµœì†Œ MAPE ê°’ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
            min_mape = min(valid_mape_values)
            min_index = None
            for i, mape in enumerate(mape_list):
                if mape == min_mape:
                    min_index = i
                    break
            
            logger.info(f"Var {min_index+2} model is selected, MAPE:{mape_list[min_index]}%")
            logger.info(f"Valid models: {len(valid_mape_values)}/{len(mape_list)}")
            
            results = forecaster.generate_predictions_varmax(current_date, min_index+2)
            logger.info(f"âœ… [VARMAX_NEW] Prediction generation completed successfully")
            
            # ìµœì¢… ì§„í–‰ë¥  95%ë¡œ ì„¤ì • (ì‹œê°í™” ìƒì„± ì „)
            prediction_state['varmax_prediction_progress'] = 95
            
        except Exception as prediction_error:
            logger.error(f"âŒ [VARMAX_NEW] Error during prediction generation: {str(prediction_error)}")
            logger.error(f"âŒ [VARMAX_NEW] Prediction error traceback: {traceback.format_exc()}")
            
            # ì˜ˆì¸¡ ì‹¤íŒ¨ ìƒíƒœë¡œ ì„¤ì •
            prediction_state['varmax_error'] = f"Prediction generation failed: {str(prediction_error)}"
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            logger.error(f"âŒ [VARMAX_NEW] Prediction state reset due to error")
            return
        
        if results['success']:
            logger.info(f"ğŸ”„ [VARMAX_NEW] Updating state with new prediction results...")
            
            # ìƒíƒœì— ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ LSTM ê²°ê³¼ì™€ ë¶„ë¦¬)
            prediction_state['varmax_predictions'] = results['predictions']
            prediction_state['varmax_half_month_averages'] = results.get('half_month_averages', [])
            prediction_state['varmax_metrics'] = results['metrics']
            prediction_state['varmax_ma_results'] = results['ma_results']
            prediction_state['varmax_selected_features'] = results['selected_features']
            prediction_state['varmax_current_date'] = results['current_date']
            prediction_state['varmax_model_info'] = results['model_info']
            
            # ì‹œê°í™” ìƒì„± (ê¸°ì¡´ app.py ë°©ì‹ í™œìš©)
            plots_info = create_varmax_visualizations(results)
            prediction_state['varmax_plots'] = plots_info
            
            prediction_state['varmax_prediction_progress'] = 100
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_error'] = None
            
            # VARMAX ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
            save_varmax_prediction(results, current_date)
            
            logger.info("âœ… [VARMAX_NEW] Prediction completed successfully")
            logger.info(f"ğŸ” [VARMAX_NEW] Final state - predictions: {len(prediction_state['varmax_predictions'])}")
        else:
            prediction_state['varmax_error'] = results['error']
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            
    except Exception as e:
        logger.error(f"âŒ [VARMAX_BG] Error in background VARMAX prediction: {str(e)}")
        logger.error(f"âŒ [VARMAX_BG] Full traceback: {traceback.format_exc()}")
        
        # ì—ëŸ¬ ìƒíƒœë¡œ ì„¤ì •í•˜ê³  ìì„¸í•œ ë¡œê¹…
        prediction_state['varmax_error'] = f"Background prediction failed: {str(e)}"
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 0
        
        logger.error(f"âŒ [VARMAX_BG] VARMAX prediction failed completely. Current state reset.")
        logger.error(f"âŒ [VARMAX_BG] Error type: {type(e).__name__}")
        logger.error(f"âŒ [VARMAX_BG] Error details: {str(e)}")
        
        # ì—ëŸ¬ ë°œìƒ ì‹œ ëª¨ë“  VARMAX ê´€ë ¨ ìƒíƒœ ì´ˆê¸°í™”
        prediction_state['varmax_predictions'] = []
        prediction_state['varmax_metrics'] = {}
        prediction_state['varmax_ma_results'] = {}
        prediction_state['varmax_selected_features'] = []
        prediction_state['varmax_current_date'] = None
        prediction_state['varmax_model_info'] = {}
        prediction_state['varmax_plots'] = {}
        prediction_state['varmax_half_month_averages'] = []

def plot_varmax_prediction_basic(sequence_df, sequence_start_date, start_day_value, 
                                f1, accuracy, mape, weighted_score, 
                                save_prefix=None, title_prefix="VARMAX Semi-monthly Prediction", file_path=None):
    """VARMAX ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„ ì‹œê°í™” (ê¸°ì¡´ plot_prediction_basicê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼)"""
    try:
        import matplotlib.pyplot as plt
        from matplotlib.gridspec import GridSpec
        import matplotlib.dates as mdates
        logger.info(f"Creating VARMAX prediction graph for {sequence_start_date}")
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if save_prefix is None:
            cache_dirs = get_file_cache_dirs(file_path)
            save_prefix = cache_dirs['plots']
        
        # ì˜ˆì¸¡ê°’ë§Œ ìˆëŠ” ë°ì´í„° ì²˜ë¦¬
        pred_df = sequence_df.dropna(subset=['Prediction'])
        valid_df = sequence_df.dropna(subset=['Actual']) if 'Actual' in sequence_df.columns else pd.DataFrame()
        
        # ê·¸ë˜í”„ ìƒì„±
        fig = plt.figure(figsize=(16, 9))
        plt.subplots_adjust(top=0.85)
        gs = GridSpec(2, 1, height_ratios=[3, 1], hspace=0.3, figure=fig)
        
        # ì œëª© ì„¤ì •
        main_title = f"{title_prefix} - {sequence_start_date}"
        subtitle = f"F1: {f1:.2f}, Acc: {accuracy:.2f}%, MAPE: {mape:.2f}%, Score: {weighted_score:.2f}%"
        
        fig.text(0.5, 0.94, main_title, ha='center', fontsize=14, weight='bold')
        fig.text(0.5, 0.90, subtitle, ha='center', fontsize=12)
        
        # ìƒë‹¨: ì˜ˆì¸¡ vs ì‹¤ì œ (ìˆëŠ” ê²½ìš°)
        ax1 = fig.add_subplot(gs[0])
        ax1.set_title("VARMAX Long-term Prediction")
        ax1.grid(True, linestyle='--', alpha=0.5)
        
        # ì˜ˆì¸¡ê°’ í”Œë¡¯
        ax1.plot(pred_df['Date'], pred_df['Prediction'],
                marker='o', color='red', label='VARMAX Predicted', linewidth=2)
        
        # ì‹¤ì œê°’ í”Œë¡¯ (ìˆëŠ” ê²½ìš°)
        if len(valid_df) > 0:
            ax1.plot(valid_df['Date'], valid_df['Actual'],
                    marker='o', color='blue', label='Actual', linewidth=2)
            
            # ë°©í–¥ì„± ì¼ì¹˜ ì—¬ë¶€ ë°°ê²½ ìƒ‰ì¹ 
            for i in range(1, len(valid_df)):
                if i < len(pred_df):
                    actual_dir = np.sign(valid_df['Actual'].iloc[i] - valid_df['Actual'].iloc[i-1])
                    pred_dir = np.sign(pred_df['Prediction'].iloc[i] - pred_df['Prediction'].iloc[i-1])
                    color = 'blue' if actual_dir == pred_dir else 'red'
                    ax1.axvspan(valid_df['Date'].iloc[i-1], valid_df['Date'].iloc[i], alpha=0.1, color=color)
        
        ax1.set_xlabel("Date")
        ax1.set_ylabel("MOPJ Price")
        ax1.legend()
        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # í•˜ë‹¨: ì˜¤ì°¨ (ì‹¤ì œê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ)
        ax2 = fig.add_subplot(gs[1])
        ax2.grid(True, linestyle='--', alpha=0.5)
        
        if len(valid_df) > 0:
            # ì˜¤ì°¨ ê³„ì‚° ë° í”Œë¡¯
            errors = valid_df['Actual'] - valid_df['Prediction']
            ax2.bar(valid_df['Date'], errors, alpha=0.7, color='orange', width=0.8)
            ax2.axhline(y=0, color='black', linestyle='-', alpha=0.8)
            ax2.set_title(f"Prediction Error (MAE: {abs(errors).mean():.2f})")
        else:
            ax2.text(0.5, 0.5, 'No actual data for error calculation', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
            ax2.set_title("Prediction Error (No validation data)")
        
        ax2.set_xlabel("Date")
        ax2.set_ylabel("Error")
        ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
        
        # íŒŒì¼ ì €ì¥
        os.makedirs(save_prefix, exist_ok=True)
        filename = f"varmax_prediction_{sequence_start_date}.png"
        filepath = os.path.join(save_prefix, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"VARMAX prediction graph saved: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"Error creating VARMAX prediction graph: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def create_varmax_visualizations(results):
    """VARMAX ê²°ê³¼ì— ëŒ€í•œ ì‹œê°í™” ìƒì„±"""
    try:
        # ê¸°ë³¸ ì˜ˆì¸¡ ê·¸ë˜í”„
        sequence_df = pd.DataFrame(results['predictions'])
        sequence_df['Date'] = pd.to_datetime(sequence_df['Date'])
        
        metrics = results['metrics']
        current_date = results['current_date']
        start_day_value = sequence_df['Prediction'].iloc[0] if len(sequence_df) > 0 else 0
        
        # ê¸°ë³¸ ê·¸ë˜í”„
        basic_plot = plot_varmax_prediction_basic(
            sequence_df, current_date, start_day_value,
            metrics['f1'], metrics['accuracy'], metrics['mape'], metrics['weighted_score']
        )
        
        # ì´ë™í‰ê·  ê·¸ë˜í”„
        ma_plot = plot_varmax_moving_average_analysis(
            results['ma_results'], current_date
        )
        
        plots_info = {
            'basic_plot': basic_plot,
            'ma_plot': ma_plot
        }
        
        logger.info("VARMAX visualizations created successfully")
        return plots_info
        
    except Exception as e:
        logger.error(f"Error creating VARMAX visualizations: {str(e)}")
        logger.error(traceback.format_exc())
        return {}

def plot_varmax_moving_average_analysis(ma_results, sequence_start_date, save_prefix=None,
                                        title_prefix="VARMAX Moving Average Analysis", file_path=None):
    """VARMAX ì´ë™í‰ê·  ë¶„ì„ ê·¸ë˜í”„"""
    try:
        import matplotlib.pyplot as plt
        import matplotlib.dates as mdates
        logger.info(f"Creating VARMAX moving average analysis for {sequence_start_date}")
        
        # íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if save_prefix is None:
            cache_dirs = get_file_cache_dirs(file_path)
            save_prefix = cache_dirs['ma_plots']
        
        if not ma_results:
            logger.warning("No moving average results to plot")
            return None
        
        windows = list(ma_results.keys())
        n_windows = len(windows)
        
        if n_windows == 0:
            logger.warning("No moving average windows found")
            return None
        
        # ê·¸ë˜í”„ ìƒì„± (2x2 ê·¸ë¦¬ë“œë¡œ ìµœëŒ€ 4ê°œ ìœˆë„ìš° í‘œì‹œ)
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f"{title_prefix} - {sequence_start_date}", fontsize=16, weight='bold')
        axes = axes.flatten()
        
        for i, window in enumerate(windows[:4]):  # ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ
            ax = axes[i]
            ma_data = ma_results[window]
            
            if not ma_data:
                ax.text(0.5, 0.5, f'No data for {window}', ha='center', va='center', transform=ax.transAxes)
                ax.set_title(f"{window} (No Data)")
                continue
            
            # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
            df = pd.DataFrame(ma_data)
            df['date'] = pd.to_datetime(df['date'])
            df = df.sort_values('date')
            
            # ì˜ˆì¸¡ê°’ê³¼ ì´ë™í‰ê·  í”Œë¡¯
            ax.plot(df['date'], df['prediction'], marker='o', color='red', 
                   label='Prediction', linewidth=2, markersize=4)
            ax.plot(df['date'], df['ma'], marker='s', color='blue', 
                   label=f'MA-{window.replace("ma", "")}', linewidth=2, markersize=4)
            
            # ì‹¤ì œê°’ í”Œë¡¯ (ìˆëŠ” ê²½ìš°)
            actual_data = df.dropna(subset=['actual'])
            if len(actual_data) > 0:
                ax.plot(actual_data['date'], actual_data['actual'], 
                       marker='^', color='green', label='Actual', linewidth=2, markersize=4)
            
            ax.set_title(f"{window.upper()} Moving Average")
            ax.grid(True, linestyle='--', alpha=0.5)
            ax.legend()
            ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
            plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(n_windows, 4):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        
        # íŒŒì¼ ì €ì¥
        os.makedirs(save_prefix, exist_ok=True)
        filename = f"varmax_ma_analysis_{sequence_start_date}.png"
        filepath = os.path.join(save_prefix, filename)
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"VARMAX moving average analysis saved: {filepath}")
        return filepath
        
    except Exception as e:
        logger.error(f"Error creating VARMAX moving average analysis: {str(e)}")
        logger.error(traceback.format_exc())
        return None

#######################################################################
# VARMAX API ì—”ë“œí¬ì¸íŠ¸
#######################################################################

# 1) VARMAX ë°˜ì›”ë³„ ì˜ˆì¸¡ ì‹œì‘
@app.route('/api/varmax/predict', methods=['POST', 'OPTIONS'])
def varmax_semimonthly_predict():
    from app.prediction.background_tasks import background_varmax_prediction # âœ… í•¨ìˆ˜ ë‚´ë¶€ì— ì¶”ê°€
    """VARMAX ë°˜ì›”ë³„ ì˜ˆì¸¡ ì‹œì‘ API"""
    # 1) ë¨¼ì €, OPTIONS(preflight) ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ 200ì„ ë¦¬í„´
    if request.method == 'OPTIONS':
        # CORS(app) ë¡œ ì„¤ì •í•´ë’€ìœ¼ë©´ ì´ë¯¸ Access-Control-Allow-Origin ë“±ì´ ë¶™ì–´ ìˆì„ ê²ƒì…ë‹ˆë‹¤.
        return make_response(('', 200))
    global prediction_state
    
    # ğŸ”§ VARMAX ë…ë¦½ ìƒíƒœ í™•ì¸ - hangëœ ìƒíƒœë©´ ìë™ ë¦¬ì…‹
    if prediction_state.get('varmax_is_predicting', False):
        current_progress = prediction_state.get('varmax_prediction_progress', 0)
        current_error = prediction_state.get('varmax_error')
        
        logger.warning(f"âš ï¸ [VARMAX_API] Prediction already in progress (progress: {current_progress}%, error: {current_error})")
        
        # ğŸ”§ ê°œì„ ëœ ìë™ ë¦¬ì…‹ ì¡°ê±´: ì—ëŸ¬ê°€ ìˆê±°ë‚˜ ì§„í–‰ë¥ ì´ ë§¤ìš° ë‚®ì€ ê²½ìš°ë§Œ ë¦¬ì…‹
        should_reset = False
        reset_reason = ""
        
        if current_error:
            should_reset = True
            reset_reason = f"error detected: {current_error}"
        elif current_progress > 0 and current_progress < 15:
            should_reset = True  
            reset_reason = f"very low progress stuck: {current_progress}%"
        
        if should_reset:
            logger.warning(f"ğŸ”„ [VARMAX_API] Auto-resetting stuck prediction - {reset_reason}")
            
            # ìƒíƒœ ë¦¬ì…‹
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            prediction_state['varmax_error'] = None
            prediction_state['varmax_predictions'] = []
            prediction_state['varmax_half_month_averages'] = []
            prediction_state['varmax_metrics'] = {}
            prediction_state['varmax_ma_results'] = {}
            prediction_state['varmax_selected_features'] = []
            prediction_state['varmax_current_date'] = None
            prediction_state['varmax_model_info'] = {}
            prediction_state['varmax_plots'] = {}
            
            logger.info(f"âœ… [VARMAX_API] Stuck state auto-reset completed, proceeding with new prediction")
        else:
            # ì •ìƒì ìœ¼ë¡œ ì§„í–‰ ì¤‘ì¸ ê²½ìš° 409 ë°˜í™˜
            return jsonify({
                'success': False,
                'error': 'VARMAX prediction already in progress',
                'progress': current_progress
            }), 409
    
    data = request.get_json(force=True)
    filepath     = data.get('filepath')
    current_date = data.get('date')
    pred_days    = data.get('pred_days', 50)
    use_cache    = data.get('use_cache', True)  # ğŸ†• ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    if not filepath or not os.path.exists(filepath):
        return jsonify({'error': 'Invalid file path'}), 400
    if not current_date:
        return jsonify({'error': 'Date is required'}), 400
    
    logger.info(f"ğŸš€ [VARMAX_API] Starting VARMAX prediction (use_cache={use_cache}) for {current_date}")
    
    thread = Thread(
        target=background_varmax_prediction,
        args=(filepath, current_date, pred_days, use_cache)
    )
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'VARMAX semi-monthly prediction started',
        'status_url': '/api/varmax/status',
        'use_cache': use_cache
    })

# 2) VARMAX ì˜ˆì¸¡ ìƒíƒœ ì¡°íšŒ
@app.route('/api/varmax/status', methods=['GET'])
def varmax_prediction_status():
    from app.prediction.background_tasks import calculate_estimated_time_remaining # âœ… í•¨ìˆ˜ ë‚´ë¶€ì— ì¶”ê°€
    """VARMAX ì˜ˆì¸¡ ìƒíƒœ í™•ì¸ API (ë‚¨ì€ ì‹œê°„ ì¶”ê°€)"""
    global prediction_state
    
    is_predicting = prediction_state.get('varmax_is_predicting', False)
    progress = prediction_state.get('varmax_prediction_progress', 0)
    error = prediction_state.get('varmax_error', None)
    
    logger.info(f"ğŸ” [VARMAX_STATUS] Current status - predicting: {is_predicting}, progress: {progress}%, error: {error}")
    
    status = {
        'is_predicting': is_predicting,
        'progress': progress,
        'error': error
    }
    
    # VARMAX ì˜ˆì¸¡ ì¤‘ì¸ ê²½ìš° ë‚¨ì€ ì‹œê°„ ê³„ì‚°
    if is_predicting and prediction_state.get('varmax_prediction_start_time'):
        time_info = calculate_estimated_time_remaining(
            prediction_state['varmax_prediction_start_time'], 
            progress
        )
        status.update(time_info)
    
    if not is_predicting and prediction_state.get('varmax_current_date'):
        status['current_date'] = prediction_state['varmax_current_date']
        logger.info(f"ğŸ” [VARMAX_STATUS] Prediction completed for date: {status['current_date']}")
    
    return jsonify(status)

# 3) VARMAX ì „ì²´ ê²°ê³¼ ì¡°íšŒ
@app.route('/api/varmax/results', methods=['GET'])
def get_varmax_results():
    """VARMAX ì˜ˆì¸¡ ê²°ê³¼ ì¡°íšŒ API"""
    global prediction_state
    
    # ğŸ” ìƒíƒœ ë””ë²„ê¹…
    logger.info(f"ğŸ” [VARMAX_API] Current prediction_state keys: {list(prediction_state.keys())}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_is_predicting: {prediction_state.get('varmax_is_predicting', 'NOT_SET')}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_predictions available: {bool(prediction_state.get('varmax_predictions'))}")
    logger.info(f"ğŸ” [VARMAX_API] varmax_ma_results available: {bool(prediction_state.get('varmax_ma_results'))}")
    
    if prediction_state.get('varmax_predictions'):
        logger.info(f"ğŸ” [VARMAX_API] Predictions count: {len(prediction_state['varmax_predictions'])}")
    
    if prediction_state.get('varmax_ma_results'):
        logger.info(f"ğŸ” [VARMAX_API] MA results keys: {list(prediction_state['varmax_ma_results'].keys())}")
    
    # ğŸ›¡ï¸ ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì™„ë£Œ ëŒ€ê¸°
    if prediction_state.get('varmax_is_predicting', False):
        logger.warning(f"âš ï¸ [VARMAX_API] Prediction still in progress: {prediction_state.get('varmax_prediction_progress', 0)}%")
        return jsonify({
            'success': False,
            'error': 'VARMAX prediction in progress',
            'progress': prediction_state.get('varmax_prediction_progress', 0)
        }), 409
    
    # ğŸ¯ ìƒíƒœì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ì§ì ‘ ë¡œë“œ (ì‹ ë¢°ì„± ê°œì„ )
    if not prediction_state.get('varmax_predictions'):
        logger.warning(f"âš ï¸ [VARMAX_API] No VARMAX predictions in state, attempting direct cache load")
        logger.info(f"ğŸ” [VARMAX_API] Current file: {prediction_state.get('current_file')}")
        
        try:
            # ìµœê·¼ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            saved_predictions = get_saved_varmax_predictions_list(limit=1)
            logger.info(f"ğŸ” [VARMAX_API] Found {len(saved_predictions)} saved predictions")
            
            if saved_predictions:
                latest_date = saved_predictions[0]['prediction_date']
                logger.info(f"ğŸ”§ [VARMAX_API] Loading latest prediction: {latest_date}")
                
                # ì§ì ‘ ë¡œë“œí•˜ê³  ìƒíƒœ ë³µì›
                cached_prediction = load_varmax_prediction(latest_date)
                if cached_prediction and cached_prediction.get('predictions'):
                    logger.info(f"âœ… [VARMAX_API] Successfully loaded from cache ({len(cached_prediction.get('predictions', []))} predictions)")
                    
                    # ğŸ”‘ ì¦‰ì‹œ ìƒíƒœ ë³µì› (ë” ì•ˆì „í•˜ê²Œ)
                    prediction_state['varmax_predictions'] = cached_prediction.get('predictions', [])
                    prediction_state['varmax_half_month_averages'] = cached_prediction.get('half_month_averages', [])
                    prediction_state['varmax_metrics'] = cached_prediction.get('metrics', {})
                    prediction_state['varmax_ma_results'] = cached_prediction.get('ma_results', {})
                    prediction_state['varmax_selected_features'] = cached_prediction.get('selected_features', [])
                    prediction_state['varmax_current_date'] = cached_prediction.get('current_date')
                    prediction_state['varmax_model_info'] = cached_prediction.get('model_info', {})
                    prediction_state['varmax_plots'] = cached_prediction.get('plots', {})
                    
                    logger.info(f"ğŸ¯ [VARMAX_API] State restored from cache - {len(prediction_state['varmax_predictions'])} predictions")
                    
                    return jsonify({
                        'success': True,
                        'current_date': cached_prediction.get('current_date'),
                        'predictions': cached_prediction.get('predictions', []),
                        'half_month_averages': cached_prediction.get('half_month_averages', []),
                        'metrics': cached_prediction.get('metrics', {}),
                        'ma_results': cached_prediction.get('ma_results', {}),
                        'selected_features': cached_prediction.get('selected_features', []),
                        'model_info': cached_prediction.get('model_info', {}),
                        'plots': cached_prediction.get('plots', {})
                    })
                else:
                    logger.warning(f"âš ï¸ [VARMAX_API] Cached prediction is empty or invalid")
            else:
                logger.warning(f"âš ï¸ [VARMAX_API] No saved predictions found")
                
        except Exception as e:
            logger.error(f"âŒ [VARMAX_API] Direct cache load failed: {e}")
            import traceback
            logger.error(f"âŒ [VARMAX_API] Cache load traceback: {traceback.format_exc()}")
        
        # ìºì‹œ ë¡œë“œë„ ì‹¤íŒ¨í•œ ê²½ìš° ëª…í™•í•œ ë©”ì‹œì§€
        logger.error(f"âŒ [VARMAX_API] No VARMAX results available in state or cache")
        return jsonify({
            'success': False,
            'error': 'No VARMAX prediction results available. Please run a new prediction.'
        }), 404
    
    logger.info(f"âœ… [VARMAX_API] Returning VARMAX results successfully from state")
    return jsonify({
        'success': True,
        'current_date':      prediction_state.get('varmax_current_date'),
        'predictions':       prediction_state.get('varmax_predictions', []),
        'half_month_averages': prediction_state.get('varmax_half_month_averages', []),
        'metrics':           prediction_state.get('varmax_metrics', {}),
        'ma_results':        prediction_state.get('varmax_ma_results', {}),
        'selected_features': prediction_state.get('varmax_selected_features', []),
        'model_info':        prediction_state.get('varmax_model_info', {}),
        'plots':             prediction_state.get('varmax_plots', {})
    })

# 4) VARMAX ì˜ˆì¸¡ê°’ë§Œ ì¡°íšŒ
@app.route('/api/varmax/predictions', methods=['GET'])
def get_varmax_predictions_only():
    """VARMAX ì˜ˆì¸¡ ê°’ë§Œ ì¡°íšŒ API"""
    global prediction_state
    
    if not prediction_state.get('varmax_predictions'):
        return jsonify({'error': 'No VARMAX prediction results available'}), 404
    
    return jsonify({
        'success': True,
        'current_date':      prediction_state['varmax_current_date'],
        'predictions':       prediction_state['varmax_predictions'],
        'model_info':        prediction_state['varmax_model_info']
    })

# 5) VARMAX ì´ë™í‰ê·  ì¡°íšŒ - ì¦‰ì„ ê³„ì‚° ë°©ì‹
@app.route('/api/varmax/moving-averages', methods=['GET'])
def get_varmax_moving_averages():
    """VARMAX ì´ë™í‰ê·  ì¡°íšŒ API - ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì¦‰ì„ ê³„ì‚°"""
    global prediction_state
    
    # ğŸ¯ ìƒíƒœì— MA ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if prediction_state.get('varmax_ma_results'):
        return jsonify({
            'success': True,
            'current_date': prediction_state['varmax_current_date'],
            'ma_results': prediction_state['varmax_ma_results']
        })
    
    # ğŸš€ ì˜ˆì¸¡ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì¦‰ì„ì—ì„œ MA ê³„ì‚°
    varmax_predictions = prediction_state.get('varmax_predictions')
    current_date = prediction_state.get('varmax_current_date')
    current_file = prediction_state.get('current_file')
    
    # ìƒíƒœì— ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìºì‹œì—ì„œ ë¡œë“œ
    if not varmax_predictions or not current_date:
        logger.info(f"ğŸ”§ [VARMAX_MA_API] No predictions in state, loading from cache")
        try:
            saved_predictions = get_saved_varmax_predictions_list(limit=1)
            if saved_predictions:
                latest_date = saved_predictions[0]['prediction_date']
                cached_prediction = load_varmax_prediction(latest_date)
                if cached_prediction and cached_prediction.get('predictions'):
                    varmax_predictions = cached_prediction.get('predictions')
                    current_date = cached_prediction.get('current_date', latest_date)
                    # current_fileì€ prediction_stateì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ì¶”ì •
                    if not current_file:
                        current_file = prediction_state.get('current_file')
                    logger.info(f"âœ… [VARMAX_MA_API] Loaded predictions from cache: {len(varmax_predictions)} items")
        except Exception as e:
            logger.error(f"âŒ [VARMAX_MA_API] Failed to load from cache: {e}")
    
    # ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬
    if not varmax_predictions or not current_date:
        return jsonify({
            'success': False,
            'error': 'No VARMAX predictions available for MA calculation'
        }), 404
    
    # ğŸ¯ ì¦‰ì„ì—ì„œ MA ê³„ì‚°
    try:
        logger.info(f"ğŸ”„ [VARMAX_MA_API] Calculating MA on-the-fly for {len(varmax_predictions)} predictions")
        
        # VARMAX í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (MA ê³„ì‚°ìš©)
        if not current_file or not os.path.exists(current_file):
            logger.error(f"âŒ [VARMAX_MA_API] File not found: {current_file}")
            return jsonify({
                'success': False,
                'error': 'Original data file not available for MA calculation'
            }), 404
            
        forecaster = VARMAXSemiMonthlyForecaster(current_file, pred_days=50)
        forecaster.load_data()  # ê³¼ê±° ë°ì´í„° ë¡œë“œ
        
        # MA ê³„ì‚°
        ma_results = forecaster.calculate_moving_averages_varmax(
            varmax_predictions, 
            current_date, 
            windows=[5, 10, 20, 30]
        )
        
        logger.info(f"âœ… [VARMAX_MA_API] MA calculation completed: {len(ma_results)} windows")
        
        # ìƒíƒœì— ì €ì¥ (ë‹¤ìŒë²ˆ ìš”ì²­ì„ ìœ„í•´)
        prediction_state['varmax_ma_results'] = ma_results
        
        return jsonify({
            'success': True,
            'current_date': current_date,
            'ma_results': ma_results
        })
        
    except Exception as e:
        logger.error(f"âŒ [VARMAX_MA_API] MA calculation failed: {e}")
        logger.error(f"âŒ [VARMAX_MA_API] Traceback: {traceback.format_exc()}")
        return jsonify({
            'success': False,
            'error': f'MA calculation failed: {str(e)}'
        }), 500

# 6) VARMAX ì˜ì‚¬ê²°ì • ì¡°íšŒ
@app.route('/api/varmax/saved', methods=['GET'])
def get_saved_varmax_predictions():
    """ì €ì¥ëœ VARMAX ì˜ˆì¸¡ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” API"""
    try:
        limit = request.args.get('limit', 100, type=int)
        predictions = get_saved_varmax_predictions_list(limit=limit)
        
        return jsonify({
            'success': True,
            'predictions': predictions,
            'count': len(predictions)
        })
        
    except Exception as e:
        logger.error(f"Error getting saved VARMAX predictions: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/varmax/saved/<date>', methods=['GET'])
def get_saved_varmax_prediction_by_date(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ë°˜í™˜í•˜ëŠ” API"""
    global prediction_state
    
    try:
        prediction = load_varmax_prediction(date)
        
        if prediction is None:
            return jsonify({
                'success': False,
                'error': f'Prediction not found for date: {date}'
            }), 404
        
        # ğŸ” ë¡œë“œëœ ì˜ˆì¸¡ ë°ì´í„° íƒ€ì… í™•ì¸
        logger.info(f"ğŸ” [VARMAX_API_LOAD] Prediction data type: {type(prediction)}")
        
        if not isinstance(prediction, dict):
            logger.error(f"âŒ [VARMAX_API_LOAD] Prediction is not a dictionary: {type(prediction)}")
            return jsonify({
                'success': False,
                'error': f'Invalid prediction data format: expected dict, got {type(prediction).__name__}'
            }), 500
        
        # ğŸ”§ ë°±ì—”ë“œ prediction_state ë³µì›
        logger.info(f"ğŸ”„ [VARMAX_LOAD] Restoring prediction_state for date: {date}")
        logger.info(f"ğŸ” [VARMAX_LOAD] Available prediction keys: {list(prediction.keys())}")
        
        # VARMAX ìƒíƒœ ë³µì› (ì•ˆì „í•œ ì ‘ê·¼)
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 100
        prediction_state['varmax_error'] = None
        prediction_state['varmax_current_date'] = prediction.get('current_date', date)
        prediction_state['varmax_predictions'] = prediction.get('predictions', [])
        prediction_state['varmax_half_month_averages'] = prediction.get('half_month_averages', [])
        prediction_state['varmax_metrics'] = prediction.get('metrics', {})
        prediction_state['varmax_ma_results'] = prediction.get('ma_results', {})
        prediction_state['varmax_selected_features'] = prediction.get('selected_features', [])
        prediction_state['varmax_model_info'] = prediction.get('model_info', {})
        prediction_state['varmax_plots'] = prediction.get('plots', {})
        
        logger.info(f"âœ… [VARMAX_LOAD] prediction_state restored successfully")
        logger.info(f"ğŸ” [VARMAX_LOAD] Restored predictions count: {len(prediction_state['varmax_predictions'])}")
        logger.info(f"ğŸ” [VARMAX_LOAD] MA results keys: {list(prediction_state['varmax_ma_results'].keys()) if prediction_state['varmax_ma_results'] else 'None'}")
        
        return jsonify({
            'success': True,
            'prediction': prediction
        })
        
    except Exception as e:
        logger.error(f"Error getting saved VARMAX prediction for {date}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/varmax/saved/<date>', methods=['DELETE'])
def delete_saved_varmax_prediction_api(date):
    """íŠ¹ì • ë‚ ì§œì˜ ì €ì¥ëœ VARMAX ì˜ˆì¸¡ì„ ì‚­ì œí•˜ëŠ” API"""
    try:
        success = delete_saved_varmax_prediction(date)
        
        if not success:
            return jsonify({
                'success': False,
                'error': f'Failed to delete prediction for date: {date}'
            }), 404
        
        return jsonify({
            'success': True,
            'message': f'Prediction for {date} deleted successfully'
        })
        
    except Exception as e:
        logger.error(f"Error deleting saved VARMAX prediction for {date}: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

# 6) VARMAX ì˜ì‚¬ê²°ì • ì¡°íšŒ
# 7) VARMAX ìƒíƒœ ë¦¬ì…‹ API (ìƒˆë¡œ ì¶”ê°€)
@app.route('/api/varmax/reset', methods=['POST', 'OPTIONS'])
@cross_origin()
def reset_varmax_state():
    """VARMAX ì˜ˆì¸¡ ìƒíƒœë¥¼ ë¦¬ì…‹í•˜ëŠ” API (hangëœ ì˜ˆì¸¡ í•´ê²°ìš©)"""
    global prediction_state
    
    if request.method == 'OPTIONS':
        return make_response('', 200)
    
    try:
        logger.info("ğŸ”„ [VARMAX_RESET] Resetting VARMAX prediction state...")
        
        # VARMAX ìƒíƒœ ì™„ì „ ë¦¬ì…‹
        prediction_state['varmax_is_predicting'] = False
        prediction_state['varmax_prediction_progress'] = 0
        prediction_state['varmax_error'] = None
        prediction_state['varmax_predictions'] = []
        prediction_state['varmax_half_month_averages'] = []
        prediction_state['varmax_metrics'] = {}
        prediction_state['varmax_ma_results'] = {}
        prediction_state['varmax_selected_features'] = []
        prediction_state['varmax_current_date'] = None
        prediction_state['varmax_model_info'] = {}
        prediction_state['varmax_plots'] = {}
        
        logger.info("âœ… [VARMAX_RESET] VARMAX state reset completed")
        
        return jsonify({
            'success': True,
            'message': 'VARMAX state reset successfully',
            'current_state': {
                'is_predicting': prediction_state.get('varmax_is_predicting', False),
                'progress': prediction_state.get('varmax_prediction_progress', 0),
                'error': prediction_state.get('varmax_error')
            }
        })
        
    except Exception as e:
        logger.error(f"âŒ [VARMAX_RESET] Error resetting VARMAX state: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to reset VARMAX state: {str(e)}'
        }), 500

@app.route('/api/varmax/decision', methods=['POST', 'OPTIONS'])
@cross_origin() 
def get_varmax_decision():
    """VARMAX ì˜ì‚¬ ê²°ì • ì¡°íšŒ API"""
    # 1) OPTIONS(preflight) ìš”ì²­ ì²˜ë¦¬
    if request.method == 'OPTIONS':
        return make_response('', 200)
    if 'file' not in request.files:
        return jsonify({'success': False, 'error': 'íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 400

    file = request.files['file']
    # íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    save_dir = '/path/to/models'
    os.makedirs(save_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    filepath = os.path.join(save_dir, secure_filename(file.filename))
    file.save(filepath)

    logger.info("POST /api/varmax/decision ë¡œ ì§„ì…")
    #data = request.get_json()
    #filepath = data.get('filepath')
    """# ìœ íš¨ì„± ê²€ì‚¬
    if not filepath or not os.path.exists(os.path.normpath(filepath)):
        return jsonify({'success': False, 'error': 'Invalid file path'}), 400"""

    results = varmax_decision(filepath)
    logger.info("ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ í˜•ì„± ì™„ë£Œ")
    column_order1 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìŒìˆ˜ ë¹„ìœ¨ [%]"]
    column_order2 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]"]

    return jsonify({
        'success': True,
        'filepath': filepath,  # â† íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        'filename': file.filename,
        'columns1': column_order1,
        'columns2': column_order2,
        'case_1':      results['case_1'],
        'case_2':      results['case_2'],
    })

@app.route('/api/market-status', methods=['GET'])
def get_market_status():
    """ìµœê·¼ 30ì¼ê°„ì˜ ì‹œì¥ ê°€ê²© ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°˜í™˜í•˜ëŠ” API"""
    try:
        # íŒŒì¼ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        file_path = request.args.get('file_path')
        if not file_path:
            return jsonify({
                'success': False,
                'error': 'File path is required'
            }), 400
        
        # URL ë””ì½”ë”© ë° íŒŒì¼ ê²½ë¡œ ì •ê·œí™” (Windows ë°±ìŠ¬ë˜ì‹œ ì²˜ë¦¬)
        import urllib.parse
        file_path = urllib.parse.unquote(file_path)  # URL ë””ì½”ë”©
        file_path = os.path.normpath(file_path)
        logger.info(f"ğŸ“Š [MARKET_STATUS] Normalized file path: {file_path}")
        
        # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if not os.path.exists(file_path):
            logger.error(f"âŒ [MARKET_STATUS] File not found: {file_path}")
            return jsonify({
                'success': False,
                'error': f'File not found: {file_path}'
            }), 400
        
        # ì›ë³¸ ë°ì´í„° ì§ì ‘ ë¡œë“œ (Date ì»¬ëŸ¼ ìœ ì§€ë¥¼ ìœ„í•´) - Excel/CSV íŒŒì¼ ëª¨ë‘ ì§€ì›
        try:
            file_ext = os.path.splitext(file_path.lower())[1]
            if file_ext == '.csv':
                df = pd.read_csv(file_path)
                logger.info(f"ğŸ“Š [MARKET_STATUS] CSV data loaded: {df.shape}")
            elif file_ext in ['.xlsx', '.xls']:
                # Excel íŒŒì¼ì˜ ê²½ìš° ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
                df = load_data_safe(file_path, use_cache=True, use_xlwings_fallback=True)
                # ì¸ë±ìŠ¤ê°€ Dateì¸ ê²½ìš° ì»¬ëŸ¼ìœ¼ë¡œ ë³µì›
                if df.index.name == 'Date':
                    df = df.reset_index()
                logger.info(f"ğŸ“Š [MARKET_STATUS] Excel data loaded with security bypass: {df.shape}")
            else:
                logger.error(f"âŒ [MARKET_STATUS] Unsupported file format: {file_ext}")
                return jsonify({
                    'success': False,
                    'error': f'Unsupported file format: {file_ext}. Only CSV and Excel files are supported.'
                }), 400
        except Exception as e:
            logger.error(f"âŒ [MARKET_STATUS] Failed to load data file: {str(e)}")
            return jsonify({
                'success': False,
                'error': f'Failed to load data file: {str(e)}'
            }), 400
        
        if df is None or df.empty:
            logger.error(f"âŒ [MARKET_STATUS] No data available or empty dataframe")
            return jsonify({
                'success': False,
                'error': 'No data available'
            }), 400
        
        # ë‚ ì§œ ì»¬ëŸ¼ í™•ì¸ ë° ì •ë ¬
        logger.info(f"ğŸ“Š [MARKET_STATUS] Columns in dataframe: {list(df.columns)}")
        if 'Date' not in df.columns:
            logger.error(f"âŒ [MARKET_STATUS] Date column not found. Available columns: {list(df.columns)}")
            return jsonify({
                'success': False,
                'error': 'Date column not found in data'
            }), 400
        
        # ë‚ ì§œë¡œ ì •ë ¬
        df = df.sort_values('Date')
        
        # íœ´ì¼ ì •ë³´ ë¡œë“œ
        holidays = get_combined_holidays(df=df)
        holiday_dates = set([h['date'] if isinstance(h, dict) else h for h in holidays])
        
        # ì˜ì—…ì¼ë§Œ í•„í„°ë§
        def is_business_day(date_str):
            date_obj = pd.to_datetime(date_str).date()
            weekday = date_obj.weekday()  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
            return weekday < 5 and date_str not in holiday_dates  # ì›”~ê¸ˆ & íœ´ì¼ ì•„ë‹˜
        
        logger.info(f"ğŸ“Š [MARKET_STATUS] Total rows before business day filtering: {len(df)}")
        logger.info(f"ğŸ“Š [MARKET_STATUS] Holiday dates count: {len(holiday_dates)}")
        
        business_days_df = df[df['Date'].apply(is_business_day)]
        logger.info(f"ğŸ“Š [MARKET_STATUS] Business days after filtering: {len(business_days_df)}")
        
        if business_days_df.empty:
            logger.error(f"âŒ [MARKET_STATUS] No business days found after filtering")
            return jsonify({
                'success': False,
                'error': 'No business days found in data'
            }), 400
        
        # ìµœê·¼ 30ì¼ ì˜ì—…ì¼ ë°ì´í„° ì¶”ì¶œ
        recent_30_days = business_days_df.tail(30)

        # Crack ë³€ìˆ˜ ê³„ì‚° (MOPJ - Brent_Singapore * 7.5)
        if 'MOPJ' in recent_30_days.columns and 'Brent_Singapore' in recent_30_days.columns:
            recent_30_days = recent_30_days.copy()  # ê²½ê³  ë°©ì§€ë¥¼ ìœ„í•œ ë³µì‚¬ë³¸ ìƒì„±
            recent_30_days['Crack'] = recent_30_days['MOPJ'] - (recent_30_days['Brent_Singapore'] * 7.5)
            logger.info(f"ğŸ“Š [MARKET_STATUS] Crack variable calculated: MOPJ - Brent_Singapore * 7.5")
        elif 'MOPJ' in recent_30_days.columns and 'Brent' in recent_30_days.columns:
            # Brent_Singaporeê°€ ì—†ìœ¼ë©´ Brentë¡œ ëŒ€ì²´ ê³„ì‚°
            recent_30_days = recent_30_days.copy()
            recent_30_days['Crack'] = recent_30_days['MOPJ'] - (recent_30_days['Brent'] * 7.5)
            logger.info(f"ğŸ“Š [MARKET_STATUS] Crack variable calculated with Brent fallback: MOPJ - Brent * 7.5")
        else:
            logger.warning(f"âš ï¸ [MARKET_STATUS] Cannot calculate Crack: Missing MOPJ or Brent columns")

        # ì¹´í…Œê³ ë¦¬ë³„ ì»¬ëŸ¼ ë¶„ë¥˜ (ì‹¤ì œ ë°ì´í„° ì»¬ëŸ¼ëª…ì— ë§ê²Œ ìˆ˜ì •)
        categories = {
            'ì›ìœ  ê°€ê²©': [
                'WTI', 'Brent', 'Dubai', 'Crack'
            ],
            'ê°€ì†”ë¦° ê°€ê²©': [
                'Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'
            ],
            'ë‚˜í”„íƒ€ ê°€ê²©': [
                'MOPJ', 'MOPAG', 'MOPS', 'Europe_CIF NWE'
            ],
            'LPG ê°€ê²©': [
                'C3_LPG', 'C4_LPG'
            ],
            'ê²½ì œì§€í‘œ': [
                'Dow_Jones', 'Euro', 'Gold', 'Exchange'
            ],
            'ì„ìœ í™”í•™ ì œí’ˆ ê°€ê²©': [
                'EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 
                'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2','MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 
                'FO_HSFO 180 CST', 'MTBE_FOB Singapore'
            ]
        }
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ í•„í„°ë§
        available_columns = set(recent_30_days.columns)
        filtered_categories = {}
        
        logger.info(f"ğŸ“Š [MARKET_STATUS] Available columns: {sorted(available_columns)}")
        
        for category, columns in categories.items():
            existing_columns = [col for col in columns if col in available_columns]
            if existing_columns:
                filtered_categories[category] = existing_columns
                logger.info(f"ğŸ“Š [MARKET_STATUS] Category '{category}': found {len(existing_columns)} columns: {existing_columns}")
            else:
                logger.warning(f"âš ï¸ [MARKET_STATUS] Category '{category}': no matching columns found from {columns}")
        
        if not filtered_categories:
            logger.error(f"âŒ [MARKET_STATUS] No categories found! Expected columns don't match available columns")
            return jsonify({
                'success': False,
                'error': 'No matching columns found for market status categories',
                'debug_info': {
                    'available_columns': sorted(available_columns),
                    'expected_categories': categories
                }
            }), 400
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° êµ¬ì„±
        result = {
            'success': True,
            'date_range': {
                'start_date': recent_30_days['Date'].iloc[0],
                'end_date': recent_30_days['Date'].iloc[-1],
                'total_days': len(recent_30_days)
            },
            'categories': {}
        }
        
        for category, columns in filtered_categories.items():
            category_data = {
                'columns': columns,
                'data': []
            }
            
            for _, row in recent_30_days.iterrows():
                data_point = {
                    'date': row['Date'],
                    'values': {}
                }
                
                for col in columns:
                    if pd.notna(row[col]):
                        data_point['values'][col] = float(row[col])
                    else:
                        data_point['values'][col] = None
                
                category_data['data'].append(data_point)
            
            result['categories'][category] = category_data
        
        logger.info(f"âœ… [MARKET_STATUS] Returned {len(recent_30_days)} business days data for {len(filtered_categories)} categories")
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"âŒ [MARKET_STATUS] Error: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to get market status: {str(e)}'
        }), 500

@app.route('/api/gpu-info', methods=['GET'])
def get_gpu_info():
    """GPU ë° ë””ë°”ì´ìŠ¤ ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” API"""
    try:
        from app.core.gpu_manager import get_detailed_gpu_utilization
        # ì‹¤ì‹œê°„ GPU í…ŒìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸
        run_test = request.args.get('test', 'false').lower() == 'true'
        
        # GPU ì •ë³´ ìˆ˜ì§‘
        device_info = {
            'cuda_available': torch.cuda.is_available(),
            'pytorch_version': torch.__version__,
            'default_device': str(DEFAULT_DEVICE),
            'current_device_info': {},
            'test_performed': False,
            'test_results': {}
        }
        
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            current_device = torch.cuda.current_device()
            
            # ì‹¤ì‹œê°„ GPU í™œìš©ë¥  í™•ì¸ (ìƒì„¸ ë²„ì „)
            gpu_utilization_stats = get_detailed_gpu_utilization()
            
            device_info.update({
                'gpu_count': gpu_count,
                'current_gpu_device': current_device,
                'cudnn_version': torch.backends.cudnn.version(),
                'cudnn_enabled': torch.backends.cudnn.enabled,
                'detailed_utilization': gpu_utilization_stats,
                'gpus': []
            })
            
            # ê° GPU ì •ë³´
            for i in range(gpu_count):
                gpu_props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total = gpu_props.total_memory / 1024**3
                
                # PyTorch ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ì•ˆì „í•œ ì†ì„± ì ‘ê·¼
                gpu_info = {
                    'device_id': i,
                    'name': getattr(gpu_props, 'name', 'Unknown GPU'),
                    'total_memory_gb': round(total, 2),
                    'allocated_memory_gb': round(allocated, 2),
                    'cached_memory_gb': round(cached, 2),
                    'memory_usage_percent': round((allocated / total) * 100, 2),
                    'compute_capability': f"{getattr(gpu_props, 'major', 0)}.{getattr(gpu_props, 'minor', 0)}",
                    'is_current': i == current_device
                }
                
                # ì„ íƒì  ì†ì„±ë“¤ (PyTorch ë²„ì „ì— ë”°ë¼ ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
                if hasattr(gpu_props, 'multiprocessor_count'):
                    gpu_info['multiprocessor_count'] = gpu_props.multiprocessor_count
                elif hasattr(gpu_props, 'multi_processor_count'):
                    gpu_info['multiprocessor_count'] = gpu_props.multi_processor_count
                else:
                    gpu_info['multiprocessor_count'] = 'N/A'
                
                # ì¶”ê°€ GPU ì†ì„±ë“¤ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ)
                optional_attrs = {
                    'max_threads_per_block': 'max_threads_per_block',
                    'max_threads_per_multiprocessor': 'max_threads_per_multiprocessor',
                    'warp_size': 'warp_size',
                    'memory_clock_rate': 'memory_clock_rate'
                }
                
                for attr_name, prop_name in optional_attrs.items():
                    if hasattr(gpu_props, prop_name):
                        gpu_info[attr_name] = getattr(gpu_props, prop_name)
                
                device_info['gpus'].append(gpu_info)
            
            # í˜„ì¬ ë””ë°”ì´ìŠ¤ ìƒì„¸ ì •ë³´
            current_gpu_props = torch.cuda.get_device_properties(current_device)
            device_info['current_device_info'] = {
                'name': current_gpu_props.name,
                'total_memory_gb': round(current_gpu_props.total_memory / 1024**3, 2),
                'allocated_memory_gb': round(torch.cuda.memory_allocated(current_device) / 1024**3, 2),
                'cached_memory_gb': round(torch.cuda.memory_reserved(current_device) / 1024**3, 2)
            }
            
            # GPU í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (ìš”ì²­ëœ ê²½ìš°)
            if run_test:
                try:
                    logger.info("ğŸ§ª APIì—ì„œ GPU í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ì¤‘...")
                    
                    # í…ŒìŠ¤íŠ¸ ì „ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_before = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    # ê°„ë‹¨í•œ GPU ì—°ì‚° í…ŒìŠ¤íŠ¸
                    test_size = 500
                    test_tensor = torch.randn(test_size, test_size, device=current_device, dtype=torch.float32)
                    test_result = torch.matmul(test_tensor, test_tensor.T)
                    computation_result = torch.sum(test_result).item()
                    
                    # í…ŒìŠ¤íŠ¸ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_after = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì°¨ì´ ê³„ì‚°
                    memory_diff = {
                        'allocated_diff': memory_after['allocated'] - memory_before['allocated'],
                        'cached_diff': memory_after['cached'] - memory_before['cached']
                    }
                    
                    device_info['test_performed'] = True
                    device_info['test_results'] = {
                        'test_tensor_size': f"{test_size}x{test_size}",
                        'computation_result': round(computation_result, 4),
                        'memory_before_gb': {
                            'allocated': round(memory_before['allocated'], 4),
                            'cached': round(memory_before['cached'], 4)
                        },
                        'memory_after_gb': {
                            'allocated': round(memory_after['allocated'], 4),
                            'cached': round(memory_after['cached'], 4)
                        },
                        'memory_diff_gb': {
                            'allocated': round(memory_diff['allocated_diff'], 4),
                            'cached': round(memory_diff['cached_diff'], 4)
                        },
                        'test_success': True
                    }
                    
                    # í…ŒìŠ¤íŠ¸ í…ì„œ ì •ë¦¬
                    del test_tensor, test_result
                    torch.cuda.empty_cache()
                    
                    # ì •ë¦¬ í›„ ë©”ëª¨ë¦¬ ìƒíƒœ
                    memory_final = {
                        'allocated': torch.cuda.memory_allocated(current_device) / 1024**3,
                        'cached': torch.cuda.memory_reserved(current_device) / 1024**3
                    }
                    
                    device_info['test_results']['memory_after_cleanup_gb'] = {
                        'allocated': round(memory_final['allocated'], 4),
                        'cached': round(memory_final['cached'], 4)
                    }
                    
                    logger.info(f"âœ… GPU í…ŒìŠ¤íŠ¸ ì™„ë£Œ: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë³€í™” {memory_diff['allocated_diff']:.4f}GB")
                    
                except Exception as test_e:
                    logger.error(f"âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(test_e)}")
                    device_info['test_performed'] = True
                    device_info['test_results'] = {
                        'test_success': False,
                        'error': str(test_e)
                    }
        else:
            device_info.update({
                'gpu_count': 0,
                'reason': 'CUDA not available - using CPU'
            })
        
        # ë¡œê·¸ì—ë„ ì •ë³´ ì¶œë ¥
        logger.info(f"ğŸ” GPU Info API í˜¸ì¶œ:")
        logger.info(f"  ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: {device_info['cuda_available']}")
        logger.info(f"  âš¡ ê¸°ë³¸ ë””ë°”ì´ìŠ¤: {device_info['default_device']}")
        if device_info['cuda_available']:
            logger.info(f"  ğŸ® GPU ê°œìˆ˜: {device_info.get('gpu_count', 0)}")
            if 'current_gpu_device' in device_info:
                logger.info(f"  ğŸ¯ í˜„ì¬ GPU: {device_info['current_gpu_device']}")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¡œê¹…
        if device_info.get('test_performed', False):
            test_results = device_info.get('test_results', {})
            if test_results.get('test_success', False):
                logger.info(f"  âœ… GPU í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            else:
                logger.warning(f"  âŒ GPU í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {test_results.get('error', 'Unknown error')}")
        
        return jsonify({
            'success': True,
            'device_info': device_info,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ GPU ì •ë³´ API ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to get GPU info: {str(e)}'
        }), 500

@app.route('/api/gpu-monitoring-comparison', methods=['GET'])
def get_gpu_monitoring_comparison():
    """ë‹¤ì–‘í•œ GPU ëª¨ë‹ˆí„°ë§ ë°©ë²•ì„ ë¹„êµí•˜ëŠ” API"""
    try:
        comparison_data = compare_gpu_monitoring_methods()
        
        # ì¶”ê°€ì ì¸ ì„¤ëª… ì •ë³´
        explanation = {
            'why_different_readings': [
                "Windows ì‘ì—… ê´€ë¦¬ìëŠ” ì£¼ë¡œ 3D ê·¸ë˜í”½ ì—”ì§„ í™œìš©ë¥ ì„ í‘œì‹œí•©ë‹ˆë‹¤",
                "nvidia-smiëŠ” CUDA ì—°ì‚° í™œìš©ë¥ ì„ ì¸¡ì •í•˜ë¯€ë¡œ ML/AI ì‘ì—…ì— ë” ì •í™•í•©ë‹ˆë‹¤",
                "ì¸¡ì • ì‹œì ì˜ ì°¨ì´ë¡œ ì¸í•´ ìˆœê°„ì ì¸ ê°’ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤",
                "GPUëŠ” ì—¬ëŸ¬ ì—”ì§„(Compute, 3D, Encoder, Decoder)ì„ ê°€ì§€ê³  ìˆì–´ ê°ê° ë‹¤ë¥¸ í™œìš©ë¥ ì„ ë³´ì…ë‹ˆë‹¤"
            ],
            'recommendations': [
                "ML/AI ì‘ì—…: nvidia-smiì˜ GPU í™œìš©ë¥  í™•ì¸",
                "ê²Œì„/3D ë Œë”ë§: Windows ì‘ì—… ê´€ë¦¬ìì˜ 3D í™œìš©ë¥  í™•ì¸", 
                "ë¹„ë””ì˜¤ ì²˜ë¦¬: nvidia-smiì˜ Encoder/Decoder í™œìš©ë¥  í™•ì¸",
                "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: PyTorch CUDA ì •ë³´ì™€ nvidia-smi ëª¨ë‘ í™•ì¸"
            ],
            'task_manager_vs_nvidia_smi': {
                "ì‘ì—… ê´€ë¦¬ì GPU": "ì£¼ë¡œ 3D ê·¸ë˜í”½ ì›Œí¬ë¡œë“œ (DirectX, OpenGL)",
                "nvidia-smi GPU": "CUDA ì—°ì‚° ì›Œí¬ë¡œë“œ (ML, AI, GPGPU)",
                "ì™œ ë‹¤ë¥¸ê°€": "ì„œë¡œ ë‹¤ë¥¸ GPU ì—”ì§„ì„ ì¸¡ì •í•˜ê¸° ë•Œë¬¸",
                "ì–´ëŠ ê²ƒì´ ì •í™•í•œê°€": "ì‘ì—… ìœ í˜•ì— ë”°ë¼ ë‹¤ë¦„ - ML/AIëŠ” nvidia-smiê°€ ì •í™•"
            }
        }
        
        # í˜„ì¬ ìƒí™© ë¶„ì„
        current_analysis = {
            'status': 'monitoring_successful',
            'notes': []
        }
        
        if comparison_data.get('nvidia_smi'):
            nvidia_util = comparison_data['nvidia_smi'].get('gpu_utilization', '0')
            try:
                util_value = float(nvidia_util)
                if util_value < 10:
                    current_analysis['notes'].append(f"í˜„ì¬ CUDA í™œìš©ë¥ ì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤ ({util_value}%)")
                    current_analysis['notes'].append("ì´ëŠ” ì •ìƒì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤ - ML ì‘ì—…ì´ ì§„í–‰ ì¤‘ì´ ì•„ë‹ ë•Œ")
                elif util_value > 50:
                    current_analysis['notes'].append(f"í˜„ì¬ CUDA í™œìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤ ({util_value}%)")
                    current_analysis['notes'].append("ML/AI ì‘ì—…ì´ í™œë°œíˆ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤")
            except:
                pass
        
        if comparison_data.get('torch_cuda'):
            memory_usage = comparison_data['torch_cuda'].get('memory_usage_percent', 0)
            if memory_usage > 1:
                current_analysis['notes'].append(f"PyTorchê°€ GPU ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤ ({memory_usage:.1f}%)")
            else:
                current_analysis['notes'].append("PyTorchê°€ í˜„ì¬ GPU ë©”ëª¨ë¦¬ë¥¼ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        return jsonify({
            'success': True,
            'comparison_data': comparison_data,
            'explanation': explanation,
            'current_analysis': current_analysis,
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"âŒ GPU ëª¨ë‹ˆí„°ë§ ë¹„êµ API ì˜¤ë¥˜: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Failed to compare GPU monitoring methods: {str(e)}'
        }), 500

# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ì—…ë°ì´íŠ¸
if __name__ == '__main__':
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    try:
        import optuna
    except ImportError:
        logger.warning("Optuna íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•´ https://inthiswork.com/archives/226539ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        logger.warning("pip install optuna ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ğŸ¯ íŒŒì¼ë³„ ìºì‹œ ì‹œìŠ¤í…œ - ë ˆê±°ì‹œ ë””ë ‰í† ë¦¬ ë° ì¸ë±ìŠ¤ íŒŒì¼ ìƒì„± ì œê±°
    # ëª¨ë“  ë°ì´í„°ëŠ” ì´ì œ íŒŒì¼ë³„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥ë©ë‹ˆë‹¤
    logger.info("ğŸš€ Starting with file-based cache system - no legacy directories needed")
    
    # ë¼ìš°íŠ¸ ë“±ë¡ í™•ì¸ì„ ìœ„í•œ ë””ë²„ê¹…
    print("Registered routes:")
    for rule in app.url_map.iter_rules():
        print(f"  {rule.endpoint}: {rule.rule} {list(rule.methods or [])}")
    
    print("Starting Flask app with attention-map endpoint...")
    app.run(debug=True, host='0.0.0.0', port=5000, threaded=True)
