import pandas as pd
import numpy as np
import calendar
import re
import logging
import os
import traceback

from pathlib import Path

# holidays ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© (ì•„ë˜ holiday ê´€ë ¨ í•¨ìˆ˜ëŠ” ì—¬ê¸°ì— ë°°ì¹˜)
from app.config import HOLIDAY_DIR
from app.utils.date_utils import format_date
from app.utils.file_utils import set_seed

logger = logging.getLogger(__name__)  # logger ì •ì˜ ì¶”ê°€

holidays = set()

def create_proper_column_names(file_path, sheet_name):
    from app.data.loader import safe_read_excel
    """í—¤ë” 3í–‰ì„ ì½ì–´ì„œ ì ì ˆí•œ ì—´ ì´ë¦„ ìƒì„±"""
    # í—¤ë” 3í–‰ì„ ì½ì–´ì˜´
    header_rows = safe_read_excel(file_path, sheet_name=sheet_name, header=None, nrows=3)
    
    # ê° ì—´ë³„ë¡œ ì ì ˆí•œ ì´ë¦„ ìƒì„±
    column_names = []
    prev_main_category = None  # ì´ì „ ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì €ì¥
    
    for col_idx in range(header_rows.shape[1]):
        values = [str(header_rows.iloc[i, col_idx]).strip() 
                 for i in range(3) 
                 if pd.notna(header_rows.iloc[i, col_idx]) and str(header_rows.iloc[i, col_idx]).strip() != 'nan']
        
        # ì²« ë²ˆì§¸ í–‰ì˜ ê°’ì´ ìˆìœ¼ë©´ ë©”ì¸ ì¹´í…Œê³ ë¦¬ë¡œ ì €ì¥
        if pd.notna(header_rows.iloc[0, col_idx]) and str(header_rows.iloc[0, col_idx]).strip() != 'nan':
            prev_main_category = str(header_rows.iloc[0, col_idx]).strip()
        
        # ì—´ ì´ë¦„ ìƒì„± ë¡œì§
        if 'Date' in values:
            column_names.append('Date')
        else:
            # ê°’ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°
            if not values:
                column_names.append(f'Unnamed_{col_idx}')
                continue
                
            # ë©”ì¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆê³ , í˜„ì¬ ê°’ë“¤ì— í¬í•¨ë˜ì§€ ì•Šì€ ê²½ìš° ì¶”ê°€
            if prev_main_category and prev_main_category not in values:
                values.insert(0, prev_main_category)
            
            # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ ì²˜ë¦¬ (ì˜ˆ: WS, Naphtha ë“±)
            if 'WS' in values and 'SG-Korea' in values:
                column_names.append('WS_SG-Korea')
            elif 'Naphtha' in values and 'Platts' in values:
                column_names.append('Naphtha_Platts_' + '_'.join([v for v in values if v not in ['Naphtha', 'Platts']]))
            else:
                column_names.append('_'.join(values))
    
    return column_names

def remove_high_missing_columns(data, threshold=70):
    """ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ê°€ì§„ ì—´ ì œê±°"""
    missing_ratio = (data.isnull().sum() / len(data)) * 100
    high_missing_cols = missing_ratio[missing_ratio >= threshold].index
    
    print(f"\n=== {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆì–´ ì œê±°ë  ì—´ ëª©ë¡ ===")
    for col in high_missing_cols:
        print(f"- {col}: {missing_ratio[col]:.1f}%")
    
    cleaned_data = data.drop(columns=high_missing_cols)
    print(f"\nì›ë³¸ ë°ì´í„° í˜•íƒœ: {data.shape}")
    print(f"ì •ì œëœ ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
    
    return cleaned_data

def clean_text_values_advanced(data):
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ê°’ ì •ì œ (ì‰¼í‘œ ì†Œìˆ˜ì  ì²˜ë¦¬ í¬í•¨)"""
    cleaned_data = data.copy()
    
    def fix_comma_decimal(value_str):
        """ì‰¼í‘œë¡œ ëœ ì†Œìˆ˜ì ì„ ì ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜"""
        if not isinstance(value_str, str) or ',' not in value_str:
            return value_str
            
        import re
        
        # íŒ¨í„´ 1: ë‹¨ìˆœ ì†Œìˆ˜ì  ì‰¼í‘œ (ì˜ˆ: "123,45")
        if re.match(r'^-?\d+,\d{1,3}$', value_str):
            return value_str.replace(',', '.')
            
        # íŒ¨í„´ 2: ì²œ ë‹¨ìœ„ êµ¬ë¶„ì + ì†Œìˆ˜ì  ì‰¼í‘œ (ì˜ˆ: "1.234,56")
        if re.match(r'^-?\d{1,3}(\.\d{3})*,\d{1,3}$', value_str):
            # ë§ˆì§€ë§‰ ì‰¼í‘œë§Œ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€ê²½
            last_comma_pos = value_str.rfind(',')
            return value_str[:last_comma_pos] + '.' + value_str[last_comma_pos+1:]
            
        # íŒ¨í„´ 3: ì‰¼í‘œë§Œ ì²œ ë‹¨ìœ„ êµ¬ë¶„ìë¡œ ì‚¬ìš© (ì˜ˆ: "1,234,567")
        if re.match(r'^-?\d{1,3}(,\d{3})+$', value_str):
            return value_str.replace(',', '')
            
        return value_str
    
    def process_value(x):
        if pd.isna(x):  # ì´ë¯¸ NaNì¸ ê²½ìš°
            return x
        
        # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬
        x_str = str(x).strip()
        
        # 1. ë¨¼ì € ì‰¼í‘œ ì†Œìˆ˜ì  ë¬¸ì œ í•´ê²°
        x_str = fix_comma_decimal(x_str)
        
        # 2. íœ´ì¼/ë¯¸ë°œí‘œ ë°ì´í„° ì²˜ë¦¬
        if x_str.upper() in ['NOP', 'NO PUBLICATION', 'NO PUB']:
            return np.nan
            
        # 3. TBA (To Be Announced) ê°’ ì²˜ë¦¬ - íŠ¹ë³„ ë§ˆí‚¹í•˜ì—¬ ë‚˜ì¤‘ì— ì „ë‚ ê°’ìœ¼ë¡œ ëŒ€ì²´
        if x_str.upper() in ['TBA', 'TO BE ANNOUNCED']:
            return 'TBA_REPLACE'
            
        # 4. '*' í¬í•¨ëœ ê³„ì‚°ì‹ ì²˜ë¦¬
        if '*' in x_str:
            try:
                # ê³„ì‚°ì‹ ì‹¤í–‰
                return float(eval(x_str.replace(' ', '')))
            except:
                return x
        
        # 5. ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
        try:
            return float(x_str)
        except:
            return x

    # ì‰¼í‘œ ì²˜ë¦¬ í†µê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜
    comma_fixes = 0
    
    # ê° ì—´ì— ëŒ€í•´ ì²˜ë¦¬
    for column in cleaned_data.columns:
        if column != 'Date':  # Date ì—´ ì œì™¸
            # ì²˜ë¦¬ ì „ ì‰¼í‘œê°€ ìˆëŠ” ê°’ë“¤ í™•ì¸
            before_comma_count = cleaned_data[column].astype(str).str.contains(',', na=False).sum()
            
            cleaned_data[column] = cleaned_data[column].apply(process_value)
            
            # ì²˜ë¦¬ í›„ ì‰¼í‘œê°€ ìˆëŠ” ê°’ë“¤ í™•ì¸
            after_comma_count = cleaned_data[column].astype(str).str.contains(',', na=False).sum()
            
            if before_comma_count > after_comma_count:
                fixed_count = before_comma_count - after_comma_count
                comma_fixes += fixed_count
                print(f"ì—´ '{column}': {fixed_count}ê°œì˜ ì‰¼í‘œ ì†Œìˆ˜ì ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.")
    
    if comma_fixes > 0:
        print(f"\nì´ {comma_fixes}ê°œì˜ ì‰¼í‘œ ì†Œìˆ˜ì ì„ ì ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.")
    
    # MOPJ ë³€ìˆ˜ ì²˜ë¦¬ (ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°)
    mopj_columns = [col for col in cleaned_data.columns if 'MOPJ' in col or 'Naphtha_Platts_MOPJ' in col]
    if mopj_columns:
        mopj_col = mopj_columns[0]  # ì²« ë²ˆì§¸ MOPJ ê´€ë ¨ ì—´ ì‚¬ìš©
        print(f"\n=== {mopj_col} ë³€ìˆ˜ ì²˜ë¦¬ ì „ ë°ì´í„° í¬ê¸° ===")
        print(f"í–‰ ìˆ˜: {len(cleaned_data)}")
        
        # ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°
        cleaned_data = cleaned_data.dropna(subset=[mopj_col])
        
        # ë¬¸ìì—´ ê°’ì´ ìˆëŠ” í–‰ ì œê±°
        try:
            pd.to_numeric(cleaned_data[mopj_col], errors='raise')
        except:
            # ìˆ«ìë¡œ ë³€í™˜í•  ìˆ˜ ì—†ëŠ” í–‰ ì°¾ê¸°
            numeric_mask = pd.to_numeric(cleaned_data[mopj_col], errors='coerce').notna()
            cleaned_data = cleaned_data[numeric_mask]
        
        print(f"\n=== {mopj_col} ë³€ìˆ˜ ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸° ===")
        print(f"í–‰ ìˆ˜: {len(cleaned_data)}")
    
    # ğŸ”§ TBA ê°’ì„ ì „ë‚  ê°’ìœ¼ë¡œ ëŒ€ì²´
    tba_replacements = 0
    if 'Date' in cleaned_data.columns:
        # ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ (ì¤‘ìš”: ì „ë‚  ê°’ ì°¸ì¡°ë¥¼ ìœ„í•´)
        cleaned_data = cleaned_data.sort_values('Date').reset_index(drop=True)
        
        for column in cleaned_data.columns:
            if column != 'Date':  # Date ì—´ ì œì™¸
                # TBA_REPLACE ë§ˆí‚¹ëœ ê°’ë“¤ ì°¾ê¸°
                tba_mask = cleaned_data[column] == 'TBA_REPLACE'
                tba_indices = cleaned_data[tba_mask].index.tolist()
                
                if tba_indices:
                    print(f"\n[TBA ì²˜ë¦¬] ì—´ '{column}'ì—ì„œ {len(tba_indices)}ê°œì˜ TBA ê°’ ë°œê²¬")
                    
                    for idx in tba_indices:
                        # ğŸ”§ ê°œì„ : ê°€ì¥ ìµœê·¼ì˜ ìœ íš¨í•œ ê°’ ì°¾ê¸° (ì—°ì† TBA ì²˜ë¦¬)
                        replacement_value = None
                        source_description = ""
                        
                        # ì´ì „ í–‰ë“¤ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©´ì„œ ìœ íš¨í•œ ê°’ ì°¾ê¸°
                        for prev_idx in range(idx-1, -1, -1):
                            candidate_value = cleaned_data.loc[prev_idx, column]
                            try:
                                if pd.notna(candidate_value) and candidate_value != 'TBA_REPLACE':
                                    replacement_value = float(candidate_value)
                                    days_back = idx - prev_idx
                                    if days_back == 1:
                                        source_description = "ì „ë‚  ê°’"
                                    else:
                                        source_description = f"{days_back}ì¼ ì „ ê°’"
                                    break
                            except (ValueError, TypeError):
                                continue
                        
                        # ê°’ ëŒ€ì²´ ìˆ˜í–‰
                        if replacement_value is not None:
                            cleaned_data.loc[idx, column] = replacement_value
                            tba_replacements += 1
                            print(f"  - í–‰ {idx+1}: TBA â†’ {replacement_value} ({source_description})")
                        else:
                            # ìœ íš¨í•œ ì´ì „ ê°’ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
                            cleaned_data.loc[idx, column] = np.nan
                            print(f"  - í–‰ {idx+1}: TBA â†’ NaN (ìœ íš¨í•œ ì´ì „ ê°’ ì—†ìŒ)")
    
    if tba_replacements > 0:
        print(f"\nâœ… ì´ {tba_replacements}ê°œì˜ TBA ê°’ì„ ì „ë‚  ê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.")
    
    return cleaned_data

def fill_missing_values_advanced(data):
    """ê³ ê¸‰ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (forward fill + backward fill)"""
    filled_data = data.copy()
    
    # Date ì—´ ì œì™¸í•œ ëª¨ë“  ìˆ˜ì¹˜í˜• ì—´ì— ëŒ€í•´
    numeric_cols = filled_data.select_dtypes(include=[np.number]).columns
    
    # ì´ì „ ê°’ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸° (forward fill)
    filled_data[numeric_cols] = filled_data[numeric_cols].ffill()
    
    # ë‚¨ì€ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš° ë‹¤ìŒ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (backward fill)
    filled_data[numeric_cols] = filled_data[numeric_cols].bfill()
    
    return filled_data

def rename_columns_to_standard(data):
    """ì—´ ì´ë¦„ì„ í‘œì¤€ í˜•íƒœë¡œ ë³€ê²½"""
    column_mapping = {
        'Date': 'Date',
        'Crude Oil_WTI': 'WTI',
        'Crude Oil_Brent': 'Brent',
        'Crude Oil_Dubai': 'Dubai',
        'WS_AG-SG_55': 'WS_55',
        'WS_75.0': 'WS_75',
        'Naphtha_Platts_MOPJ': 'MOPJ',
        'Naphtha_MOPAG': 'MOPAG',
        'Naphtha_MOPS': 'MOPS',
        'Naphtha_Monthly Spread': 'Monthly Spread',
        'LPG_Argus FEI_C3': 'C3_LPG',
        'LPG_C4': 'C4_LPG',
        'Gasoline_FOB SP_92RON': 'Gasoline_92RON',
        'Gasoline_95RON': 'Gasoline_95RON',
        'Ethylene_Platts_CFR NEA': 'EL_CRF NEA',
        'Ethylene_CFR SEA': 'EL_CRF SEA',
        'Propylene_Platts_FOB Korea': 'PL_FOB Korea',
        'Benzene_Platts_FOB Korea': 'BZ_FOB Korea',
        'Benzene_Platts_FOB SEA': 'BZ_FOB SEA',
        'Benzene_Platts_FOB US M1': 'BZ_FOB US M1',
        'Benzene_Platts_FOB US M2': 'BZ_FOB US M2',
        'Benzene_Platts_H2-TIME SPREAD': 'BZ_H2-TIME SPREAD',
        'Toluene_Platts_FOB Korea': 'TL_FOB Korea',
        'Toluene_Platts_FOB US M1': 'TL_FOB US M1',
        'Toluene_Platts_FOB US M2': 'TL_FOB US M2',
        'MX_Platts FE_FOB K': 'MX_FOB Korea',
        'PX_FOB   Korea': 'PX_FOB Korea',
        'SM_FOB   Korea': 'SM_FOB Korea',
        'RPG Value_Calculated_FOB PG': 'RPG Value_FOB PG',
        'FO_Platts_HSFO 180 CST': 'FO_HSFO 180 CST',
        'MTBE_Platts_FOB S\'pore': 'MTBE_FOB Singapore',
        'MTBE_Dow_Jones': 'Dow_Jones',
        'MTBE_Euro': 'Euro',
        'MTBE_Gold': 'Gold',
        'PP (ICIS)_CIF NWE': 'Europe_CIF NWE',
        'PP (ICIS)_M.G.\n10ppm': 'Europe_M.G_10ppm',
        'PP (ICIS)_RBOB (NYMEX)_M1': 'RBOB (NYMEX)_M1',
        'Brent_WTI': 'Brent_WTI',
        'MOPJ_Mopag_Nap': 'MOPJ_MOPAG',
        'MOPJ_MOPS_Nap': 'MOPJ_MOPS',
        'Naphtha_Spread': 'Naphtha_Spread',
        'MG92_E Nap': 'MG92_E Nap',
        'C3_MOPJ': 'C3_MOPJ',
        'C4_MOPJ': 'C4_MOPJ',
        'Nap_Dubai': 'Nap_Dubai',
        'MG92_Nap_mops': 'MG92_Nap_MOPS',
        '95R_92R_Asia': '95R_92R_Asia',
        'M1_M2_RBOB': 'M1_M2_RBOB',
        'RBOB_Brent_m1': 'RBOB_Brent_m1',
        'RBOB_Brent_m2': 'RBOB_Brent_m2',
        'EL': 'EL_MOPJ',
        'PL': 'PL_MOPJ',
        'BZ_MOPJ': 'BZ_MOPJ',
        'TL': 'TL_MOPJ',
        'PX': 'PX_MOPJ',
        'HD': 'HD_EL',
        'LD_EL': 'LD_EL',
        'LLD': 'LLD_EL',
        'PP_PL': 'PP_PL',
        'SM_EL+BZ_Margin': 'SM_EL+BZ',
        'US_FOBK_BZ': 'US_FOBK_BZ',
        'NAP_HSFO_180': 'NAP_HSFO_180',
        'MTBE_MOPJ': 'MTBE_MOPJ',
        'MTBE_PG': 'Freight_55_PG',
        'MTBE_Maili': 'Freight_55_Maili',
        'Freight (55)_Ruwais_Yosu': 'Freight_55_Yosu',
        'Freight (55)_Daes\'': 'Freight_55_Daes',
        'Freight (55)_Chiba': 'Freight_55_Chiba',
        'Freight (55)_PG': 'Freight_75_PG',
        'Freight (55)_Maili': 'Freight_75_Maili',
        'Freight (75)_Ruwais_Yosu': 'Freight_75_Yosu',
        'Freight (75)_Daes\'': 'Freight_75_Daes',
        'Freight (75)_Chiba': 'Freight_75_Chiba',
        'Freight (75)_PG': 'Flat Rate_PG',
        'Freight (75)_Maili': 'Flat Rate_Maili',
        'Flat Rate_Ruwais_Yosu': 'Flat Rate_Yosu',
        'Flat Rate_Daes\'': 'Flat Rate_Daes',
        'Flat Rate_Chiba': 'Flat Rate_Chiba'
    }
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì—´ë§Œ ë§¤í•‘
    existing_columns = data.columns.tolist()
    final_mapping = {}
    
    for old_name, new_name in column_mapping.items():
        if old_name in existing_columns:
            final_mapping[old_name] = new_name
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì—´ë“¤ í™•ì¸
    unmapped_columns = [col for col in existing_columns if col not in column_mapping.keys()]
    if unmapped_columns:
        print(f"\n=== ë§¤í•‘ë˜ì§€ ì•Šì€ ì—´ë“¤ ===")
        for col in unmapped_columns:
            print(f"- {col}")
    
    # ì—´ ì´ë¦„ ë³€ê²½
    renamed_data = data.rename(columns=final_mapping)
    
    print(f"\n=== ì—´ ì´ë¦„ ë³€ê²½ ì™„ë£Œ ===")
    print(f"ë³€ê²½ëœ ì—´ ê°œìˆ˜: {len(final_mapping)}")
    print(f"ìµœì¢… ë°ì´í„° í˜•íƒœ: {renamed_data.shape}")
    
    return renamed_data

# process_data_250620.pyì˜ ì¶”ê°€ í•¨ìˆ˜ë“¤
def remove_missing_and_analyze(data, threshold=10):
    """
    ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ì„ ê°€ì§„ ì—´ì„ ì œê±°í•˜ê³  ë¶„ì„í•˜ëŠ” í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ê²°ì¸¡ì¹˜ ë¹„ìœ¨ ê³„ì‚°
    missing_ratio = (data.isnull().sum() / len(data)) * 100
    
    # threshold% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì‹ë³„
    high_missing_cols = missing_ratio[missing_ratio >= threshold]
    
    if len(high_missing_cols) > 0:
        logger.info(f"\n=== {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆì–´ ì œê±°ë  ì—´ ëª©ë¡ ===")
        for col, ratio in high_missing_cols.items():
            logger.info(f"- {col}: {ratio:.1f}%")
        
        # ê²°ì¸¡ì¹˜ê°€ threshold% ì´ìƒì¸ ì—´ ì œê±°
        cleaned_data = data.drop(columns=high_missing_cols.index)
        logger.info(f"\nì›ë³¸ ë°ì´í„° í˜•íƒœ: {data.shape}")
        logger.info(f"ì •ì œëœ ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
    else:
        cleaned_data = data
        logger.info(f"\nì œê±°í•  {threshold}% ì´ìƒ ê²°ì¸¡ì¹˜ ì—´ ì—†ìŒ: {data.shape}")
    
    return cleaned_data

def find_text_missings(data, text_patterns=['NOP', 'No Publication']):
    """
    ë¬¸ìì—´ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ë¥¼ ì°¾ëŠ” í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    logger.info("\n=== ë¬¸ìì—´ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ ë¶„ì„ ===")
    
    # ê° íŒ¨í„´ë³„ë¡œ ê²€ì‚¬
    for pattern in text_patterns:
        logger.info(f"\n['{pattern}' í¬í•¨ëœ ë°ì´í„° í™•ì¸]")
        
        # ëª¨ë“  ì—´ì— ëŒ€í•´ ê²€ì‚¬
        for column in data.columns:
            # ë¬¸ìì—´ ë°ì´í„°ë§Œ ê²€ì‚¬
            if data[column].dtype == 'object':
                # í•´ë‹¹ íŒ¨í„´ì´ í¬í•¨ëœ ë°ì´í„° ì°¾ê¸°
                mask = data[column].astype(str).str.contains(pattern, na=False, case=False)
                matches = data[mask]
                
                if len(matches) > 0:
                    logger.info(f"\nì—´: {column}")
                    logger.info(f"ë°œê²¬ëœ íšŸìˆ˜: {len(matches)}")

def final_clean_data_improved(data):
    """
    ìµœì¢… ë°ì´í„° ì •ì œ í•¨ìˆ˜ (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    M1_M2_RBOB ì»¬ëŸ¼ì˜ ê²°ì¸¡ì¹˜ë‚˜ 'Q' ê°’ì„ RBOB_Brent_m1 - RBOB_Brent_m2ë¡œ ê³„ì‚°í•´ì„œ ì±„ì›€
    """
    # ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„±
    cleaned_data = data.copy()
    
    # MTBE_Dow_Jones ì—´ íŠ¹ë³„ ì²˜ë¦¬
    for col in ['MTBE_Dow_Jones']:
        if col in cleaned_data.columns:
            # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
            cleaned_data[col] = pd.to_numeric(cleaned_data[col], errors='coerce')
    
    # ğŸ”§ M1_M2_RBOB ì—´ íŠ¹ë³„ ì²˜ë¦¬: ê²°ì¸¡ì¹˜ì™€ 'Q' ê°’ì„ ê³„ì‚°ìœ¼ë¡œ ì±„ìš°ê¸°
    if 'M1_M2_RBOB' in cleaned_data.columns and 'RBOB_Brent_m1' in cleaned_data.columns and 'RBOB_Brent_m2' in cleaned_data.columns:
        logger.info(f"\n=== M1_M2_RBOB ì—´ ì²˜ë¦¬ ì‹œì‘ ===")
        logger.info(f"ì²˜ë¦¬ ì „ ë°ì´í„° íƒ€ì…: {cleaned_data['M1_M2_RBOB'].dtype}")
        logger.info(f"ì²˜ë¦¬ ì „ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {cleaned_data['M1_M2_RBOB'].isnull().sum()}")
        
        # 'Q' ê°’ë“¤ê³¼ ê¸°íƒ€ ë¬¸ìì—´ ê°’ë“¤ì„ NaNìœ¼ë¡œ ë³€í™˜
        original_values = cleaned_data['M1_M2_RBOB'].copy()
        q_count = 0
        other_string_count = 0
        
        # 'Q' ê°’ ê°œìˆ˜ í™•ì¸
        if cleaned_data['M1_M2_RBOB'].dtype == 'object':
            q_mask = cleaned_data['M1_M2_RBOB'].astype(str).str.upper() == 'Q'
            q_count = q_mask.sum()
            
            # ê¸°íƒ€ ë¬¸ìì—´ ê°’ë“¤ í™•ì¸
            numeric_convertible = pd.to_numeric(cleaned_data['M1_M2_RBOB'], errors='coerce')
            string_mask = pd.isna(numeric_convertible) & cleaned_data['M1_M2_RBOB'].notna()
            other_string_count = string_mask.sum() - q_count
            
            if q_count > 0:
                logger.info(f"'Q' ê°’ {q_count}ê°œ ë°œê²¬")
            if other_string_count > 0:
                logger.info(f"ê¸°íƒ€ ë¬¸ìì—´ ê°’ {other_string_count}ê°œ ë°œê²¬")
        
        # 'Q' ê°’ë“¤ê³¼ ê¸°íƒ€ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜
        cleaned_data['M1_M2_RBOB'] = cleaned_data['M1_M2_RBOB'].replace('Q', np.nan)
        cleaned_data['M1_M2_RBOB'] = cleaned_data['M1_M2_RBOB'].replace('q', np.nan)
        
        # ë¬¸ìì—´ë¡œ ì €ì¥ëœ ìˆ«ìë“¤ì„ ì‹¤ì œ ìˆ«ìë¡œ ë³€í™˜
        cleaned_data['M1_M2_RBOB'] = pd.to_numeric(cleaned_data['M1_M2_RBOB'], errors='coerce')
        
        # ê²°ì¸¡ì¹˜ì™€ 'Q' ê°’ë“¤ì„ ê³„ì‚°ìœ¼ë¡œ ì±„ìš°ê¸°: M1_M2_RBOB = RBOB_Brent_m1 - RBOB_Brent_m2
        missing_mask = cleaned_data['M1_M2_RBOB'].isnull()
        missing_count_before = missing_mask.sum()
        
        if missing_count_before > 0:
            logger.info(f"ê²°ì¸¡ì¹˜ {missing_count_before}ê°œë¥¼ ê³„ì‚°ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤: M1_M2_RBOB = RBOB_Brent_m1 - RBOB_Brent_m2")
            
            # ê³„ì‚° ê°€ëŠ¥í•œ í–‰ë“¤ë§Œ ì„ íƒ (m1, m2 ë‘˜ ë‹¤ ìœ íš¨í•œ ê°’ì´ ìˆëŠ” ê²½ìš°)
            can_calculate = (missing_mask & 
                           cleaned_data['RBOB_Brent_m1'].notna() & 
                           cleaned_data['RBOB_Brent_m2'].notna())
            calculated_count = can_calculate.sum()
            
            if calculated_count > 0:
                # ê³„ì‚° ìˆ˜í–‰
                calculated_values = (cleaned_data.loc[can_calculate, 'RBOB_Brent_m1'] - 
                                   cleaned_data.loc[can_calculate, 'RBOB_Brent_m2'])
                
                cleaned_data.loc[can_calculate, 'M1_M2_RBOB'] = calculated_values
                logger.info(f"ì‹¤ì œë¡œ ê³„ì‚°ëœ ê°’: {calculated_count}ê°œ")
                
                # ê³„ì‚° ê²€ì¦ (ì²˜ìŒ 5ê°œ ê°’ ì¶œë ¥)
                logger.info(f"=== ê³„ì‚° ê²€ì¦ (ì²˜ìŒ 5ê°œ ê³„ì‚°ëœ ê°’) ===")
                calculated_rows = cleaned_data[can_calculate].head(5)
                for idx, row in calculated_rows.iterrows():
                    m1_val = row['RBOB_Brent_m1']
                    m2_val = row['RBOB_Brent_m2']
                    calculated_val = row['M1_M2_RBOB']
                    logger.info(f"ì¸ë±ìŠ¤ {idx}: {m1_val:.6f} - {m2_val:.6f} = {calculated_val:.6f}")
                    
            else:
                logger.warning("ê³„ì‚° ê°€ëŠ¥í•œ í–‰ì´ ì—†ìŠµë‹ˆë‹¤ (RBOB_Brent_m1 ë˜ëŠ” RBOB_Brent_m2ì— ê²°ì¸¡ì¹˜ê°€ ìˆìŒ)")
        
        # ì²˜ë¦¬ í›„ ê²°ê³¼ í™•ì¸
        missing_count_after = cleaned_data['M1_M2_RBOB'].isnull().sum()
        valid_count = cleaned_data['M1_M2_RBOB'].notna().sum()
        
        logger.info(f"\n=== M1_M2_RBOB ì—´ ì²˜ë¦¬ í›„ ===")
        logger.info(f"ë°ì´í„° íƒ€ì…: {cleaned_data['M1_M2_RBOB'].dtype}")
        logger.info(f"ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_count_after}")
        logger.info(f"ìœ íš¨ ë°ì´í„° ê°œìˆ˜: {valid_count}")
        logger.info(f"ì²˜ë¦¬ëœ ê²°ì¸¡ì¹˜ ê°œìˆ˜: {missing_count_before - missing_count_after}")
        
        if valid_count > 0:
            logger.info(f"ìµœì†Œê°’: {cleaned_data['M1_M2_RBOB'].min():.6f}")
            logger.info(f"ìµœëŒ€ê°’: {cleaned_data['M1_M2_RBOB'].max():.6f}")
            logger.info(f"í‰ê· ê°’: {cleaned_data['M1_M2_RBOB'].mean():.6f}")
    
    else:
        # í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš°
        missing_cols = []
        for col in ['M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2']:
            if col not in cleaned_data.columns:
                missing_cols.append(col)
        
        if missing_cols:
            logger.warning(f"M1_M2_RBOB ê³„ì‚°ì— í•„ìš”í•œ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_cols}")
    
    return cleaned_data

def clean_and_trim_data(data, start_date='2013-02-06'):
    """
    ë°ì´í„° ì •ì œ ë° ë‚ ì§œ ë²”ìœ„ ì¡°ì • í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ì‹œì‘ ë‚ ì§œ ì´í›„ì˜ ë°ì´í„°ë§Œ ì„ íƒ
    cleaned_data = data[data['Date'] >= pd.to_datetime(start_date)].copy()
    
    # ê¸°ë³¸ ì •ë³´ ì¶œë ¥
    logger.info(f"=== ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ ===")
    logger.info(f"ì›ë³¸ ë°ì´í„° ê¸°ê°„: {data['Date'].min()} ~ {data['Date'].max()}")
    logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° ê¸°ê°„: {cleaned_data['Date'].min()} ~ {cleaned_data['Date'].max()}")
    logger.info(f"ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {len(data)}")
    logger.info(f"ì²˜ë¦¬ëœ ë°ì´í„° í–‰ ìˆ˜: {len(cleaned_data)}")
    
    return cleaned_data

def load_and_process_data_improved(file_path, sheet_name, start_date):
    from app.data.loader import safe_read_excel
    """
    ê°œì„ ëœ ë°ì´í„° ë¡œë“œ ë° ì²˜ë¦¬ í•¨ìˆ˜
    (process_data_250620.pyì—ì„œ ê°€ì ¸ì˜¨ í•¨ìˆ˜)
    """
    # ì—´ ì´ë¦„ ìƒì„±
    column_names = create_proper_column_names(file_path, sheet_name)
    
    # ì‹¤ì œ ë°ì´í„° ì½ê¸°
    data = safe_read_excel(file_path, sheet_name=sheet_name, header=None, skiprows=3)
    data.columns = column_names
    
    # Date ì—´ ë³€í™˜
    data['Date'] = pd.to_datetime(data['Date'], errors='coerce')
    
    # ì‹œì‘ ë‚ ì§œ ì´í›„ ë°ì´í„°ë§Œ í•„í„°ë§
    data = data[data['Date'] >= start_date]
    
    # ë¶ˆí•„ìš”í•œ ì—´ ì œê±°
    data = data.loc[:, ~data.columns.str.startswith('Unnamed')]
    
    return data

def process_excel_data_complete(file_path, sheet_name='29 Nov 2010 till todate', start_date='2013-01-04'):
    """
    Excel ë°ì´í„°ë¥¼ ì™„ì „íˆ ì²˜ë¦¬í•˜ëŠ” í†µí•© í•¨ìˆ˜
    (process_data_250620.pyì˜ ë©”ì¸ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ í•¨ìˆ˜í™”)
    """
    try:
        logger.info("=== Excel ë°ì´í„° ì™„ì „ ì²˜ë¦¬ ì‹œì‘ === ğŸ“Š")
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ì²˜ë¦¬
        cleaned_data = load_and_process_data_improved(file_path, sheet_name, pd.Timestamp(start_date))
        logger.info(f"ì´ˆê¸° ë°ì´í„° í˜•íƒœ: {cleaned_data.shape}")
        
        # 2. 70% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì œê±°
        final_data = remove_high_missing_columns(cleaned_data, threshold=70)
        
        # 3. 10% ì´ìƒ ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì œê±°  
        final_cleaned_data = remove_missing_and_analyze(final_data, threshold=10)
        
        # 4. í…ìŠ¤íŠ¸ í˜•íƒœì˜ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        text_patterns = ['NOP', 'No Publication', 'N/A', 'na', 'NA', 'none', 'None', '-']
        find_text_missings(final_cleaned_data, text_patterns)
        
        # 5. í…ìŠ¤íŠ¸ ê°’ë“¤ ì •ì œ
        final_cleaned_data_v2 = clean_text_values_advanced(final_cleaned_data)
        
        # 6. ìµœì¢… ì •ì œ
        final_data_clean = final_clean_data_improved(final_cleaned_data_v2)
        
        # 7. ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
        filled_final_data = fill_missing_values_advanced(final_data_clean)
        
        # 8. ë‚ ì§œ ë²”ìœ„ ì¡°ì •
        trimmed_data = clean_and_trim_data(filled_final_data, start_date='2013-02-06')
        
        # 9. ì—´ ì´ë¦„ì„ ìµœì¢… í˜•íƒœë¡œ ë³€ê²½
        final_renamed_data = rename_columns_to_standard(trimmed_data)
        
        logger.info(f"\n=== ìµœì¢… ê²°ê³¼ ===")
        logger.info(f"ìµœì¢… ë°ì´í„° í˜•íƒœ: {final_renamed_data.shape}")
        logger.info(f"ìµœì¢… ì—´ ì´ë¦„ë“¤: {len(final_renamed_data.columns)}ê°œ")
        
        return final_renamed_data
        
    except Exception as e:
        logger.error(f"Excel ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(traceback.format_exc())
        return None

def prepare_data(train_data, val_data, sequence_length, predict_window, target_col_idx, augment=False):
    """í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ í˜•íƒœë¡œ ì¤€ë¹„"""
    X_train, y_train, prev_train = [], [], []
    for i in range(len(train_data) - sequence_length - predict_window + 1):
        seq = train_data[i:i+sequence_length]
        target = train_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx]
        prev_value = train_data[i+sequence_length-1, target_col_idx]
        X_train.append(seq)
        y_train.append(target)
        prev_train.append(prev_value)
        if augment:
            # ê°„ë‹¨í•œ ë°ì´í„° ì¦ê°•
            noise = np.random.normal(0, 0.001, seq.shape)
            aug_seq = seq + noise
            X_train.append(aug_seq)
            y_train.append(target)
            prev_train.append(prev_value)
    
    X_val, y_val, prev_val = [], [], []
    for i in range(len(val_data) - sequence_length - predict_window + 1):
        X_val.append(val_data[i:i+sequence_length])
        y_val.append(val_data[i+sequence_length:i+sequence_length+predict_window, target_col_idx])
        prev_val.append(val_data[i+sequence_length-1, target_col_idx])
    
    return map(np.array, [X_train, y_train, prev_train, X_val, y_val, prev_val])

# ë°ì´í„°ì—ì„œ í‰ì¼ ë¹ˆ ë‚ ì§œë¥¼ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
def detect_missing_weekdays_as_holidays(df, date_column='Date'):
    """
    ë°ì´í„°í”„ë ˆì„ì—ì„œ í‰ì¼(ì›”~ê¸ˆ)ì¸ë° ë°ì´í„°ê°€ ì—†ëŠ” ë‚ ì§œë“¤ì„ íœ´ì¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„
        date_column (str): ë‚ ì§œ ì»¬ëŸ¼ëª…
    
    Returns:
        set: ê°ì§€ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    if df.empty or date_column not in df.columns:
        return set()
    
    try:
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        df_dates = pd.to_datetime(df[date_column]).dt.date
        date_set = set(df_dates)
        
        # ë°ì´í„° ë²”ìœ„ì˜ ì²« ë‚ ê³¼ ë§ˆì§€ë§‰ ë‚ 
        start_date = min(df_dates)
        end_date = max(df_dates)
        
        # ì „ì²´ ê¸°ê°„ì˜ ëª¨ë“  í‰ì¼ ìƒì„±
        current_date = start_date
        missing_weekdays = set()
        
        while current_date <= end_date:
            # í‰ì¼ì¸ì§€ í™•ì¸ (ì›”ìš”ì¼=0, ì¼ìš”ì¼=6)
            if current_date.weekday() < 5:  # ì›”~ê¸ˆ
                if current_date not in date_set:
                    missing_weekdays.add(current_date.strftime('%Y-%m-%d'))
            current_date += pd.Timedelta(days=1)
        
        logger.info(f"Detected {len(missing_weekdays)} missing weekdays as potential holidays")
        if missing_weekdays:
            logger.info(f"Missing weekdays sample: {list(missing_weekdays)[:10]}")
        
        return missing_weekdays
        
    except Exception as e:
        logger.error(f"Error detecting missing weekdays: {str(e)}")
        return set()

def load_holidays_from_file(filepath=None):
    from app.data.loader import load_data_safe_holidays, load_csv_safe_with_fallback, safe_read_excel
    """
    CSV ë˜ëŠ” Excel íŒŒì¼ì—ì„œ íœ´ì¼ ëª©ë¡ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    
    Args:
        filepath (str): íœ´ì¼ ëª©ë¡ íŒŒì¼ ê²½ë¡œ, Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©
    
    Returns:
        set: íœ´ì¼ ë‚ ì§œ ì§‘í•© (YYYY-MM-DD í˜•ì‹)
    """
    # ê¸°ë³¸ íŒŒì¼ ê²½ë¡œ - holidays í´ë”ë¡œ ë³€ê²½
    if filepath is None:
        holidays_dir = Path('holidays')
        holidays_dir.mkdir(exist_ok=True)
        filepath = str(holidays_dir / 'holidays.csv')
    
    # íŒŒì¼ í™•ì¥ì í™•ì¸
    _, ext = os.path.splitext(filepath)
    
    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ ëª©ë¡ ìƒì„±
    if not os.path.exists(filepath):
        logger.warning(f"Holiday file {filepath} not found. Creating default holiday file.")
        
        # ê¸°ë³¸ 2025ë…„ ì‹±ê°€í´ ê³µíœ´ì¼
        default_holidays = [
            "2025-01-01", "2025-01-29", "2025-01-30", "2025-03-31", "2025-04-18", 
            "2025-05-01", "2025-05-12", "2025-06-07", "2025-08-09", "2025-10-20", 
            "2025-12-25", "2026-01-01"
        ]
        
        # ê¸°ë³¸ íŒŒì¼ ìƒì„±
        df = pd.DataFrame({'date': default_holidays, 'description': ['Singapore Holiday']*len(default_holidays)})
        
        if ext.lower() == '.xlsx':
            df.to_excel(filepath, index=False)
        else:
            df.to_csv(filepath, index=False)
        
        logger.info(f"Created default holiday file at {filepath}")
        return set(default_holidays)
    
    try:
        from app.data.loader import load_data_safe_holidays, load_csv_safe_with_fallback, safe_read_excel
        # íŒŒì¼ ë¡œë“œ - ë³´ì•ˆ ë¬¸ì œë¥¼ ê³ ë ¤í•œ ì•ˆì „í•œ ë¡œë”© ì‚¬ìš©
        if ext.lower() == '.xlsx':
            # Excel íŒŒì¼ì˜ ê²½ìš° xlwings ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ ì‚¬ìš©
            try:
                df = load_data_safe_holidays(filepath)
            except Exception as e:
                logger.warning(f"âš ï¸ [HOLIDAYS] xlwings loading failed, using pandas: {str(e)}")
                df = safe_read_excel(filepath)
        else:
            # CSV íŒŒì¼ ë¡œë“œ - ì•ˆì „í•œ fallback ì‚¬ìš©
            df = load_csv_safe_with_fallback(filepath)
        
        # 'date' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
        if 'date' not in df.columns:
            logger.error(f"Holiday file {filepath} does not have 'date' column")
            return set()
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format: {date_str}")
        
        logger.info(f"Loaded {len(holidays)} holidays from {filepath}")
        return holidays
        
    except Exception as e:
        logger.error(f"Error loading holiday file: {str(e)}")
        logger.error(traceback.format_exc())
        return set()

# íœ´ì¼ ì •ë³´ì™€ ë°ì´í„° ë¹ˆ ë‚ ì§œë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
def get_combined_holidays(df=None, filepath=None):
    """
    íœ´ì¼ íŒŒì¼ì˜ íœ´ì¼ê³¼ ë°ì´í„°ì—ì„œ ê°ì§€ëœ íœ´ì¼ì„ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        df (pd.DataFrame): ë°ì´í„°í”„ë ˆì„ (ë¹ˆ ë‚ ì§œ ê°ì§€ìš©)
        filepath (str): íœ´ì¼ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        set: ê²°í•©ëœ íœ´ì¼ ë‚ ì§œ ì§‘í•©
    """
    # íœ´ì¼ íŒŒì¼ì—ì„œ íœ´ì¼ ë¡œë“œ
    file_holidays = load_holidays_from_file(filepath)
    
    # ë°ì´í„°ì—ì„œ ë¹ˆ í‰ì¼ ê°ì§€
    data_holidays = set()
    if df is not None:
        data_holidays = detect_missing_weekdays_as_holidays(df)
    
    # ë‘ ì„¸íŠ¸ ê²°í•©
    combined_holidays = file_holidays.union(data_holidays)
    
    logger.info(f"Combined holidays: {len(file_holidays)} from file + {len(data_holidays)} from data = {len(combined_holidays)} total")
    
    return combined_holidays

# íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ í•¨ìˆ˜
def update_holidays(filepath=None, df=None):
    """íœ´ì¼ ì •ë³´ë¥¼ ì¬ë¡œë“œí•˜ëŠ” í•¨ìˆ˜ (ë°ì´í„° ë¹ˆ ë‚ ì§œ í¬í•¨)"""
    global holidays
    holidays = get_combined_holidays(df, filepath)
    return holidays

def update_holidays_safe(filepath=None, df=None):
    """
    ì•ˆì „í•œ íœ´ì¼ ì •ë³´ ì—…ë°ì´íŠ¸ í•¨ìˆ˜ - xlwings ë³´ì•ˆ ìš°íšŒ ê¸°ëŠ¥ í¬í•¨
    """
    global holidays
    
    # XLWINGS_AVAILABLEì„ í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import
    from app.data.loader import XLWINGS_AVAILABLE
    
    try:
        # ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ íœ´ì¼ ë¡œë“œ ì‹œë„
        holidays = get_combined_holidays(df, filepath)
        logger.info(f"âœ… [HOLIDAY_SAFE] Standard holiday loading successful: {len(holidays)} holidays")
        return holidays
        
    except (PermissionError, OSError, pd.errors.ExcelFileError) as e:
        # íŒŒì¼ ì ‘ê·¼ ì˜¤ë¥˜ ì‹œ xlwingsë¡œ ëŒ€ì²´ ì‹œë„ (Excel íŒŒì¼ë§Œ)
        if filepath and filepath.endswith(('.xlsx', '.xls')) and XLWINGS_AVAILABLE:
            logger.warning(f"âš ï¸ [HOLIDAY_BYPASS] Standard holiday loading failed: {str(e)}")
            logger.info("ğŸ”“ [HOLIDAY_BYPASS] Attempting xlwings bypass for holiday file...")
            
            try:
                # xlwingsë¡œ íœ´ì¼ íŒŒì¼ ë¡œë“œ
                file_holidays = load_holidays_from_file_safe(filepath)
                
                # ë°ì´í„°ì—ì„œ ë¹ˆ í‰ì¼ ê°ì§€ (ê¸°ì¡´ ë°©ì‹)
                data_holidays = set()
                if df is not None:
                    data_holidays = detect_missing_weekdays_as_holidays(df)
                
                # ë‘ ì„¸íŠ¸ ê²°í•©
                holidays = file_holidays.union(data_holidays)
                
                logger.info(f"âœ… [HOLIDAY_BYPASS] xlwings holiday loading successful: {len(file_holidays)} from file + {len(data_holidays)} from data = {len(holidays)} total")
                return holidays
                
            except Exception as xlwings_error:
                logger.error(f"âŒ [HOLIDAY_BYPASS] xlwings holiday loading also failed: {str(xlwings_error)}")
                # ê¸°ë³¸ íœ´ì¼ë¡œ í´ë°±
                logger.info("ğŸ”„ [HOLIDAY_FALLBACK] Using default holidays")
                holidays = load_holidays_from_file()  # ê¸°ë³¸ íŒŒì¼ì—ì„œ ë¡œë“œ
                return holidays
        else:
            # xlwingsë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë©´ ê¸°ë³¸ íœ´ì¼ë¡œ í´ë°±
            logger.warning(f"âš ï¸ [HOLIDAY_FALLBACK] Cannot use xlwings, using default holidays: {str(e)}")
            holidays = load_holidays_from_file()  # ê¸°ë³¸ íŒŒì¼ì—ì„œ ë¡œë“œ
            return holidays

def load_holidays_from_file_safe(filepath):
    """
    xlwingsë¥¼ ì‚¬ìš©í•œ ì•ˆì „í•œ íœ´ì¼ íŒŒì¼ ë¡œë”© (CSV ë° Excel ì§€ì›)
    """
    import os  # í•„ìš”í•œ ê²½ìš° ì¶”ê°€
    
    try:
        # í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from app.data.loader import load_data_safe_holidays, load_csv_safe_with_fallback
        # íŒŒì¼ í™•ì¥ì í™•ì¸í•˜ì—¬ ì ì ˆí•œ xlwings í•¨ìˆ˜ ì‚¬ìš©
        _, ext = os.path.splitext(filepath)
        
        if ext.lower() in ['.xlsx', '.xls']:
            # Excel íŒŒì¼ì˜ ê²½ìš° ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©
            df = load_data_safe_holidays(filepath)
        else:
            # CSV íŒŒì¼ì˜ ê²½ìš° ì•ˆì „í•œ fallback ì‚¬ìš©
            logger.info(f"ğŸ”“ [HOLIDAY_CSV_SAFE] Loading CSV holiday file with xlwings: {os.path.basename(filepath)}")
            df = load_csv_safe_with_fallback(filepath)
            
            # CSVì˜ ê²½ìš° ì»¬ëŸ¼ëª… ì •ê·œí™” (Excelì€ load_data_safe_holidaysì—ì„œ ì²˜ë¦¬ë¨)
            df.columns = df.columns.str.lower()
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            if 'date' not in df.columns:
                first_col = df.columns[0]
                df = df.rename(columns={first_col: 'date'})
                logger.info(f"ğŸ”„ [HOLIDAY_CSV_SAFE] Renamed '{first_col}' to 'date'")
            
            # description ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
            if 'description' not in df.columns:
                df['description'] = 'Holiday'
                logger.info(f"â• [HOLIDAY_CSV_SAFE] Added default 'description' column")
        
        # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™”
        holidays_set = set()
        for date_str in df['date']:
            try:
                date = pd.to_datetime(date_str)
                holidays_set.add(date.strftime('%Y-%m-%d'))
            except:
                logger.warning(f"Invalid date format in xlwings holiday data: {date_str}")
        
        logger.info(f"ğŸ”“ [HOLIDAY_XLWINGS_SAFE] Loaded {len(holidays_set)} holidays with xlwings ({ext.lower()} file)")
        return holidays_set
        
    except Exception as e:
        logger.error(f"âŒ [HOLIDAY_XLWINGS_SAFE] xlwings holiday loading failed: {str(e)}")
        raise e
    
# ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜ (preprocessorì— ë‘ëŠ” ê²ƒì´ ì í•©)
variable_groups = { # app_rev.pyì—ì„œ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜´
    'crude_oil': ['WTI', 'Brent', 'Dubai', 'Brent_Singapore'],
    'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
    'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
    'lpg': ['C3_LPG', 'C4_LPG'],
    'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
    'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
    'spread': ['biweekly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
    'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
    'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
    'economics': ['Dow_Jones', 'Euro', 'Gold', 'Exchange'],
    'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
    'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
    'Flat Rate_Chiba'],
    'ETF': ['DIG', 'DUG', 'IYE', 'VDE', 'XLE']
}

def calculate_group_vif(df, variables):
    """ê·¸ë£¹ ë‚´ ë³€ìˆ˜ë“¤ì˜ VIF ê³„ì‚°"""
    # ë³€ìˆ˜ê°€ í•œ ê°œ ì´í•˜ë©´ VIF ê³„ì‚° ë¶ˆê°€
    if len(variables) <= 1:
        return pd.DataFrame({
            "Feature": variables,
            "VIF": [1.0] * len(variables)
        })
    
    # ëª¨ë“  ë³€ìˆ˜ê°€ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    available_vars = [var for var in variables if var in df.columns]
    if len(available_vars) <= 1:
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [1.0] * len(available_vars)
        })
    
    try:
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        
        vif_data = pd.DataFrame()
        vif_data["Feature"] = available_vars
        vif_data["VIF"] = [variance_inflation_factor(df[available_vars].values, i) 
                          for i in range(len(available_vars))]
        return vif_data.sort_values('VIF', ascending=False)
    except Exception as e:
        logger.error(f"Error calculating VIF: {str(e)}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
        return pd.DataFrame({
            "Feature": available_vars,
            "VIF": [float('nan')] * len(available_vars)
        })

def analyze_group_correlations(df, variable_groups, target_col='MOPJ'):
    """ê·¸ë£¹ë³„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    logger.info("Analyzing correlations for each group:")
    group_correlations = {}
    
    for group_name, variables in variable_groups.items():
        # ê° ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ ë°ì´í„°í”„ë ˆì„ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
            
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        group_correlations[group_name] = correlations
        
        logger.info(f"\n{group_name} group correlations with {target_col}:")
        logger.info(str(correlations))
    
    return group_correlations

def select_features_from_groups(df, variable_groups, target_col='MOPJ', vif_threshold=50.0, corr_threshold=0.8):
    """ê° ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ë³€ìˆ˜ ì„ íƒ"""
    selected_features = []
    selection_process = {}
    
    logger.info(f"\nCorrelation threshold: {corr_threshold}")
    
    for group_name, variables in variable_groups.items():
        logger.info(f"\nProcessing {group_name} group:")
        
        # í•´ë‹¹ ê·¸ë£¹ì˜ ë³€ìˆ˜ë“¤ì´ dfì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        available_vars = [var for var in variables if var in df.columns]
        if not available_vars:
            logger.warning(f"Warning: No variables from {group_name} group found in dataframe")
            continue
            
        if target_col not in df.columns:
            logger.warning(f"Warning: Target column {target_col} not found in dataframe")
            continue
        
        # ê·¸ë£¹ ë‚´ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = df[available_vars].corrwith(df[target_col]).abs().sort_values(ascending=False)
        logger.info(f"\nCorrelations with {target_col}:")
        logger.info(str(correlations))
        
        # ìƒê´€ê´€ê³„ê°€ ì„ê³„ê°’ ì´ìƒì¸ ë³€ìˆ˜ë§Œ í•„í„°ë§
        high_corr_vars = correlations[correlations >= corr_threshold].index.tolist()
        
        if not high_corr_vars:
            logger.warning(f"Warning: No variables in {group_name} group meet the correlation threshold of {corr_threshold}")
            continue
        
        # ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ” ë³€ìˆ˜ë“¤ì— ëŒ€í•´ VIF ê³„ì‚°
        if len(high_corr_vars) > 1:
            vif_data = calculate_group_vif(df[high_corr_vars], high_corr_vars)
            logger.info(f"\nVIF values for {group_name} group (high correlation vars only):")
            logger.info(str(vif_data))
            
            # VIF ê¸°ì¤€ ì ìš©í•˜ì—¬ ë‹¤ì¤‘ê³µì„ ì„± ë‚®ì€ ë³€ìˆ˜ ì„ íƒ
            low_vif_vars = vif_data[vif_data['VIF'] < vif_threshold]['Feature'].tolist()
            
            if low_vif_vars:
                # ë‚®ì€ VIF ë³€ìˆ˜ë“¤ ì¤‘ ìƒê´€ê´€ê³„ê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ ì„ íƒ
                for var in correlations.index:
                    if var in low_vif_vars:
                        selected_var = var
                        break
                else:
                    selected_var = high_corr_vars[0]
            else:
                selected_var = high_corr_vars[0]
        else:
            selected_var = high_corr_vars[0]
            vif_data = pd.DataFrame({"Feature": [selected_var], "VIF": [1.0]})
        
        # ì„ íƒëœ ë³€ìˆ˜ê°€ ìƒê´€ê´€ê³„ ì„ê³„ê°’ì„ ë§Œì¡±í•˜ëŠ”ì§€ í™•ì¸ (ì•ˆì „ì¥ì¹˜)
        if correlations[selected_var] >= corr_threshold:
            selected_features.append(selected_var)
            
            selection_process[group_name] = {
                'selected_variable': selected_var,
                'correlation': correlations[selected_var],
                'all_correlations': correlations.to_dict(),
                'vif_data': vif_data.to_dict() if not vif_data.empty else {},
                'high_corr_vars': high_corr_vars
            }
            
            logger.info(f"\nSelected variable from {group_name}: {selected_var} (corr: {correlations[selected_var]:.4f})")
        else:
            logger.info(f"\nNo variable selected from {group_name}: correlation threshold not met")
    
    # ìƒê´€ê´€ê³„ ê¸°ì¤€ ì¬í™•ì¸ (ìµœì¢… ì•ˆì „ì¥ì¹˜)
    final_features = []
    for feature in selected_features:
        corr = abs(df[feature].corr(df[target_col]))
        if corr >= corr_threshold:
            final_features.append(feature)
            logger.info(f"Final selection: {feature} (corr: {corr:.4f})")
        else:
            logger.info(f"Excluded: {feature} (corr: {corr:.4f}) - below threshold")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
    if target_col not in final_features:
        final_features.append(target_col)
        logger.info(f"Added target column: {target_col}")
    
    # ìµœì†Œ íŠ¹ì„± ìˆ˜ í™•ì¸
    if len(final_features) < 3:
        logger.warning(f"Selected features ({len(final_features)}) < 3, lowering threshold to 0.5")
        return select_features_from_groups(df, variable_groups, target_col, vif_threshold, 0.5)
    
    return final_features, selection_process
