import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import optuna # optimize_hyperparameters_semimonthly_kfoldì—ì„œ ì‚¬ìš©
import logging
from torch.utils.data import DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split # prepare_dataì—ì„œ ì‚¬ìš©

from app.utils.file_utils import set_seed
from app.prediction.metrics import calculate_f1_score, calculate_direction_accuracy, calculate_mape, calculate_direction_weighted_score
from app.models.loss_functions import DirectionalLoss
from app.data.preprocessor import prepare_data
from app.data.loader import load_data
from app.data.cache_manager import get_file_cache_dirs, find_compatible_hyperparameters
from app.core.gpu_manager import log_device_usage, check_gpu_availability
from app.core import DEFAULT_DEVICE, CUDA_AVAILABLE
from app.utils.date_utils import format_date, get_semimonthly_period
from app.core.state_manager import prediction_state

import traceback
import json
import os

DEFAULT_DEVICE, CUDA_AVAILABLE = check_gpu_availability()

# ë°ì´í„° ë¡œë”ì˜ ì›Œì»¤ ì‹œë“œ ê³ ì •ì„ ìœ„í•œ í•¨ìˆ˜
def seed_worker(worker_id): # ì—¬ê¸°ì„œ seed_worker ì •ì˜
    set_seed(np.random.get_seed_value() + worker_id) # numpy seedë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘ì„± í™•ë³´
g = torch.Generator() # DataLoaderì˜ generatorëŠ” ì—¬ê¸°ì— ì •ì˜
SEED = 42
g.manual_seed(SEED)

logger = logging.getLogger(__name__)

# ê°œì„ ëœ LSTM ì˜ˆì¸¡ ëª¨ë¸
class ImprovedLSTMPredictor(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, dropout=0.2, output_size=23):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # hidden_sizeë¥¼ 8ì˜ ë°°ìˆ˜ë¡œ ì¡°ì •
        self.adjusted_hidden = (hidden_size // 8) * 8
        if self.adjusted_hidden < 32:
            self.adjusted_hidden = 32
        
        # LSTM dropout ì„¤ì •
        self.lstm_dropout = 0.0 if num_layers == 1 else dropout
        
        # ê³„ì¸µì  LSTM êµ¬ì¡°
        self.lstm_layers = nn.ModuleList([
            nn.LSTM(
                input_size=input_size if i == 0 else self.adjusted_hidden,
                hidden_size=self.adjusted_hidden,
                num_layers=1,
                batch_first=True
            ) for i in range(num_layers)
        ])
        
        # ë“€ì–¼ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.temporal_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )
        
        self.feature_attention = nn.MultiheadAttention(
            self.adjusted_hidden,
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # Layer Normalization
        self.layer_norms = nn.ModuleList([
            nn.LayerNorm(self.adjusted_hidden) for _ in range(num_layers)
        ])
        
        self.final_layer_norm = nn.LayerNorm(self.adjusted_hidden)
        
        # Dropout ë ˆì´ì–´
        self.dropout_layer = nn.Dropout(dropout)
        
        # ì´ì „ ê°’ ì •ë³´ë¥¼ ê²°í•©í•˜ê¸° ìœ„í•œ ë ˆì´ì–´
        self.prev_value_encoder = nn.Sequential(
            nn.Linear(1, self.adjusted_hidden // 4),
            nn.ReLU(),
            nn.Linear(self.adjusted_hidden // 4, self.adjusted_hidden)
        )
        
        # ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œì„ ìœ„í•œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv_layers = nn.Sequential(
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(self.adjusted_hidden, self.adjusted_hidden, kernel_size=3, padding=1),
            nn.ReLU()
        )
        
        # ì¶œë ¥ ë ˆì´ì–´ - ê³„ì¸µì  êµ¬ì¡°
        self.output_layers = nn.ModuleList([
            nn.Linear(self.adjusted_hidden, self.adjusted_hidden // 2),
            nn.Linear(self.adjusted_hidden // 2, self.adjusted_hidden // 4),
            nn.Linear(self.adjusted_hidden // 4, output_size)
        ])
        
        # ì”ì°¨ ì—°ê²°ì„ ìœ„í•œ í”„ë¡œì ì…˜ ë ˆì´ì–´
        self.residual_proj = nn.Linear(self.adjusted_hidden, output_size)
        
    def forward(self, x, prev_value=None, return_attention=False):
        batch_size = x.size(0)
        
        # ê³„ì¸µì  LSTM ì²˜ë¦¬
        lstm_out = x
        skip_connections = []
        
        for i, (lstm, layer_norm) in enumerate(zip(self.lstm_layers, self.layer_norms)):
            lstm_out, _ = lstm(lstm_out)
            lstm_out = layer_norm(lstm_out)
            lstm_out = self.dropout_layer(lstm_out)
            skip_connections.append(lstm_out)
        
        # ì‹œê°„ì  ì–´í…ì…˜
        temporal_context, temporal_weights = self.temporal_attention(lstm_out, lstm_out, lstm_out)
        temporal_context = self.dropout_layer(temporal_context)
        
        # íŠ¹ì§• ì–´í…ì…˜
        # íŠ¹ì§• ì°¨ì›ìœ¼ë¡œ ë³€í™˜ (B, L, H) -> (B, H, L)
        feature_input = lstm_out.transpose(1, 2)
        feature_input = self.conv_layers(feature_input)
        feature_input = feature_input.transpose(1, 2)
        
        feature_context, feature_weights = self.feature_attention(feature_input, feature_input, feature_input)
        feature_context = self.dropout_layer(feature_context)
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
        combined_context = temporal_context + feature_context
        for skip in skip_connections:
            combined_context = combined_context + skip
        
        combined_context = self.final_layer_norm(combined_context)
        
        # ì´ì „ ê°’ ì •ë³´ ì²˜ë¦¬
        if prev_value is not None:
            prev_value = prev_value.unsqueeze(1) if len(prev_value.shape) == 1 else prev_value
            prev_encoded = self.prev_value_encoder(prev_value)
            combined_context = combined_context + prev_encoded.unsqueeze(1)
        
        # ìµœì¢… íŠ¹ì§• ì¶”ì¶œ (ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤)
        final_features = combined_context[:, -1, :]
        
        # ê³„ì¸µì  ì¶œë ¥ ì²˜ë¦¬
        out = final_features
        residual = self.residual_proj(final_features)
        
        for i, layer in enumerate(self.output_layers):
            out = layer(out)
            if i < len(self.output_layers) - 1:
                out = F.relu(out)
                out = self.dropout_layer(out)
        
        # ì”ì°¨ ì—°ê²° ì¶”ê°€
        out = out + residual
        
        if return_attention:
            attention_weights = {
                'temporal_weights': temporal_weights,
                'feature_weights': feature_weights
            }
            return out, attention_weights
        
        return out
        
    def get_attention_maps(self, x, prev_value=None):
        """ì–´í…ì…˜ ê°€ì¤‘ì¹˜ ë§µì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        with torch.no_grad():
            # forward ë©”ì„œë“œì— return_attention=True ì „ë‹¬
            _, attention_weights = self.forward(x, prev_value, return_attention=True)
            
            # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ í‰ê·  ê³„ì‚° (multi-head -> single map)
            temporal_weights = attention_weights['temporal_weights'].mean(dim=1)  # í—¤ë“œ í‰ê· 
            feature_weights = attention_weights['feature_weights'].mean(dim=1)    # í—¤ë“œ í‰ê· 
            
            return {
                'temporal_weights': temporal_weights.cpu().numpy(),
                'feature_weights': feature_weights.cpu().numpy()
            }

# TimeSeriesDataset ë° í‰ê°€ ë©”íŠ¸ë¦­ìŠ¤
class TimeSeriesDataset(Dataset):
    def __init__(self, X, y, device, prev_values=None):
        if isinstance(X, torch.Tensor):
            self.X = X
            self.y = y
        else:
            self.X = torch.FloatTensor(X).to(device)
            self.y = torch.FloatTensor(y).to(device)
        
        if prev_values is not None:
            if isinstance(prev_values, torch.Tensor):
                self.prev_values = prev_values
            else:
                self.prev_values = torch.FloatTensor(prev_values).to(device)
        else:
            self.prev_values = None
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        if self.prev_values is not None:
            return self.X[idx], self.y[idx], self.prev_values[idx]
        return self.X[idx], self.y[idx]

def train_model(features, target_col, current_date, historical_data, device, params):
    """LSTM ëª¨ë¸ í•™ìŠµ"""
    try:
        # ì¼ê´€ëœ í•™ìŠµ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        # ë””ë°”ì´ìŠ¤ ì‚¬ìš© ì •ë³´ ë¡œê¹…
        log_device_usage(device, "LSTM ëª¨ë¸ í•™ìŠµ ì‹œì‘")
        
        # íŠ¹ì„± ì´ë¦„ í™•ì¸
        if target_col not in features:
            features.append(target_col)
        
        # í•™ìŠµ ë°ì´í„° ì¤€ë¹„ (í˜„ì¬ ë‚ ì§œê¹Œì§€)
        train_df = historical_data[features].copy()
        target_col_idx = train_df.columns.get_loc(target_col)
        
        # ìŠ¤ì¼€ì¼ë§
        scaler = StandardScaler()
        train_data = scaler.fit_transform(train_df)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        sequence_length = params.get('sequence_length', 20)
        hidden_size = params.get('hidden_size', 128)
        num_layers = params.get('num_layers', 2)
        dropout = params.get('dropout', 0.2)
        learning_rate = params.get('learning_rate', 0.001)
        num_epochs = params.get('num_epochs', 100)
        batch_size = params.get('batch_size', 32)
        alpha = params.get('loss_alpha', 0.7)  # DirectionalLoss alpha
        beta = params.get('loss_beta', 0.2)    # DirectionalLoss beta
        patience = params.get('patience', 20)   # ì¡°ê¸° ì¢…ë£Œ ì¸ë‚´
        predict_window = params.get('predict_window', 23)  # ì˜ˆì¸¡ ê¸°ê°„
        
        # 80/20 ë¶„í•  (ì—°ëŒ€ìˆœ)
        train_size = int(len(train_data) * 0.8)
        train_set = train_data[:train_size]
        val_set = train_data[train_size:]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„
        X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
            train_set, val_set, sequence_length, predict_window, target_col_idx
        )
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(X_train) < batch_size:
            batch_size = max(1, len(X_train) // 2)
            logger.warning(f"ë°°ì¹˜ í¬ê¸°ê°€ ë°ì´í„° í¬ê¸°ë³´ë‹¤ ì»¤ì„œ ì¡°ì •: {batch_size} (ë°ì´í„°: {len(X_train)})")
        
        if len(X_train) == 0 or len(X_val) == 0:
            raise ValueError("Insufficient data for training")
        
        logger.info(f"ğŸ¯ ì‚¬ìš©í•  ë°°ì¹˜ í¬ê¸°: {batch_size}")
        
        # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± (CPUì—ì„œ ìƒì„±, í•™ìŠµ ì‹œ GPUë¡œ ì´ë™)
        train_dataset = TimeSeriesDataset(X_train, y_train, torch.device('cpu'), prev_train)
        
        # GPU í™œìš©ë¥  ìµœì í™”ë¥¼ ìœ„í•œ DataLoader ì„¤ì •
        num_workers = 0 if device.type == 'cuda' else 2  # CUDAì—ì„œëŠ” ë©€í‹°í”„ë¡œì„¸ì‹± ë¹„í™œì„±í™”
        pin_memory = device.type == 'cuda'  # GPU ì‚¬ìš© ì‹œ pin_memory í™œì„±í™”
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=True,
            worker_init_fn=seed_worker,
            generator=g,
            num_workers=num_workers,
            pin_memory=pin_memory,
            persistent_workers=False if num_workers == 0 else True
        )
        
        val_dataset = TimeSeriesDataset(X_val, y_val, torch.device('cpu'), prev_val)
        val_loader = DataLoader(
            val_dataset, 
            batch_size=min(batch_size, len(X_val)),  # ê²€ì¦ì—ì„œë„ ë°°ì¹˜ í¬ê¸° ìµœì í™”
            shuffle=False,
            num_workers=num_workers,
            pin_memory=pin_memory
        )
        
        logger.info(f"ğŸ”§ DataLoader ì„¤ì •: workers={num_workers}, pin_memory={pin_memory}, train_batch={batch_size}, val_batch={min(batch_size, len(X_val))}")
        
        # ëª¨ë¸ ìƒì„±
        logger.info("ğŸ“ˆ ImprovedLSTMPredictor ì‚¬ìš©")
        model = ImprovedLSTMPredictor(
            input_size=train_data.shape[1],
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout,
            output_size=predict_window
        ).to(device)
        
        # ëª¨ë¸ì´ GPUì— ì˜¬ë¼ê°”ëŠ”ì§€ í™•ì¸
        model_device = next(model.parameters()).device
        logger.info(f"ğŸ¤– ImprovedLSTM ëª¨ë¸ì´ {model_device}ì— ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤")
        log_device_usage(model_device, "ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
        logger.info(f"ğŸ“ˆ DirectionalLoss ì‚¬ìš©: alpha={alpha}, beta={beta}")
        criterion = DirectionalLoss(alpha=alpha, beta=beta)
        
        # ìµœì í™”ê¸° ë° ìŠ¤ì¼€ì¤„ëŸ¬
        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5,
            patience=patience//2
        )
        
        # í•™ìŠµ
        best_val_loss = float('inf')
        best_model_state = None
        patience_counter = 0
        
        # GPU ìµœì í™” ì„¤ì •
        if device.type == 'cuda':
            torch.backends.cudnn.benchmark = True  # ì…ë ¥ í¬ê¸°ê°€ ì¼ì •í•  ë•Œ ì„±ëŠ¥ í–¥ìƒ
            torch.cuda.empty_cache()  # ìºì‹œ ì •ë¦¬
            
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì¤‘")
        
        for epoch in range(num_epochs):
            # í•™ìŠµ ëª¨ë“œ
            model.train()
            train_loss = 0
            batch_count = 0
            
            for X_batch, y_batch, prev_batch in train_loader:
                optimizer.zero_grad()
                
                # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                
                # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                y_pred = model(X_batch, prev_batch)
                loss = criterion(y_pred, y_batch, prev_batch)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                train_loss += loss.item()
                batch_count += 1
                
            # ì²« ë²ˆì§¸ ì—í¬í¬ì™€ ì£¼ê¸°ì ìœ¼ë¡œ GPU ìƒíƒœ ë¡œê¹…
            if epoch == 0 or (epoch + 1) % 10 == 0:
                log_device_usage(device, f"ì—í¬í¬ {epoch+1}/{num_epochs}")
            
            # ê²€ì¦ ëª¨ë“œ
            model.eval()
            val_loss = 0
            
            with torch.no_grad():
                for X_batch, y_batch, prev_batch in val_loader:
                    # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                    X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                    y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                    prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                    
                    # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                    y_pred = model(X_batch, prev_batch)
                    loss = criterion(y_pred, y_batch, prev_batch)
                    
                    val_loss += loss.item()
                
                val_loss /= len(val_loader)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                scheduler.step(val_loss)
                
                # ëª¨ë¸ ì €ì¥ (ìµœì )
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    best_model_state = model.state_dict().copy()
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # ì¡°ê¸° ì¢…ë£Œ
                if patience_counter >= patience:
                    logger.info(f"Early stopping at epoch {epoch+1}")
                    break
        
        # ìµœì  ëª¨ë¸ ë³µì›
        if best_model_state is not None:
            model.load_state_dict(best_model_state)
        
        logger.info(f"Model training completed with best validation loss: {best_val_loss:.4f}")
        
        # í•™ìŠµ ì™„ë£Œ í›„ GPU ìƒíƒœ í™•ì¸
        log_device_usage(device, "ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        
        # GPU ìºì‹œ ì •ë¦¬
        if device.type == 'cuda':
            torch.cuda.empty_cache()
            logger.info("ğŸ§¹ GPU ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
        
        # ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, íŒŒë¼ë¯¸í„° ë°˜í™˜
        return model, scaler, target_col_idx
    
    except Exception as e:
        logger.error(f"Error in model training: {str(e)}")
        logger.error(traceback.format_exc())
        raise e
    
def optimize_hyperparameters_semimonthly_kfold(train_data, input_size, target_col_idx, device, current_period, file_path=None, n_trials=30, k_folds=10, use_cache=True):
    """
    ì‹œê³„ì—´ K-fold êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ì›”ë³„ ë°ì´í„°ì— ëŒ€í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” (Purchase_decision_5days.py ë°©ì‹)
    """
    # ì¼ê´€ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
    set_seed()
    
    logger.info(f"\n===== {current_period} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (ì‹œê³„ì—´ {k_folds}-fold êµì°¨ ê²€ì¦) =====")
    
    # ğŸ”§ í™•ì¥ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ ë¡œì§ - ê¸°ì¡´ íŒŒì¼ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë„ íƒìƒ‰
    file_cache_dir = get_file_cache_dirs(file_path)['models']
    cache_file = os.path.join(file_cache_dir, f"hyperparams_kfold_{current_period.replace('-', '_')}.json")
    logger.info(f"ğŸ“ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ íŒŒì¼: {cache_file}")
    
    # models ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(file_cache_dir, exist_ok=True)
    
    # ğŸ” 1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìºì‹œ í™•ì¸
    if use_cache:
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r') as f:
                    cached_params = json.load(f)
                logger.info(f"âœ… [{current_period}] í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ ì™„ë£Œ")
                return cached_params
            except Exception as e:
                logger.error(f"ìºì‹œ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
    
    # ğŸ” 2ë‹¨ê³„: ë°ì´í„° í™•ì¥ ì‹œ ê¸°ì¡´ íŒŒì¼ì˜ ë™ì¼ ê¸°ê°„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
    if use_cache:
        logger.info(f"ğŸ” [{current_period}] í˜„ì¬ íŒŒì¼ì— ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ íŒŒì¼ì—ì„œ ë™ì¼ ê¸°ê°„ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤...")
        compatible_hyperparams = find_compatible_hyperparameters(file_path, current_period)
        if compatible_hyperparams:
            logger.info(f"âœ… [{current_period}] ë™ì¼ ê¸°ê°„ì˜ í˜¸í™˜ ê°€ëŠ¥í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
            logger.info(f"    ğŸ“ Source: {compatible_hyperparams['source_file']}")
            logger.info(f"    ğŸ“Š Extension info: {compatible_hyperparams['extension_info']}")
            
            # ğŸ”§ ìˆ˜ì •: ìºì‹œ ì €ì¥ì— ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
            try:
                with open(cache_file, 'w') as f:
                    json.dump(compatible_hyperparams['hyperparams'], f, indent=2)
                logger.info(f"ğŸ’¾ [{current_period}] í˜¸í™˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ í˜„ì¬ íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"âš ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì €ì¥ ì‹¤íŒ¨, í•˜ì§€ë§Œ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {str(e)}")
            
            # ğŸ”‘ í•µì‹¬: ì €ì¥ ì„±ê³µ/ì‹¤íŒ¨ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
            logger.info(f"ğŸš€ [{current_period}] ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return compatible_hyperparams['hyperparams']
                
        logger.info(f"ğŸ†• [{current_period}] ë™ì¼ ê¸°ê°„ì˜ ê¸°ì¡´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìµœì í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    
            # ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ (ìµœì í™” ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    default_params = {
        'sequence_length': 46,
        'hidden_size': 224,
        'num_layers': 6,
        'dropout': 0.318369281841675,
        'batch_size': 49,
        'learning_rate': 0.0009452489017042499,
        'num_epochs': 72,
        'loss_alpha': 0.7,  # DirectionalLoss alpha
        'loss_beta': 0.2,   # DirectionalLoss beta
        'patience': 14
    }
    
    # ë°ì´í„° ê¸¸ì´ í™•ì¸ - ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë°”ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
    MIN_DATA_SIZE = 100
    if len(train_data) < MIN_DATA_SIZE:
        logger.warning(f"í›ˆë ¨ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ ({len(train_data)} ë°ì´í„° í¬ì¸íŠ¸ < {MIN_DATA_SIZE}). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # K-fold ë¶„í•  ë¡œì§
    predict_window = 23  # ì˜ˆì¸¡ ìœˆë„ìš° í¬ê¸°
    min_fold_size = 20 + predict_window + 5  # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ + ì˜ˆì¸¡ ìœˆë„ìš° + ì—¬ìœ 
    max_possible_folds = len(train_data) // min_fold_size
    
    if max_possible_folds < 2:
        logger.warning(f"ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ k-foldë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê°€ëŠ¥í•œ fold: {max_possible_folds} < 2). ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return default_params
    
    # ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ fold ìˆ˜ ì¡°ì •
    k_folds = min(k_folds, max_possible_folds)
    fold_size = len(train_data) // (k_folds + 1)  # +1ì€ ì˜ˆì¸¡ ìœˆë„ìš°ë¥¼ ìœ„í•œ ì¶”ê°€ ë¶€ë¶„

    logger.info(f"ë°ì´í„° í¬ê¸°: {len(train_data)}, Fold ìˆ˜: {k_folds}, ê° Fold í¬ê¸°: {fold_size}")

    # fold ë¶„í• ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„±
    folds = []
    for i in range(k_folds):
        test_start = i * fold_size
        test_end = (i + 1) * fold_size
        
        train_indices = list(range(0, test_start)) + list(range(test_end, len(train_data)))
        test_indices = list(range(test_start, test_end))
        
        folds.append((train_indices, test_indices))
    
    # Optuna ëª©ì  í•¨ìˆ˜ ì •ì˜
    def objective(trial):
        # ì¼ê´€ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed(SEED + trial.number)  # trialë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œë¡œ ë‹¤ì–‘ì„± ë³´ì¥í•˜ë©´ì„œë„ ì¬í˜„ ê°€ëŠ¥
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ìˆ˜ì • - ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœëŒ€ê°’ ì œí•œ
        max_seq_length = min(fold_size - predict_window - 5, 60)
        
        # ìµœì†Œ ì‹œí€€ìŠ¤ ê¸¸ì´ë„ ì œí•œ
        min_seq_length = min(10, max_seq_length)
        
        if max_seq_length <= min_seq_length:
            logger.warning(f"ì‹œí€€ìŠ¤ ê¸¸ì´ ë²”ìœ„ê°€ ë„ˆë¬´ ì œí•œì ì…ë‹ˆë‹¤ (min={min_seq_length}, max={max_seq_length}). í•´ë‹¹ trial ê±´ë„ˆë›°ê¸°.")
            return float('inf')
        
        params = {
            'sequence_length': trial.suggest_int('sequence_length', min_seq_length, max_seq_length),
            'hidden_size': trial.suggest_int('hidden_size', 32, 256, step=8),
            'num_layers': trial.suggest_int('num_layers', 2, 6),
            'dropout': trial.suggest_float('dropout', 0.1, 0.5),
            'batch_size': trial.suggest_int('batch_size', 16, min(128, fold_size)),
            'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),
            'num_epochs': trial.suggest_int('num_epochs', 50, 200),
            'patience': trial.suggest_int('patience', 10, 30),
            'loss_alpha': trial.suggest_float('loss_alpha', 0.5, 0.9),
            'loss_beta': trial.suggest_float('loss_beta', 0.1, 0.3)
        }
        
        # loss_gamma ì œê±°ë¨ - ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ DirectionalLossë§Œ ì‚¬ìš©
        
        # K-fold êµì°¨ ê²€ì¦
        fold_losses = []
        valid_fold_count = 0
        
        for fold_idx, (train_indices, test_indices) in enumerate(folds):
            try:
                # ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ fold í¬ê¸°ë³´ë‹¤ í¬ë©´ ê±´ë„ˆë›°ê¸°
                if params['sequence_length'] >= len(test_indices):
                    logger.warning(f"Fold {fold_idx+1}: ì‹œí€€ìŠ¤ ê¸¸ì´({params['sequence_length']})ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°({len(test_indices)})ë³´ë‹¤ í½ë‹ˆë‹¤.")
                    continue
                
                # foldë³„ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
                fold_train_data = train_data[train_indices]
                fold_test_data = train_data[test_indices]
                
                # ë°ì´í„° ì¤€ë¹„
                X_train, y_train, prev_train, X_val, y_val, prev_val = prepare_data(
                    fold_train_data, fold_test_data, params['sequence_length'],
                    predict_window, target_col_idx, augment=False
                )
                
                # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
                if len(X_train) < params['batch_size'] or len(X_val) < 1:
                    logger.warning(f"Fold {fold_idx+1}: ë°ì´í„° ë¶ˆì¶©ë¶„ (í›ˆë ¨: {len(X_train)}, ê²€ì¦: {len(X_val)})")
                    continue
                
                # ë°ì´í„°ì…‹ ë° ë¡œë” ìƒì„± (CPUì—ì„œ ìƒì„±, í•™ìŠµ ì‹œ GPUë¡œ ì´ë™)
                train_dataset = TimeSeriesDataset(X_train, y_train, torch.device('cpu'), prev_train)
                batch_size = min(params['batch_size'], len(X_train))
                train_loader = DataLoader(
                    train_dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    worker_init_fn=seed_worker,
                    generator=g
                )
                
                val_dataset = TimeSeriesDataset(X_val, y_val, torch.device('cpu'), prev_val)
                val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)
                
                # ëª¨ë¸ ìƒì„±
                model = ImprovedLSTMPredictor(
                    input_size=input_size,
                    hidden_size=params['hidden_size'],
                    num_layers=params['num_layers'],
                    dropout=params['dropout'],
                    output_size=predict_window
                ).to(device)

                # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
                criterion = DirectionalLoss(
                    alpha=params['loss_alpha'],
                    beta=params['loss_beta']
                )

                optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'])
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5,
                    patience=params['patience']//2
                )

                # best_val_loss ë³€ìˆ˜ ëª…ì‹œì  ì •ì˜
                best_val_loss = float('inf')
                patience_counter = 0
                
                for epoch in range(params['num_epochs']):
                    # í•™ìŠµ
                    model.train()
                    train_loss = 0
                    for X_batch, y_batch, prev_batch in train_loader:
                        optimizer.zero_grad()
                        
                        # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                        X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                        y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                        prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                        
                        # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                        y_pred = model(X_batch, prev_batch)
                        loss = criterion(y_pred, y_batch, prev_batch)
                        
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                        optimizer.step()
                        train_loss += loss.item()
                    
                    # ê²€ì¦
                    model.eval()
                    val_loss = 0
                    
                    with torch.no_grad():
                        for X_batch, y_batch, prev_batch in val_loader:
                            # ëª¨ë¸ê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ë°ì´í„° ì´ë™
                            X_batch = X_batch.to(device, non_blocking=device.type=='cuda')
                            y_batch = y_batch.to(device, non_blocking=device.type=='cuda')
                            prev_batch = prev_batch.to(device, non_blocking=device.type=='cuda')
                            
                            # ëª¨ë¸ ì˜ˆì¸¡ ë° ì†ì‹¤ ê³„ì‚°
                            y_pred = model(X_batch, prev_batch)
                            loss = criterion(y_pred, y_batch, prev_batch)
                            
                            val_loss += loss.item()
                        
                        val_loss /= len(val_loader)
                        
                        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                        scheduler.step(val_loss)
                        
                        if val_loss < best_val_loss:
                            best_val_loss = val_loss
                            patience_counter = 0
                        else:
                            patience_counter += 1
                        
                        if patience_counter >= params['patience']:
                            break
                
                valid_fold_count += 1
                fold_losses.append(best_val_loss)
                
            except Exception as e:
                logger.error(f"Error in fold {fold_idx+1}: {str(e)}")
                continue
        
        # ëª¨ë“  foldê°€ ì‹¤íŒ¨í•œ ê²½ìš° ë§¤ìš° í° ì†ì‹¤ê°’ ë°˜í™˜
        if not fold_losses:
            logger.warning("ëª¨ë“  foldê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì´ íŒŒë¼ë¯¸í„° ì¡°í•©ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
            return float('inf')
        
        # ì„±ê³µí•œ foldì˜ í‰ê·  ì†ì‹¤ê°’ ë°˜í™˜
        return sum(fold_losses) / len(fold_losses)
    
    # Optuna ìµœì í™” ì‹œë„
    try:
        import optuna
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        study.optimize(objective, n_trials=n_trials)
        
        # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°
        if study.best_trial.value == float('inf'):
            logger.warning(f"ëª¨ë“  trialì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return default_params
            
        best_params = study.best_params
        logger.info(f"\n{current_period} ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° (K-fold):")
        for key, value in best_params.items():
            logger.info(f"  {key}: {value}")
        
        # ëª¨ë“  í•„ìˆ˜ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
        required_keys = ['sequence_length', 'hidden_size', 'num_layers', 'dropout', 
                        'batch_size', 'learning_rate', 'num_epochs', 'patience',
                        'warmup_steps', 'lr_factor', 'lr_patience', 'min_lr',
                        'loss_alpha', 'loss_beta', 'loss_gamma', 'loss_delta']
        
        for key in required_keys:
            if key not in best_params:
                # ëˆ„ë½ëœ í‚¤ê°€ ìˆìœ¼ë©´ ê¸°ë³¸ê°’ í• ë‹¹
                if key == 'warmup_steps':
                    best_params[key] = 382
                elif key == 'lr_factor':
                    best_params[key] = 0.49
                elif key == 'lr_patience':
                    best_params[key] = 8
                elif key == 'min_lr':
                    best_params[key] = 1e-7
                elif key == 'loss_gamma':
                    best_params[key] = 0.07
                elif key == 'loss_delta':
                    best_params[key] = 0.07
                else:
                    best_params[key] = default_params[key]
        
        # ìºì‹œì— ì €ì¥
        with open(cache_file, 'w') as f:
            json.dump(best_params, f, indent=2)
        
        logger.info(f"í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ {cache_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return best_params
        
    except Exception as e:
        logger.error(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì˜¤ë¥˜: {str(e)}")
        traceback.print_exc()
        
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë°˜í™˜
        return default_params
    
class WarmupScheduler:
    def __init__(self, optimizer, warmup_steps, max_lr):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_lr = max_lr
        self.current_step = 0

    def step(self):
        self.current_step += 1
        # warmup ë‹¨ê³„ ë™ì•ˆ ì„ í˜• ì¦ê°€
        lr = self.max_lr * self.current_step / self.warmup_steps
        # warmup ë‹¨ê³„ë¥¼ ì´ˆê³¼í•˜ë©´ max_lrë¡œ ê³ ì •
        if self.current_step > self.warmup_steps:
            lr = self.max_lr
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr