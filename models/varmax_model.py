import logging
import pandas as pd
import numpy as np
import random
import torch
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score, precision_score, recall_score, f1_score

import traceback
from statsmodels.tsa.statespace.varmax import VARMAX
from app.core.state_manager import prediction_state

logger = logging.getLogger(__name__)

# VARMAX ê´€ë ¨ import (ì„ íƒì  ê°€ì ¸ì˜¤ê¸°)
try:
    from statsmodels.tsa.statespace.varmax import VARMAX
    VARMAX_AVAILABLE = True
except ImportError:
    VARMAX_AVAILABLE = False
    logger.warning("VARMAX not available. Please install statsmodels.")

SEED = 42

def set_seed(seed=SEED):
    """
    ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ ë³´ì¥
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    
    # PyTorchì˜ deterministic ë™ì‘ ê°•ì œ
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    # Optuna ì‹œë“œ ì„¤ì • (í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ìš©)
    try:
        import optuna
        # Optuna 2.x ë²„ì „ í˜¸í™˜
        if hasattr(optuna.samplers, 'RandomSampler'):
            optuna.samplers.RandomSampler(seed=seed)
        # ë ˆê±°ì‹œ ì§€ì›
        if hasattr(optuna.samplers, '_random'):
            optuna.samplers._random.seed(seed)
    except Exception as e:
        logger.debug(f"Optuna ì‹œë“œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    logger.debug(f"ğŸ¯ ëœë¤ ì‹œë“œ {seed}ë¡œ ê³ ì •ë¨")

# ë‚ ì§œ í¬ë§·íŒ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
def format_date(date_obj, format_str='%Y-%m-%d'):
    """ë‚ ì§œ ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    try:
        # pandas Timestamp ë˜ëŠ” datetime.datetime
        if hasattr(date_obj, 'strftime'):
            return date_obj.strftime(format_str)
        
        # numpy.datetime64
        elif isinstance(date_obj, np.datetime64):
            # ë‚ ì§œ í¬ë§·ì´ 'YYYY-MM-DD'ì¸ ê²½ìš°
            return str(date_obj)[:10]
        
        # ë¬¸ìì—´ì¸ ê²½ìš° ì´ë¯¸ ë‚ ì§œ í˜•ì‹ì´ë¼ë©´ ì¶”ê°€ ì²˜ë¦¬
        elif isinstance(date_obj, str):
            # GMT í˜•ì‹ì´ë©´ íŒŒì‹±í•˜ì—¬ ë³€í™˜
            if 'GMT' in date_obj:
                parsed_date = datetime.strptime(date_obj, '%a, %d %b %Y %H:%M:%S GMT')
                return parsed_date.strftime(format_str)
            return date_obj[:10] if len(date_obj) > 10 else date_obj
        
        # ê·¸ ì™¸ ê²½ìš°
        else:
            return str(date_obj)
    
    except Exception as e:
        logger.warning(f"ë‚ ì§œ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
        return str(date_obj)

#######################################################################
# VARMAX ê´€ë ¨ í´ë˜ìŠ¤ ë° í•¨ìˆ˜
#######################################################################

class VARMAXSemiMonthlyForecaster:
    """VARMAX ê¸°ë°˜ ë°˜ì›”ë³„ ì‹œê³„ì—´ ì˜ˆì¸¡ í´ë˜ìŠ¤ - ì„¸ ë²ˆì§¸ íƒ­ìš©"""
    
    def __init__(self, file_path, result_var='MOPJ', pred_days=50):
        # ì¼ê´€ëœ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ê³ ì •
        set_seed()
        
        self.file_path = file_path
        self.result_var = result_var
        self.pred_days = pred_days
        self.df1 = None
        self.df_origin = None
        self.target_df = None
        self.df_train = None
        self.df_test = None
        self.ts_exchange = None
        self.exogenous_data = None
        self.varx_result = None
        self.pred_df = None
        self.final_forecast_var = None
        self.final_value = None
        self.final_index = None
        self.filtered_vars = None
        self.var_num = None  # ê¸°ë³¸ê°’
        self.r2_train = None
        self.r2_test = None
        self.pred_index = None
        self.selected_vars = []
        self.mape_value = None

    def load_data(self):
        """ë°ì´í„° ë¡œë“œ (VARMAX ëª¨ë¸ìš© - ëª¨ë“  ë°ì´í„° ì‚¬ìš©, ìµœê·¼ 800ê°œë¡œ ì œí•œ)"""
        try:
            # VARMAX ëª¨ë¸ì€ ì¥ê¸°ì˜ˆì¸¡ì´ë¯€ë¡œ ëª¨ë“  ë°ì´í„° ì‚¬ìš© (2022ë…„ ì´ì „ í¬í•¨)
            from app.data.loader import load_data as data_loader
            df_full = data_loader(self.file_path, model_type='varmax')
            # ê¸°ì¡´ ë¡œì§ ìœ ì§€: ìµœê·¼ 800ê°œ ë°ì´í„°ë§Œ ì‚¬ìš©
            self.df_origin = df_full.iloc[-800:]
            logger.info(f"VARMAX data loaded: {self.df_origin.shape} (last 800 records from full dataset)")
            logger.info(f"Date range: {self.df_origin.index.min()} to {self.df_origin.index.max()}")
        except Exception as e:
            logger.error(f"Data loading failed: {str(e)}")
            raise e

    def select_variables(self, current_date=None):
        """ë³€ìˆ˜ ì„ íƒ - í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€"""
        try:
            # ğŸ”‘ ìˆ˜ì •: í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if current_date is not None:
                if isinstance(current_date, str):
                    current_date = pd.to_datetime(current_date)
                recent_data = self.df_origin[self.df_origin.index <= current_date]
                logger.info(f"ğŸ”§ Variable selection using data up to {current_date.strftime('%Y-%m-%d')} ({len(recent_data)} records)")
            else:
                recent_data = self.df_origin
                logger.info(f"ğŸ”§ Variable selection using all available data ({len(recent_data)} records)")
            
            correlations = recent_data.corr()[self.result_var]
            correlations = correlations.drop(self.result_var)
            correlations = correlations.sort_values(ascending=False)
            select = correlations.index.tolist()
            self.selected_vars = select
            
            # ë³€ìˆ˜ ê·¸ë£¹ ì •ì˜ (ì›ë³¸ ì½”ë“œì™€ ë™ì¼)
            variable_groups = {
                'crude_oil': ['WTI', 'Brent', 'Dubai'],
                'gasoline': ['Gasoline_92RON', 'Gasoline_95RON', 'Europe_M.G_10ppm', 'RBOB (NYMEX)_M1'],
                'naphtha': ['MOPAG', 'MOPS', 'Europe_CIF NWE'],
                'lpg': ['C3_LPG', 'C4_LPG'],
                'product': ['EL_CRF NEA', 'EL_CRF SEA', 'PL_FOB Korea', 'BZ_FOB Korea', 'BZ_FOB SEA', 'BZ_FOB US M1', 'BZ_FOB US M2', 'TL_FOB Korea', 'TL_FOB US M1', 'TL_FOB US M2',
                'MX_FOB Korea', 'PX_FOB Korea', 'SM_FOB Korea', 'RPG Value_FOB PG', 'FO_HSFO 180 CST', 'MTBE_FOB Singapore'],
                'spread': ['Monthly Spread','BZ_H2-TIME SPREAD', 'Brent_WTI', 'MOPJ_MOPAG', 'MOPJ_MOPS', 'Naphtha_Spread', 'MG92_E Nap', 'C3_MOPJ', 'C4_MOPJ', 'Nap_Dubai',
                'MG92_Nap_MOPS', '95R_92R_Asia', 'M1_M2_RBOB', 'RBOB_Brent_m1', 'RBOB_Brent_m2', 'EL_MOPJ', 'PL_MOPJ', 'BZ_MOPJ', 'TL_MOPJ', 'PX_MOPJ', 'HD_EL', 'LD_EL', 'LLD_EL', 'PP_PL',
                'SM_EL+BZ', 'US_FOBK_BZ', 'NAP_HSFO_180', 'MTBE_MOPJ'],
                'economics': ['Dow_Jones', 'Euro', 'Gold'],
                'freight': ['Freight_55_PG', 'Freight_55_Maili', 'Freight_55_Yosu', 'Freight_55_Daes', 'Freight_55_Chiba',
                'Freight_75_PG', 'Freight_75_Maili', 'Freight_75_Yosu', 'Freight_75_Daes', 'Freight_75_Chiba', 'Flat Rate_PG', 'Flat Rate_Maili', 'Flat Rate_Yosu', 'Flat Rate_Daes',
                'Flat Rate_Chiba']
            }
            
            # ê·¸ë£¹ë³„ ìµœì  ë³€ìˆ˜ ì„ íƒ
            self.filtered_vars = []
            for group, variables in variable_groups.items():
                filtered_group_vars = [var for var in variables if var in self.selected_vars]
                if filtered_group_vars:
                    best_var = max(filtered_group_vars, key=lambda x: abs(correlations[x]))
                    self.filtered_vars.append(best_var)
            
            self.selected_vars = sorted(self.filtered_vars, key=lambda x: abs(correlations[x]), reverse=True)
            logger.info(f"Selected {len(self.selected_vars)} variables for VARMAX prediction")
            logger.info(f"Top 5 selected variables: {self.selected_vars[:5]}")
            
        except Exception as e:
            logger.error(f"Variable selection failed: {str(e)}")
            raise e

    def prepare_data_for_prediction(self, current_date):
        """ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„"""
        try:
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if isinstance(current_date, str):
                current_date = pd.to_datetime(current_date)
            
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            
            filtered_values = self.selected_vars
            input_columns = filtered_values[:self.var_num]
            output_column = [self.result_var]
            
            self.final_value = historical_data.iloc[-1][self.result_var]
            self.final_index = historical_data.index[-1]

            self.target_df = historical_data[input_columns + output_column]
            
            self.df_train = self.target_df
            
            # ì™¸ìƒë³€ìˆ˜ (í™˜ìœ¨) ì„¤ì •
            if 'Exchange' in self.df_origin.columns:
                self.ts_exchange = historical_data['Exchange']
                self.exogenous_data = pd.DataFrame(self.ts_exchange, index=self.ts_exchange.index)
            else:
                self.exogenous_data = None
                
        except Exception as e:
            logger.error(f"Data preparation failed: {str(e)}")
            raise e

    def fit_varmax_model(self):
        """VARMAX ëª¨ë¸ í•™ìŠµ"""
        try:
            if not VARMAX_AVAILABLE:
                raise ImportError("VARMAX dependencies not available")
                
            logger.info("ğŸ”„ [VARMAX_FIT] Starting VARMAX model fitting...")
            logger.info(f"ğŸ”„ [VARMAX_FIT] Training data shape: {self.df_train.shape}")
            logger.info(f"ğŸ”„ [VARMAX_FIT] Exogenous data available: {self.exogenous_data is not None}")
            
            best_p = 7
            best_q = 0
            
            logger.info(f"ğŸ”„ [VARMAX_FIT] Creating VARMAX model with order=({best_p}, {best_q})")
            varx_model = VARMAX(endog=self.df_train, exog=self.exogenous_data, order=(best_p, best_q))
            
            logger.info("ğŸ”„ [VARMAX_FIT] Starting model fitting (this may take a while)...")
            
            # ğŸ”‘ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            prediction_state['varmax_prediction_progress'] = 50
            
            self.varx_result = varx_model.fit(disp=False, maxiter=1000)
            
            if hasattr(self.varx_result, 'converged') and not self.varx_result.converged:
                logger.warning("âš ï¸ [VARMAX_FIT] VARMAX model did not converge (res.converged=False)")
            else:
                logger.info("âœ… [VARMAX_FIT] VARMAX model converged successfully")
                
            logger.info("âœ… [VARMAX_FIT] VARMAX model fitted successfully")
            
            # ğŸ”‘ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            prediction_state['varmax_prediction_progress'] = 60
            
        except Exception as e:
            logger.error(f"âŒ [VARMAX_FIT] VARMAX fitting failed: {str(e)}")
            logger.error(f"âŒ [VARMAX_FIT] Fitting error traceback: {traceback.format_exc()}")
            
            # ğŸ”‘ ì—ëŸ¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            prediction_state['varmax_error'] = f"Model fitting failed: {str(e)}"
            prediction_state['varmax_is_predicting'] = False
            prediction_state['varmax_prediction_progress'] = 0
            
            raise e

    def forecast_varmax(self):
        """VARMAX ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            # ë¯¸ë˜ ì™¸ìƒë³€ìˆ˜ ì¤€ë¹„
            if self.exogenous_data is not None:
                # ë§ˆì§€ë§‰ ê°’ì„ ì˜ˆì¸¡ ê¸°ê°„ë§Œí¼ ë°˜ë³µ
                last_exog_value = self.ts_exchange.iloc[-1]
                future_dates = pd.bdate_range(start=self.final_index + pd.Timedelta(days=1), periods=self.pred_days)
                exog_future = pd.DataFrame([last_exog_value] * self.pred_days, 
                                         index=future_dates, 
                                         columns=self.exogenous_data.columns)
            else:
                exog_future = None
                future_dates = pd.bdate_range(start=self.final_index + pd.Timedelta(days=1), periods=self.pred_days)
                
            # VARMAX ì˜ˆì¸¡
            varx_forecast = self.varx_result.forecast(steps=self.pred_days, exog=exog_future)
            self.pred_index = future_dates
            self.pred_df = pd.DataFrame(varx_forecast.values, index=self.pred_index, columns=self.df_train.columns)
            logger.info(f"VARMAX forecast completed for {self.pred_days} days")
            
        except Exception as e:
            logger.error(f"VARMAX forecasting failed: {str(e)}")
            raise e

    def residual_correction(self):
        """ëœë¤í¬ë ˆìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ ì”ì°¨ ë³´ì •"""
        try:
            if not VARMAX_AVAILABLE:
                logger.warning("VARMAX not available, skipping residual correction")
                self.final_forecast_var = self.pred_df[[self.result_var]]
                self.r2_train = 0.0
                self.r2_test = 0.0
                return
                
            # ì”ì°¨ ê³„ì‚°
            residuals_origin = self.df_train - self.varx_result.fittedvalues
            residuals_real = residuals_origin.iloc[1:]
            X = residuals_real.iloc[:, :-1]
            y = residuals_real.iloc[:, -1]
            
            # í…ŒìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
            test_size_value = min(0.3, (self.pred_days + 1) / len(self.target_df))
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_value, shuffle=False)
            
            # ëœë¤í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ
            rfr_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rfr_model.fit(X_train, y_train)
            
            # ì„±ëŠ¥ í‰ê°€
            y_train_pred = rfr_model.predict(X_train)
            y_test_pred = rfr_model.predict(X_test)
            self.r2_train = r2_score(y_train, y_train_pred)
            self.r2_test = r2_score(y_test, y_test_pred)
            
            # ì˜ˆì¸¡ì— ì”ì°¨ ë³´ì • ì ìš©
            var_predictions = self.pred_df[[self.result_var]]
            
            # ìµœê·¼ ì”ì°¨ ë°ì´í„°ë¡œ ì˜ˆì¸¡
            recent_residuals = residuals_real.iloc[-self.pred_days:, :-1]
            if len(recent_residuals) < self.pred_days:
                # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ í–‰ì„ ë°˜ë³µ
                last_residual = residuals_real.iloc[-1:, :-1]
                additional_rows = self.pred_days - len(recent_residuals)
                repeated_residuals = pd.concat([last_residual] * additional_rows, ignore_index=True)
                recent_residuals = pd.concat([recent_residuals, repeated_residuals])[:self.pred_days]
            
            rfr_predictions = rfr_model.predict(recent_residuals.iloc[:len(var_predictions)])
            rfr_pred_df = pd.DataFrame(rfr_predictions, 
                                     index=var_predictions.index, 
                                     columns=var_predictions.columns)
            
            # ìµœì¢… ì˜ˆì¸¡ê°’ = VARMAX ì˜ˆì¸¡ + ì”ì°¨ ë³´ì •
            self.final_forecast_var = var_predictions.add(rfr_pred_df)
            
            logger.info(f"Residual correction completed. Train R2: {self.r2_train:.4f}, Test R2: {self.r2_test:.4f}")
            
        except Exception as e:
            logger.error(f"Residual correction failed: {str(e)}")
            # ë³´ì • ì‹¤íŒ¨ ì‹œ ì›ë³¸ VARMAX ì˜ˆì¸¡ê°’ ì‚¬ìš©
            self.final_forecast_var = self.pred_df[[self.result_var]]
            self.r2_train = 0.0
            self.r2_test = 0.0

    def calculate_performance_metrics(self, actual_data=None):
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° (ì‹¤ì œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)"""
        if actual_data is None:
            return {
                'f1': 0.0,
                'accuracy': 0.0,
                'mape': 0.0,
                'weighted_score': 0.0,
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0
            }
        
        try:
            # ë°©í–¥ì„± ì˜ˆì¸¡ ì„±ëŠ¥
            pred_series = self.final_forecast_var[self.result_var]
            actual_series = actual_data
            
            pred_trend = (pred_series.diff() > 0).astype(int)[1:]
            actual_trend = (actual_series.diff() > 0).astype(int)[1:]
            
            # ê³µí†µ ì¸ë±ìŠ¤ë¡œ ë§ì¶¤
            common_idx = pred_trend.index.intersection(actual_trend.index)
            if len(common_idx) > 0:
                pred_trend_common = pred_trend.loc[common_idx]
                actual_trend_common = actual_trend.loc[common_idx]
                
                if VARMAX_AVAILABLE:
                    precision = precision_score(actual_trend_common, pred_trend_common, zero_division=0)
                    recall = recall_score(actual_trend_common, pred_trend_common, zero_division=0)
                    f1 = f1_score(actual_trend_common, pred_trend_common, zero_division=0)
                    accuracy = (actual_trend_common == pred_trend_common).mean() * 100
                else:
                    precision = recall = f1 = accuracy = 0.0
            else:
                precision = recall = f1 = accuracy = 0.0
            
            # MAPE ê³„ì‚°
            common_values_pred = pred_series.loc[common_idx] if len(common_idx) > 0 else pred_series
            common_values_actual = actual_series.loc[common_idx] if len(common_idx) > 0 else actual_series
            
            mask = common_values_actual != 0
            if mask.any():
                mape = np.mean(np.abs((common_values_actual[mask] - common_values_pred[mask]) / common_values_actual[mask])) * 100
            else:
                mape = 0.0
            
            return {
                'f1': f1,
                'accuracy': accuracy,
                'mape': mape,
                'weighted_score': f1 * 100,  # F1 ì ìˆ˜ë¥¼ ê°€ì¤‘ ì ìˆ˜ë¡œ ì‚¬ìš©
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0,
                'precision': precision,
                'recall': recall
            }
            
        except Exception as e:
            logger.error(f"Performance calculation failed: {str(e)}")
            return {
                'f1': 0.0,
                'accuracy': 0.0,
                'mape': 0.0,
                'weighted_score': 0.0,
                'r2_train': self.r2_train or 0.0,
                'r2_test': self.r2_test or 0.0
            }

    def calculate_moving_averages(self, predictions, current_date, windows=[5, 10, 23]):
        """ì´ë™í‰ê·  ê³„ì‚° (ê¸°ì¡´ app.py ë°©ì‹ê³¼ ë™ì¼)"""
        try:
            results = {}
            
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ê³¼ê±° ë°ì´í„° ì¶”ê°€ (ì´ë™í‰ê·  ê³„ì‚°ìš©)
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            historical_series = historical_data[self.result_var].tail(30)  # ìµœê·¼ 30ì¼
            
            # ì˜ˆì¸¡ ì‹œë¦¬ì¦ˆ ìƒì„±
            prediction_series = pd.Series(
                data=pred_df['Prediction'].values,
                index=pred_df['Date']
            )
            
            # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
            combined_series = pd.concat([historical_series, prediction_series])
            combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
            combined_series = combined_series.sort_index()
            
            # ê° ìœˆë„ìš°ë³„ ì´ë™í‰ê·  ê³„ì‚°
            for window in windows:
                rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
                
                window_results = []
                for i, row in pred_df.iterrows():
                    date = row['Date']
                    pred_value = row['Prediction']
                    actual_value = row['Actual']
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê· 
                    ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                    
                    window_results.append({
                        'date': date,
                        'prediction': pred_value,
                        'actual': actual_value,
                        'ma': ma_value
                    })
                
                results[f'ma{window}'] = window_results
            
            return results
            
        except Exception as e:
            logger.error(f"Moving average calculation failed: {str(e)}")
            return {}

    def calculate_moving_averages_varmax(self, predictions, current_date, windows=[5, 10, 20, 30]):
        try:
            results = {}
            
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ê³¼ê±° ë°ì´í„° ì¶”ê°€ (ì´ë™í‰ê·  ê³„ì‚°ìš©)
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            historical_series = historical_data[self.result_var].tail(30)  # ìµœê·¼ 30ì¼
            
            # ì˜ˆì¸¡ ì‹œë¦¬ì¦ˆ ìƒì„±
            prediction_series = pd.Series(
                data=pred_df['Prediction'].values,
                index=pred_df['Date']
            )
            
            # ê³¼ê±°ì™€ ì˜ˆì¸¡ ë°ì´í„° ê²°í•©
            combined_series = pd.concat([historical_series, prediction_series])
            combined_series = combined_series[~combined_series.index.duplicated(keep='last')]
            combined_series = combined_series.sort_index()
            
            # ê° ìœˆë„ìš°ë³„ ì´ë™í‰ê·  ê³„ì‚°
            for window in windows:
                rolling_avg = combined_series.rolling(window=window, min_periods=1).mean()
                
                window_results = []
                for i, row in pred_df.iterrows():
                    date = row['Date']
                    pred_value = row['Prediction']
                    actual_value = row['Actual']
                    
                    # í•´ë‹¹ ë‚ ì§œì˜ ì´ë™í‰ê· 
                    ma_value = rolling_avg.loc[date] if date in rolling_avg.index else None
                    
                    window_results.append({
                        'date': date,
                        'prediction': pred_value,
                        'actual': actual_value,
                        'ma': ma_value
                    })
                
                results[f'ma{window}'] = window_results
            
            return results
            
        except Exception as e:
            logger.error(f"Moving average calculation failed: {str(e)}")
            return {}

    def calculate_half_month_averages(self, predictions, current_date):
        """VarmaxResult ì»´í¬ë„ŒíŠ¸ìš© ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚°"""
        try:
            # ì˜ˆì¸¡ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            pred_df = pd.DataFrame(predictions)
            pred_df['Date'] = pd.to_datetime(pred_df['Date'])
            pred_df = pred_df.sort_values('Date')
            
            # ë°˜ì›” ê¸°ê°„ë³„ë¡œ ê·¸ë£¹í™”
            half_month_groups = {}
            
            for _, row in pred_df.iterrows():
                date = row['Date']
                
                # ë°˜ì›” ë¼ë²¨ ìƒì„± (ì˜ˆ: 25_05_1 = 2025ë…„ 5ì›” ìƒë°˜ê¸°)
                year = date.year % 100  # ì—°ë„ ë§ˆì§€ë§‰ ë‘ ìë¦¬
                month = date.month
                half = 1 if date.day <= 15 else 2
                
                half_month_label = f"{year:02d}_{month:02d}_{half}"
                
                if half_month_label not in half_month_groups:
                    half_month_groups[half_month_label] = []
                
                half_month_groups[half_month_label].append(row['Prediction'])
            
            # ê° ë°˜ì›” ê¸°ê°„ì˜ í‰ê·  ê³„ì‚°
            half_month_data = []
            for label, values in half_month_groups.items():
                avg_value = np.mean(values)
                half_month_data.append({
                    'half_month_label': label,
                    'half_month_avg': float(avg_value),
                    'count': len(values)
                })
            
            # ë¼ë²¨ìˆœìœ¼ë¡œ ì •ë ¬
            half_month_data.sort(key=lambda x: x['half_month_label'])
            
            logger.info(f"ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚° ì™„ë£Œ: {len(half_month_data)}ê°œ ê¸°ê°„")
            
            return half_month_data
            
        except Exception as e:
            logger.error(f"Half month averages calculation failed: {str(e)}")
            return []

    def prepare_variable_for_prediction(self, current_date):
        """ì˜ˆì¸¡ìš© ë°ì´í„° ì¤€ë¹„"""
        try:
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©
            if isinstance(current_date, str):
                current_date = pd.to_datetime(current_date)
            
            # í˜„ì¬ ë‚ ì§œê¹Œì§€ì˜ ë°ì´í„° í•„í„°ë§
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            
            filtered_values = self.selected_vars
            input_columns = filtered_values[:self.var_num]
            output_column = [self.result_var]
            
            self.final_value = historical_data.iloc[-1-self.pred_days][self.result_var]
            self.final_index = historical_data.index[-1-self.pred_days]

            self.target_df = historical_data[input_columns + output_column]
            
            self.df_train = self.target_df[:-self.pred_days]
            
            # ì™¸ìƒë³€ìˆ˜ (í™˜ìœ¨) ì„¤ì •
            if 'Exchange' in self.df_origin.columns:
                self.ts_exchange = historical_data['Exchange']
                self.ts_exchange = self.ts_exchange[:-self.pred_days]
                self.exogenous_data = pd.DataFrame(self.ts_exchange, index=self.ts_exchange.index)
            else:
                self.exogenous_data = None
                
        except Exception as e:
            logger.error(f"Data preparation failed: {str(e)}")
            raise e

    def calculate_mape(self, predicted, actual):
        mape = np.mean(np.abs((actual - predicted) / actual)) * 100
        return mape 

    def generate_variables_varmax(self, current_date, var_num):
        """ë³€ìˆ˜ ìˆ˜ ì˜ˆì¸¡ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        try:
            self.var_num = var_num
            self.load_data()
            self.select_variables(current_date)
            self.prepare_variable_for_prediction(current_date)
            self.fit_varmax_model()
            logger.info("VARMAX ë³€ìˆ˜ ì„ ì • ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

            self.forecast_varmax()
            logger.info("VARMAX ë³€ìˆ˜ ì„ ì • ëª¨ë¸ ì˜ˆì¸¡ ì™„ë£Œ")

            self.residual_correction()
            logger.info(f"ì”ì°¨ ë³´ì • ì™„ë£Œ (R2 train={self.r2_train:.3f}, test={self.r2_test:.3f})")
            historical_data = self.df_origin[self.df_origin.index <= current_date]
            test_data = historical_data[-self.pred_days:]
            self.final_forecast_var.index = test_data.index
            self.mape_value = self.calculate_mape(self.final_forecast_var[self.result_var], test_data[self.result_var])
            
            return self.mape_value

        except Exception as e:
            logger.error(f"VARMAX variables generation failed: {str(e)}")
            return None

    def generate_predictions_varmax(self, current_date, var_num):
        """VARMAX ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            logger.info(f"ğŸ”„ [VARMAX_GEN] Starting VARMAX prediction generation")
            logger.info(f"ğŸ”„ [VARMAX_GEN] Parameters: current_date={current_date}, var_num={var_num}")
            
            self.var_num = var_num
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 1: Loading data...")
            prediction_state['varmax_prediction_progress'] = 35
            self.load_data()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 2: Selecting variables...")
            prediction_state['varmax_prediction_progress'] = 40
            self.select_variables(current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 3: Preparing data for prediction...")
            prediction_state['varmax_prediction_progress'] = 45
            self.prepare_data_for_prediction(current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 4: Fitting VARMAX model...")
            # fit_varmax_model ë‚´ì—ì„œ 50â†’60ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë¨
            self.fit_varmax_model()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 5: Forecasting...")
            prediction_state['varmax_prediction_progress'] = 65
            self.forecast_varmax()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 6: Residual correction...")
            prediction_state['varmax_prediction_progress'] = 70
            self.residual_correction()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 7: Converting results to standard format...")
            prediction_state['varmax_prediction_progress'] = 75
            # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            predictions = []
            for date, value in self.final_forecast_var.iterrows():
                predictions.append({
                    'Date': format_date(date),
                    'Prediction': float(value[self.result_var]),
                    'Actual': None  # ì‹¤ì œê°’ì€ ë¯¸ë˜ì´ë¯€ë¡œ None
                })
            logger.info(f"ğŸ”„ [VARMAX_GEN] Converted {len(predictions)} predictions")
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 8: Calculating performance metrics...")
            prediction_state['varmax_prediction_progress'] = 80
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self.calculate_performance_metrics()
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 9: Calculating moving averages...")
            prediction_state['varmax_prediction_progress'] = 85
            # ì´ë™í‰ê·  ê³„ì‚° (VARMAXìš©)
            ma_results = self.calculate_moving_averages_varmax(predictions, current_date)
            
            logger.info(f"ğŸ”„ [VARMAX_GEN] Step 10: Calculating half-month averages...")
            prediction_state['varmax_prediction_progress'] = 90
            # ë°˜ì›” í‰ê·  ë°ì´í„° ê³„ì‚° (VarmaxResult ì»´í¬ë„ŒíŠ¸ìš©)
            half_month_data = self.calculate_half_month_averages(predictions, current_date)
            
            logger.info(f"âœ… [VARMAX_GEN] All steps completed successfully!")
            logger.info(f"âœ… [VARMAX_GEN] Final results: {len(predictions)} predictions, {len(ma_results)} MA windows")
            
            return {
                'success': True,
                'predictions': predictions,  # ì›ë˜ ì˜ˆì¸¡ ë°ì´í„° (ì°¨íŠ¸ìš©)
                'half_month_averages': half_month_data,  # ë°˜ì›” í‰ê·  ë°ì´í„° (VarmaxResult ì»´í¬ë„ŒíŠ¸ìš©)
                'metrics': metrics,
                'ma_results': ma_results,
                'selected_features': self.selected_vars[:var_num],
                'current_date': format_date(current_date),
                'model_info': {
                    'model_type': 'VARMAX',
                    'variables_used': var_num,
                    'prediction_days': self.pred_days,
                    'r2_train': self.r2_train,
                    'r2_test': self.r2_test
                }
            }
            
        except Exception as e:
            logger.error(f"âŒ [VARMAX_GEN] VARMAX prediction failed: {str(e)}")
            logger.error(f"âŒ [VARMAX_GEN] Full traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': str(e)
            }
        
#######################################################################
# VARMAX ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
#######################################################################

def varmax_decision(file_path):
    """Varmax ì˜ì‚¬ê²°ì • ê´€ë ¨"""
    fp = pd.read_csv(file_path)
    df = pd.DataFrame(fp, columns=fp.columns)
    col = df.columns
    # 1) ë¶„ì„ì— ì‚¬ìš©í•  ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
    vars_pct = ['max_pct2', 'min_pct2', 'mean_pct2', 'max_pct3', 'min_pct3', 'mean_pct3']
    logger.info(f'ë°ì´í„°í”„ë ˆì„{df}')
    rename_dict = {
    'max_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct2': '[í˜„ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct2': '[í˜„ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]',
    'max_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ì¦ê°€ìœ¨]',
    'min_pct3': '[ì´ì „ ë°˜ì›” ìµœëŒ€ ê°ì†Œìœ¨]',
    'mean_pct3': '[ì´ì „ ë°˜ì›” í‰ê·  ë³€ë™ë¥ ]'
    }
    rename_col = list(rename_dict.values())
    df = df.rename(columns=rename_dict)
    logger.info(f'ì—´{col}')
    # 2) Case ì •ì˜
    case1 = df['saving_rate'] < 0
    abs_thresh = df['saving_rate'].abs().quantile(0.9)
    case2 = df['saving_rate'].abs() >= abs_thresh

    # 3) ìµœì  ì¡°ê±´ íƒìƒ‰ í•¨ìˆ˜
    def find_best_condition(df, case_mask, var):
        best = None
        for direction in ['greater', 'less']:
            for p in np.linspace(0.1, 0.9, 9):
                th = df[var].quantile(p)
                if direction == 'greater':
                    mask = df[var] > th
                else:
                    mask = df[var] < th
                # ìƒ˜í”Œ ìˆ˜ê°€ ë„ˆë¬´ ì ì€ ê²½ìš° ì œì™¸
                if mask.sum() < 5:
                    continue
                prop = case_mask[mask].mean()
                if best is None or prop > best[4]:
                    best = (direction, p, th, mask.sum(), prop)
        return best

    # 5) ê° ë³€ìˆ˜ë³„ ìµœì  ì¡°ê±´ ì°¾ê¸°
    results_case1 = {var: find_best_condition(df, case1, var) for var in rename_col}
    results_case2 = {var: find_best_condition(df, case2, var) for var in rename_col}

    from itertools import combinations
    # 6) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ saving_rate < 0 ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€ (ìƒ˜í”Œ ìˆ˜ â‰¥ 30)
    combi_results_case1 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case1[mask].mean()
                        combi_results_case1.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìŒìˆ˜ ë¹„ìœ¨ [%]": round(rate*100, 3)
                        })
    column_order1 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìŒìˆ˜ ë¹„ìœ¨ [%]"]
    combi_df_case1 = pd.DataFrame(combi_results_case1).sort_values(by="ìŒìˆ˜ ë¹„ìœ¨ [%]", ascending=False)
    combi_df_case1 = combi_df_case1.reindex(columns=column_order1)

    # 7) ë‘ ë³€ìˆ˜ ì¡°í•©ì„ ì‚¬ìš©í•˜ì—¬ ì ˆëŒ“ê°’ ìƒìœ„ 10% ë¶„ë¥˜ ì„±ëŠ¥ í‰ê°€
    combi_results_case2 = []

    for var1, var2 in combinations(rename_col, 2):
        for d1 in ['greater', 'less']:
            for d2 in ['greater', 'less']:
                for p1 in np.linspace(0.1, 0.9, 9):
                    for p2 in np.linspace(0.1, 0.9, 9):
                        th1 = df[var1].quantile(p1)
                        th2 = df[var2].quantile(p2)
                        mask1 = df[var1] > th1 if d1 == 'greater' else df[var1] < th1
                        mask2 = df[var2] > th2 if d2 == 'greater' else df[var2] < th2
                        mask = mask1 & mask2
                        n = mask.sum()
                        if n < 30:
                            continue
                        rate = case2[mask].mean()
                        combi_results_case2.append({
                            "ì¡°ê±´1": f"{var1} { '>' if d1 == 'greater' else '<' } {round(th1, 3)}%",
                            "ì¡°ê±´2": f"{var2} { '>' if d2 == 'greater' else '<' } {round(th2, 3)}%",
                            "ìƒ˜í”Œ ìˆ˜": n,
                            "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]": round(rate*100, 3)
                        })
    column_order2 = ["ì¡°ê±´1", "ì¡°ê±´2", "ìƒ˜í”Œ ìˆ˜", "ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]"]
    combi_df_case2 = pd.DataFrame(combi_results_case2).sort_values(by="ìƒìœ„ ë³€ë™ì„± í™•ë¥  [%]", ascending=False)
    combi_df_case2 = combi_df_case2.reindex(columns=column_order2)
    return {
        'case_1': combi_df_case1.to_dict(orient='records'),
        'case_2': combi_df_case2.to_dict(orient='records')
    }

# VARMAX ê´€ë ¨ ìºì‹œ í•¨ìˆ˜ import (í˜¸í™˜ì„±ì„ ìœ„í•´)
try:
    from app.data.cache_manager import load_varmax_prediction, save_varmax_prediction
    logger.info("âœ… [VARMAX_MODEL] Successfully imported cache functions")
except ImportError as e:
    logger.warning(f"âš ï¸ [VARMAX_MODEL] Failed to import cache functions: {e}")
    
    # Fallback í•¨ìˆ˜ ì •ì˜
    def load_varmax_prediction(prediction_date):
        """Fallback load_varmax_prediction function"""
        logger.warning("âš ï¸ [VARMAX_MODEL] Using fallback load_varmax_prediction")
        return None
    
    def save_varmax_prediction(prediction_data, prediction_date):
        """Fallback save_varmax_prediction function"""
        logger.warning("âš ï¸ [VARMAX_MODEL] Using fallback save_varmax_prediction")
        return False